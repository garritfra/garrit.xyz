<?xml version="1.0"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>
            garrit.xyz
        </title>
        <link>
            https://garrit.xyz
        </link>
        <description>
            Garrit Franke
        </description>
        <language>
            en
        </language>
        <lastBuildDate>
            Sun, 01 Sep 2024 00:00:00 +0000
        </lastBuildDate>
        <item>
            <title>
                Mental AI Fog and how to cure it
            </title>
            <guid>
                https://garrit.xyz/posts/2024-09-01-AI-fog
            </guid>
            <link>
                https://garrit.xyz/posts/2024-09-01-AI-fog?utm_source=rss
            </link>
            <pubDate>
                Sun, 01 Sep 2024 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>The term &quot;AI Slop&quot; is currently on the rise. It describes all the AI generated images and texts we see on the internet. I&#39;d like to propose a term that basically describes reverse AI Slop: <strong>Mental AI Fog</strong>.</p>

<p>Instead of <em>consuming</em> too much AI generated content (which also applies), AI Fog describes the inability to <em>produce</em> content without the help of AI. We&#39;re surrounded by flowery written articles and resources that we think that it&#39;s not worth it to put in the effort to write a text ourselves. This is comparable to how computer keyboards, spellchecking and autocorrection have rendered my generation and the ones to come incapable of comfortably writing longform text.</p>

<p>I&#39;m currently strongly suffering from AI fog. I&#39;m so used to letting some LLM flesh out a quick and dirty prompt nowadays that it&#39;s hard for me to write this text, get the point across and not feel insecure about my style of writing. This site is supposed to be a way to persist my thoughts whenever I want to, but are they still <em>my</em> thoughts if they have been proofread and corrected by a computer?</p>

<p>As a result, all these thoughts are piling up in my head. Where I previously braindumped thoughts on a piece of paper, I now only come up with a couple of words and let the AI elaborate. <strong>I&#39;m consuming what are supposed to be my own thoughts</strong>, which perpetuates the cycle.</p>

<p>This needs to stop. I need to get back <strong>creating</strong> things myself. I decided to abandon the use of LLMs for most content on this site. And where AI has been used, it will be clearly mentioned. I&#39;m contemplating adding some sort of &quot;Backed by AI&quot; label for certain pages to make it harder for myself to fall back to a helping hand. I will likely still be using LLMs, but making it obvious will force me to mindfully choose where it will be used.</p>

<p>Is this something you can relate to? Is AI fog even a fitting term for this? I don&#39;t know. And if it isn&#39;t, that&#39;s okay because I came up with it myself.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Sentiment analysis using ML models
            </title>
            <guid>
                https://garrit.xyz/posts/2024-08-31-sentiment-analysis-using-ml-models
            </guid>
            <link>
                https://garrit.xyz/posts/2024-08-31-sentiment-analysis-using-ml-models?utm_source=rss
            </link>
            <pubDate>
                Sat, 31 Aug 2024 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>I just rewrote parts of my <a href="https://github.com/garritfra/positive_hackernews">Positive Hacker News RSS Feed</a> project to use an ML model to filter out any negative news from the Hacker News timeline. This method is far more reliable than the previous method of using a <a href="https://www.nltk.org/api/nltk.sentiment.vader.html">rule-based sentiment analyzer</a> through NLTK.</p>

<p>I&#39;m using the model <a href="https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest?text=Messer-Angreiferin+verletzt+f%C3%BCnf+Menschen+in+einem+Bus">cardiffnlp/twitter-roberta-base-sentiment-latest</a>, which was trained on a huge amount of tweets. It&#39;s really tiny (~500 MB) and easily runs inside the existing GitHub Actions workflows. You can try out the model yourself on the HuggingFace model card.</p>

<p>&lt;img width=&quot;522&quot; alt=&quot;grafik&quot; src=&quot;https://github.com/user-attachments/assets/06f42df6-624a-4108-ada8-d0d37a53e693&quot;&gt;</p>

<p>If you want to subscribe to more positive tech news, simply replace your Hacker News feed of your RSS reader with this one (or add it if you haven&#39;t already):
https://garritfra.github.io/positive_hackernews/feed.xml</p>]]>
            </description>
        </item>
        <item>
            <title>
                How embedding models encode semantic meaning
            </title>
            <guid>
                https://garrit.xyz/posts/2024-08-03-how-embedding-models-encode-semantic-meaning
            </guid>
            <link>
                https://garrit.xyz/posts/2024-08-03-how-embedding-models-encode-semantic-meaning?utm_source=rss
            </link>
            <pubDate>
                Sat, 03 Aug 2024 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>Embedding models have long been a daunting concept for me. But what are they? And why are they so useful? Let&#39;s break it down in simple terms.</p>

<h2>What&#39;s an embedding?</h2>

<p>An embedding is basically a numerical representation of a piece of information - it could be text, audio, an image, or even a video. Think of it as a way to capture the essence or meaning of that information in a list of numbers.</p>

<p>For example, let&#39;s say we have this text: &quot;show me a list of ground transportation at boston airport&quot;. An embedding model might turn that into something like this:</p>

<p><code>
[0.03793335, -0.008010864, -0.002319336, -0.0110321045, -0.019882202, -0.023864746, 0.011428833, -0.030349731, -0.044830322, 0.028289795, -0.02810669, -0.0032749176, -0.04208374, -0.0077705383, -0.0033798218, -0.06335449, ... ]
</code></p>

<p>At first, thus looks like a jumble of numbers. But each of these numbers points to a specific area within the embedding model&#39;s &quot;space&quot;, where similar words or concepts might be located.</p>

<h2>Visualizing embeddings</h2>

<p>To help wrap our heads around this, let&#39;s look at a visualization. This beautiful image shows the entirety of the <a href="https://huggingface.co/nomic-ai/nomic-embed-text-v1.5">nomic-embed-text-v1.5</a> embedding model, as generated by <a href="https://atlas.nomic.ai/map/nomic-text-embed-v1-5m-sample">this visualization tool</a>:</p>

<p><img alt="nomic-embed-text-v1.5-full" src="/assets/posts/2024-08-03-how-embedding-models-encode-semantic-meaning/nomic-embed-text-v1.5-full.jpeg"/></p>

<p>Now, if we take our example text about Boston airport transportation and plot its embeddings on this map, we&#39;d see that some clusters are lit up, especially around &quot;transportation&quot;. This means that the model has figured out that the topic of the query must be related to transportation in some way.</p>

<p><img alt="nomic-embed-text-v1.5-query" src="/assets/posts/2024-08-03-how-embedding-models-encode-semantic-meaning/nomic-embed-text-v1.5-query.jpeg"/></p>

<p>Zooming into this image, we can see more specific topics around transportation, like &quot;Airport&quot;, &quot;Travel&quot; or &quot;Highways&quot; are lit up, which more closely matches our query.</p>

<p><img alt="nomic-embed-text-v1.5-transportation" src="/assets/posts/2024-08-03-how-embedding-models-encode-semantic-meaning/nomic-embed-text-v1.5-transportation.jpeg"/></p>

<p>In a nutshell, embedding models are able to group terms by topics that are related to each other.</p>

<h2>Why should we care about embeddings?</h2>

<p>Encoding meaning in text has tons of different use cases. One that I&#39;m particularly excited about is building RAG applications. RAG stands for Retrieval-Augmented Generation and refers to a method for Large Language Models (LLMs), where, given a question, you enrich the original question with relevant bits of information before answering it.</p>

<p>Here&#39;s how embeddings are useful for RAG:</p>

<ol><li>You have a bunch of documents in your data source.</li><li>You use an embedding model to turn each document into a list of numbers (like we saw earlier).</li><li>When someone asks a question, you also turn that question into a list of numbers.</li><li>Then, you find the documents whose number lists are most similar to your question&#39;s number list.</li><li>Voila! You&#39;ve found the most relevant documents to answer the question.</li></ol>

<p>This method is way better than previously used techniques like just searching for exact words in the documents. It&#39;s like the difference between having a librarian who only looks at book titles, and one who actually understands what the books are about.</p>

<h2>Other things you can do with embeddings</h2>

<p>Beyond RAG applications, embeddings are super useful for all sorts of things:</p>

<ol><li><strong>Smarter searches</strong>: Find related stuff even if the exact words don&#39;t match.</li><li><strong>Better recommendations</strong>: &quot;You liked this? You might also like these similar things!&quot;</li><li><strong>Language translation</strong>: Help computers understand that &quot;dog&quot; in English and &quot;perro&quot; in Spanish mean the same thing.</li><li><strong>Sentiment analysis</strong>: Figure out if someone&#39;s happy or grumpy based on their tweet.</li></ol>

<h2>Wrapping it up</h2>

<p>Embeddings are a clever way to turn words (or images, or sounds) into numbers that computers can understand and compare. By doing this, we can make emerging AI technologies a whole lot smarter at understanding language and finding connections between ideas.</p>

<p>Next time you&#39;re chatting with an AI or getting scarily accurate recommendations online, you can nod knowingly and think, &quot;Ah yes, embeddings at work!&quot;</p>]]>
            </description>
        </item>
        <item>
            <title>
                ðŸ”— Linkdump: LLMs
            </title>
            <guid>
                https://garrit.xyz/posts/2024-07-02-linkdump-llms
            </guid>
            <link>
                https://garrit.xyz/posts/2024-07-02-linkdump-llms?utm_source=rss
            </link>
            <pubDate>
                Tue, 02 Jul 2024 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>The more I&#39;m getting into large language models (LLMs), the more I&#39;m fascinated about what you can do with them. To &quot;digest&quot; my reading list of cool articles and projects regarding LLMs, I assembled the following list. If you&#39;re also interested but haven&#39;t started your journey into this neverending rabbit hole, these may contain some good pointers:</p>

<h2>Running LLMs</h2>

<ul><li><a href="https://ollama.com/">Ollama</a> - Host different LLMs locally (extremely easy!)</li><li><a href="https://docs.vllm.ai">vLLM</a> - Like Ollama but more scalable and production-ready</li><li><a href="https://github.com/Mozilla-Ocho/llamafile">llamafile</a> - LLMs as a single, portable binary</li></ul>

<h2>Building Chatbots &amp; Applications</h2>

<ul><li><a href="https://www.llamaindex.ai/">LlamaIndex</a> - Framework for building LLM applications</li><li><a href="https://github.com/run-llama/create-llama/">create-llama</a> - LlamaIndex template generator (perfect for if you don&#39;t know how to structure your app)</li><li><a href="https://www.langchain.com/">LangChain</a> - Like LlamaIndex with a less clean interface IMO</li><li><a href="https://haystack.deepset.ai/overview/intro">Haystack</a> - Another LLM framework. Haven&#39;t tested it though</li><li><a href="https://sdk.vercel.ai/docs/introduction">Vercel AI SDK</a> - For integrating your LLM app into a web frontend</li></ul>

<h2>Training and finetuning</h2>

<ul><li><a href="https://huggingface.co/">HuggingFace</a> - GitHub but for LLMs</li><li><a href="https://medium.com/data-science-in-your-pocket/lora-for-fine-tuning-llms-explained-with-codes-and-example-62a7ac5a3578">LoRA for Fine-Tuning LLMs explained with codes and example</a> - Great LoRA Primer</li><li><a href="https://huggingface.co/autotrain">AutoTrain</a> - NoCode LLM finetuning</li><li><a href="https://github.com/georgian-io/LLM-Finetuning-Toolkit">llm-toolkit</a> - Another finetuning framework</li><li><a href="https://mlflow.org/">MLFlow</a> - End to End platform for LLM training</li></ul>

<h2>Miscellaneous projects</h2>

<ul><li><a href="https://github.com/iyaja/llama-fs">llama-fs</a> - Self-organinizing filesystem</li><li><a href="https://github.com/ask-fini/paramount">paramount</a> - Measure agent accuracy</li></ul>

<h2>Blogs</h2>

<ul><li><a href="https://simonwillison.net/tags/llms/">Simon Willison</a> - Awesome posts from a LLM enthusiast</li><li><a href="https://matt-rickard.com/tags/ai">Matt Richard</a> - Matt is currently inactive, but his blog is a treasure trove</li></ul>]]>
            </description>
        </item>
        <item>
            <title>
                Testing SMTP connections
            </title>
            <guid>
                https://garrit.xyz/posts/2024-06-27-testing-smtp-connections
            </guid>
            <link>
                https://garrit.xyz/posts/2024-06-27-testing-smtp-connections?utm_source=rss
            </link>
            <pubDate>
                Thu, 27 Jun 2024 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>Just a quick note to my future self on how to test a SMTP connection with nothing but a tiny busybox container.</p>

<p>In my case specifically, I tested the connection from inside a Kubernetes cluster. Here&#39;s the quickest way to get a temporary pod up and running:</p>

<p><code>
kubectl run -n backend -i --tty --rm debug --image=busybox --restart=Never
</code></p>

<p>Busybox comes with telnet installed, which we can use to establish a connection to the server:</p>

<p><code>
/ # telnet smtp.mydomain.com 25
Connected to smtp.mydomain.com
220 mail.mydomain.com ESMTP Postfix (SMTP)
</code></p>

<p>Next, we can issue the SMTP commands through the open TCP connection to send a test mail. Lines beginning with a status code are server responses:</p>

<p><code></code>`
HELO smtp.mydomain.com
250 smtp.mydomain.com
MAIL FROM:<a href="mailto:noreply@mydomain.com">noreply@mydomain.com</a>                       <br/>250 2.1.0 Ok
RCPT TO:<a href="mailto:receiver@foo.com">receiver@foo.com</a>
250 2.1.5 Ok
DATA<br/>354 End data with &lt;CR&gt;&lt;LF&gt;.&lt;CR&gt;&lt;LF&gt;
From: [noreply] <a href="mailto:noreply@mydomain.com">noreply@mydomain.com</a>
To: [Receiver] <a href="mailto:receiver@foo.com">receiver@foo.com</a>
Date: Thu, 27 Jun 2024 10:08:26 -0200
Subject: Test Message</p>

<p>This is a test message.</p>

<p>.
250 2.0.0 Ok: queued as 2478B7F135
<code></code>`</p>

<p>In case there&#39;s a firewall issue, you might not be able to establish a connection in the first place, or you won&#39;t get a reply to your TCP commands. In our case, everything worked fine.</p>

<p>I hope this is useful!</p>]]>
            </description>
        </item>
        <item>
            <title>
                Host your own LLM
            </title>
            <guid>
                https://garrit.xyz/posts/2024-06-17-host-your-own-llm
            </guid>
            <link>
                https://garrit.xyz/posts/2024-06-17-host-your-own-llm?utm_source=rss
            </link>
            <pubDate>
                Mon, 17 Jun 2024 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>I&#39;m currently dipping my toes into Large Language Models (LLMs, or &quot;AI&quot;) and what you can do with them. It&#39;s a fascinating topic, so expect some more posts on this in the coming days and weeks.</p>

<p>For starters, I wanted to document how I got my first LLM running on my local machine (a 2022 MacBook Pro). <a href="https://ollama.com/">Ollama</a> makes this process super easy. You just install it (<code>brew install ollama</code> in my case) and then run the model:</p>

<p><code>
ollama run llama3
</code></p>

<p>This will download the model and open a prompt, so you can start chatting right away!</p>

<p>You can think of Ollama as the <a href="https://www.docker.com/">Docker</a> CLI but for LLMs. There&#39;s a <a href="https://ollama.com/library">directory of LLMs</a>, and if a model has multiple different sizes, you can use it like you would pull a different docker tag:</p>

<p><code>
ollama pull llama3:8b
ollama pull llama3:70b
</code></p>

<p>The best thing about ollama is that it also exposes a web server for you to integrate the LLM into your application. As an example, here&#39;s how you would curl your local LLM:</p>

<p><code>
curl http://localhost:11434/api/chat -d &#39;{
    &quot;model&quot;: &quot;llama3&quot;,      
    &quot;messages&quot;: [{ &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Are you a robot?&quot; }],
    &quot;stream&quot;: false
}&#39;
{&quot;model&quot;:&quot;llama3&quot;,&quot;created_at&quot;:&quot;2024-06-17T11:19:23.510588Z&quot;,&quot;message&quot;:{&quot;role&quot;:&quot;assistant&quot;,&quot;content&quot;:&quot;I am not a human, but I&#39;m also not a traditional robot. I&#39;m an artificial intelligence language model designed to simulate conversation and answer questions to the best of my ability. My \&quot;brain\&quot; is a complex algorithm that processes natural language inputs and generates responses based on patterns and associations learned from large datasets.\n\nWhile I don&#39;t have a physical body or consciousness like humans do, I&#39;m designed to interact with humans in a way that feels natural and conversational. I can understand and respond to questions, make suggestions, and even tell jokes (though my humor may be a bit... algorithmic).\n\nSo, while I&#39;m not a human or a traditional robot, I exist at the intersection of technology and language, designed to assist and communicate with humans in a helpful way!&quot;},&quot;done_reason&quot;:&quot;stop&quot;,&quot;done&quot;:true,&quot;total_duration&quot;:12565842250,&quot;load_duration&quot;:7059262291,&quot;prompt_eval_count&quot;:15,&quot;prompt_eval_duration&quot;:331275000,&quot;eval_count&quot;:156,&quot;eval_duration&quot;:5172858000}
</code></p>

<p>If your local machine is not beefy enough and you want to try out a large LLM on a rented server (AWS has <code>g5.2xlarge</code>, which gave me good results for <code>mixtral 8x7b</code>), you also have to set <code>OLLAMA_HOST=0.0.0.0</code> in your environment variables to be able to reach the remote server. <strong>This exposes the LLM to the public internet, so be careful when chosing your deployment strategy.</strong></p>

<p>And there you go! You just deployed your very own LLM. Pretty cool, huh?</p>]]>
            </description>
        </item>
        <item>
            <title>
                I just cleaned up 40 GB of Brew caches
            </title>
            <guid>
                https://garrit.xyz/posts/2024-06-03-i-just-cleaned-up-40-gb-of-brew-caches
            </guid>
            <link>
                https://garrit.xyz/posts/2024-06-03-i-just-cleaned-up-40-gb-of-brew-caches?utm_source=rss
            </link>
            <pubDate>
                Mon, 03 Jun 2024 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<blockquote><p><strong>EDIT</strong>: This trick will probably not be as effective on your system as it was on my system. After writing this post I realized that I had the <code>HOMEBREW_NO_INSTALL_CLEANUP=1</code> flag enabled on my system.</p></blockquote>

<p>My system (MacOS) is getting more cluttered the more I use it. I&#39;m sure you can relate. If you&#39;re using <a href="https://brew.sh/">Brew</a> as your package manager (which you should ðŸ˜‰), you might want to consider running the following command:</p>

<p><code>
brew cleanup -s
</code></p>

<p>For some reason this failed after some time with a &quot;directory not found&quot; error, but you can just run it again and it will continue cleaning up old caches. Once it was done, this freed up <strong>40 GB of disk space</strong> on my system. It might make sense to run this as a cronjob? Either way, I just wanted to jot this down before I enevitably forget this, as usual.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Going from self hosted to managed software
            </title>
            <guid>
                https://garrit.xyz/posts/2024-05-24-going-from-self-hosted-to-managed-software
            </guid>
            <link>
                https://garrit.xyz/posts/2024-05-24-going-from-self-hosted-to-managed-software?utm_source=rss
            </link>
            <pubDate>
                Fri, 24 May 2024 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>Some time ago I was heavily into <a href="https://garrit.xyz/posts/2022-09-26-self-hosted-software-im-thankful-for">self hosting my own software</a>. Over time though, it became apparent that maintaining these services is a huge burden. I either abandoned most of the services or found a replacement that suits my needs and saves me time that&#39;s better spent on other things in life.</p>

<p>Today, I finally pulled the plug on <a href="https://miniflux.app/">Miniflux</a>, the last service I used to self host (not counting House Assistant on a Pi, which I use to automate some stuff at home). The Hetzner server it ran on cost me ~4â‚¬ a month. The maintainer of Miniflux offers a managed hosting solution for 15$ a year, so to save money and time, and to support the project, I decided to switch from self-hosted to managed.</p>

<h2>Will I ever self host again?</h2>

<p>I really don&#39;t know. Some services (Miniflux included) required very little maintenance to keep running, so I could see myself spinning up a server again if I really wanted to run some software that doesn&#39;t have managed hosting. For now though, I&#39;m planning on using managed services wherever I can. Nevertheless I&#39;m proud of the things I learned during my time of self hosting my software. I think it gave me a huge boost both in terms of know how and my career, and I&#39;d encourage everyone to dip a toe into self hosting.</p>]]>
            </description>
        </item>
        <item>
            <title>
                About my 'smart' G-Shock
            </title>
            <guid>
                https://garrit.xyz/posts/2024-05-10-about-my-smart-g-shock
            </guid>
            <link>
                https://garrit.xyz/posts/2024-05-10-about-my-smart-g-shock?utm_source=rss
            </link>
            <pubDate>
                Fri, 10 May 2024 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>About two weeks ago, I got myself my very first G-Shock. My entry ticket to the <a href="https://mastodon.social/tags/CasioCult">#CasioCult</a>! Since I didn&#39;t want to miss out on any fitness features, I went with the <a href="https://www.casio.com/de/watches/gshock/product.DW-H5600-1/">G-Shock GW-H5600</a>, which comes with heart rate monitoring, step tracking and some other nice to have features.</p>

<p><img alt="A G-Shock DW-H5600" src="https://github.com/garritfra/garrit.xyz/assets/32395585/ab17acee-5997-4a3d-8536-c8c8243d72d2"/></p>

<p>Having read every review I could find, I knew that the watch had some quirks, but I ultimately loved the look of it. But two weeks later, I&#39;m forced to return the watch. Let me first go over what I liked and dislike about this watch.</p>

<h2>Pros</h2>

<p><strong>It&#39;s a G-Shock</strong></p>

<p>I love the look and feel of a G-Shock and other Casio watches. This is my first G-Shock, but I&#39;ve had an eye on this brand for quite some time. It&#39;s tough, sturdy, waterproof and probably also bullet proof.</p>

<p><strong>The display</strong></p>

<p>Oh, the display! It&#39;s gorgeous. Many people are complaining why this MIP display hasn&#39;t found its way to the rest of the G-Shock lineup. But I&#39;m sure they will retrofit other models soon.</p>

<p><strong>Proper paper manual</strong></p>

<p>My first impression after the unboxing was &quot;holy sheers, the manual is a <strong>book</strong>&quot;! And I mean that in the best way possible. Every little feature of the watch is explained in extreme detail in the manual. You rarely see this in other watches or gadgets these days.</p>

<h2>Cons</h2>

<p><strong>Questionable UI and UX decisions</strong></p>

<p>G-Shocks are known for user interfaces that &quot;just work&quot;. In this watch however, there are a few things that really make the watch frustrating to operate. Just to name a few quirks:</p>

<ul><li>There&#39;s only &quot;forward&quot;. If you scrolled passed something, you have to go aaaaall the way through the menu.</li><li>The timer can only be increased in 30 second steps. The UI is quite laggy, so setting a 10 minute timer takes about 1 minute of mashing buttons. There&#39;s also no press and hold.</li><li>The stopwatch only counts in seconds. No milliseconds.</li><li>You&#39;re locked to one task at a time. If you have a timer running, you can&#39;t go out of the menu. If you&#39;re recording a workout, you can&#39;t change the settings. And my favorite: You have to be on the home screen to sync settings from your phone to your watch. Just ... why?!</li><li>With a more or less recent update, the light now always turns on when you press any button. This makes the &quot;light&quot; button on the top right completely obsolete.</li><li>Unspecific battery indicator. You have four bars in the battery icon, that&#39;s it. The watch is solar powered, but it&#39;s to top up the battery at best. Don&#39;t expect to run fully off solar with this watch.</li></ul>

<p><strong>A bit thick</strong></p>

<p>Below the already thick bezel of the G-Shock sits the heart rate monitor, which is obnoxiously huge. I personally didn&#39;t feel like it irritated my skin, but it does add to the thickness.</p>

<p><strong>Fitness Features</strong></p>

<p>This is a two-edged sword. I originally wanted to list this as a pro. Features like run tracking, cardio status and sleep analysis are there, and they work fine. The step tracking and heart rate monitoring can be a bit off from time to time, but it&#39;s fine for the occasional run.</p>

<p>What&#39;s really bothering me is the mediocre implementation. There&#39;s no workout detection, the autopause feature barely works and you&#39;re limited to running, walking, gym workout and interval training. I also want to log my bike rides, which will always show up as a running activity in my app.</p>

<p>They recently added Strava integration, which sort of works. The data is all there, but for some reason the pace and moving time that Strava computes are completely off. For example, I ran 55 minutes straight with an average pace of 9 minutes / km. But with the data the watch sends, Strava thinks I only moved for 26 minutes with a pace of 4 minutes / km. If that were true, I could compete in the olympics! ðŸ˜…</p>

<p><strong>Notifications barely work</strong></p>

<p>During the time I owned the watch, there was only one random day where the watch received notifications from my phone. I can&#39;t explain why, but I just didn&#39;t get this to work properly.</p>

<p><strong>Case is not screwed in. Could fall off</strong></p>

<p>This is just a minor thing if you really care about durability. All G-Shock squares so far had screwed in bezels. On this model, you can pull off the bezel with enough force. I&#39;ve never done it, but losing the bezel might be a concern.</p>

<h2>What&#39;s next?</h2>

<p>I enjoyed my time with this watch, but I filed a return. As pretty as the watch is, it is just too frustrating to use. Smart watches aren&#39;t a core part of Casio&#39;s business, and you can feel that in the lack of quality of the software. The app was likely developed as a one-off contract by some random agency, and once the contract ends, the watch won&#39;t ever be improved again.</p>

<p>I&#39;ve now ordered the <a href="https://www.garmin.com/de-DE/p/741462">Garmin Instinct 2S Solar</a>, which is very similar in terms of specs. MIP display, top-up solar charging, long-lasting battery and fitness features. But on top of that, it has features like music control, GPS, contactless payment and a compass. Contrary to Casio, Smartwatches are a core part of Garmin&#39;s business, which makes me believe that they go out of their way to develop the best possible watch they can. I will likely follow up with how this new watch compares to my G-Shock.</p>]]>
            </description>
        </item>
        <item>
            <title>
                I sold my car
            </title>
            <guid>
                https://garrit.xyz/posts/2024-05-08-i-sold-my-car
            </guid>
            <link>
                https://garrit.xyz/posts/2024-05-08-i-sold-my-car?utm_source=rss
            </link>
            <pubDate>
                Wed, 08 May 2024 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>I grew up in the countryside, where busses only came once an hour, but only until 7 PM. And on the weekend, you <strong>really</strong> had to plan ahead. Living without a car was almost unthinkable.</p>

<p><img alt="My car" src="/assets/posts/2024-05-08-i-sold-my-car/my-car.jpeg"/></p>

<p>After moving to the city in 2020, I found that I used my car less and less. My commute to work was easier with the bike (if I wasn&#39;t working from home), and for anything else, busses and trams were the way to go. The occasional family visit was the only excuse to use my car.</p>

<p>Owning a car doesn&#39;t &quot;feel&quot; expensive. Sure, gas is expensive, but simply owning one doesn&#39;t feel like a huge burden on your wallet. Just to get a rough estimate, I jotted down all running costs.</p>

<p><img alt="Running costs" src="/assets/posts/2024-05-08-i-sold-my-car/running-costs.jpeg"/></p>

<p>My car costs <strong>over 1000â‚¬ per year</strong>, or about 85â‚¬ per month, just to be moved about twice a month! So, a few weeks ago, I decided to sell it.</p>

<p>I created a posting on <a href="https://www.kleinanzeigen.de/">Kleinanzeigen</a> (a german descendant of - and previously owned by - eBay), and after about two weeks, the car had a new owner. I hope that it&#39;s in better hands now and lives on to be a happy and frequently used car.</p>

<p>Going forward, I&#39;m not worried about my mobility at all. My partner still owns her car and has the option to take the bus to work. So, if we&#39;re not on the road together, I can always fall back to using her car. And if she needs her car and I&#39;m desperate to go somewhere afar, there&#39;s also multiple ride-sharing options available in my town. They are a bit expensive to use, but spending 40â‚¬ for a day trip to my family every two months is still way less than paying 85â‚¬ a month for a car that&#39;s sitting there idle.</p>]]>
            </description>
        </item>
    </channel>
</rss>