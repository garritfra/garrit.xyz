{"pageProps":{"posts":[{"slug":"whom-do-you-trust","markdownBody":"\nNowadays, password managers are a necessity if you care even the slightest about your personal belongings on the interwebs. But think about it, do you really want to trust another company to store your most sensitive information?\n\n![](https://images.unsplash.com/photo-1522251670181-320150ad6dab?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=2566&q=80)\n\n##### TL;DR\n\nUse a **stateless** password manager like [LessPass](https://lesspass.com/) to access your password without relying on a third party to store your data.\n\n## Why use a password manager in the first place?\n\nHaving a single password for multiple accounts is convenient. What's also convenient, is **using** this password on multiple accounts once you have access to a single one. What might be convenient to you, might also be convenient to others. Many people, [especially celebrities](https://web.archive.org/web/20170225163642/http://uk.businessinsider.com/twitter-says-it-wasnt-hacked-passwords-reused-older-hacks-malware-to-blame-2016-6), fall victim to this trap of comfort.\n\nTo counteract this, people are (and should be) using different passwords for different accounts. These passwords differ in a single letter or digit (Twitter: `porridge4president`, GitHub: `poridge5president`, etc.), or they don't match at all (Twitter: `porridge4president`, GitHub: `YouWontGuessThisOne`).\n\nThe problem that most likely arises from this technique is called _password chaos_ ([Source](https://encyclopedia2.thefreedictionary.com/password+chaos)):\n\n> _\"The confusion that arises when users have many unique passwords.\"_\n\nThe aim of a password manager is [to solve this problem by storing all of your passwords in a single place and securing it with an _ultra secure superpassword!_ (©)](https://www.businessinsider.com/how-to-use-password-manager-store-protect-yourself-hackers-lastpass-1password-dashlane-2017-2?r=DE&IR=T). This way, you can use arbitrary passwords - preferebly gibberish that doesn't make sense to humans (nor machines) - without losing them, as long as you have your _ultra secure superpassword!_ (©) - aka your masterpassword. The benefits are obvious: You get rid of the password chaos problem while staying relatively secure. Eventhough password managers are quite benificial, some people (including myself) see a catch in them.\n\n## Relying on a third party\n\nRelying on third party companies doesn't seem like a big deal. After all, you are probably using some form of cloud service to host your photos. Yet there's a lot of trust involved in letting others handle your private data, especially your passwords. In 2017, [a major password manager got hacked, exposing sensitive data including users and their passwords](https://www.zdnet.com/article/onelogin-hit-by-data-breached-exposing-sensitive-customer-data/). This shows the potential for security breaches in an application that inherently seemed safe. But what if I told you that there is an alternative to this? A password manager that does not store your data at all?\n\n## A Stateless password manager\n\nRecently, I stumbled across [LessPass](https://lesspass.com/#/). LessPass is a password manager that is very different from what I have seen so far. Instead of storing passwords that either you or a random password generator came up with, it computes passwords on the fly, given a website, username, your masterpassword and some options.\n\n![LessPass](../assets/lesspass.gif)\n\nThe key here is that LessPass uses a **pure function**, i.e. a function that given the same parameters will always give the same result. Using this technique, there's no need to store your password in a database owned by a large company, nor do you have to sync your passwords between your devices ([but there's an app](https://play.google.com/store/apps/details?id=com.lesspass.android&hl=de)). The computation happens right on your machine, and **only** on your machine. If you want to find out more about how it works under the hood, you can check out [the authors blog post](https://blog.lesspass.com/lesspass-how-it-works-dde742dd18a4#.vbgschksh). He goes into great detail on what alorithm is used to compute your passwords and how to utilize every feature of LessPass.\n\n## Conclusion\n\nBeing a little privacy nerd myself, I often think twice about what services I want to use, often even looking into self-hosted alternatives to major products. There are multiple products that offer self hosted solutions to store your passwords, however I also don't even trust _myself_ with such sensitive data either. LessPass eliminates the need to have a third party watch over your data, let alone to store it on their servers.\n","frontmatter":{"title":"Whom do you trust?","date":"2020-03-17","tags":"security, privacy, note"}},{"slug":"testing-isnt-hard","markdownBody":"\n\"I write two tests before implementing a method\", \"My project has 90% coverage\".\n\nI don't know about you, but that's something I don't hear very often. But why is that?\n\nTesting is not even that difficult to do, but yet it is always coming short in my projects. About a year ago, I've tried to implement tests in my React applications with little success, mostly because integrating `enzyme` and configuring it correctly is not that intuitive as a relatively new developer. I want to share my (partly opinionated) approach to JavaScript testing with `jest`, to get you started. In a later post I will demonstrate a way to implement `enzyme` into your React projects.\n\n# The basics of testing JavaScript functions\n\nTo get started, you need a npm-project. I don't think I have to explain that, but just in case:\n\n```bash\nmkdir awesome-testing-project\ncd awesome-testing-project\nnpm init -y\n```\n\nOf course, we need a unit we want to test. What about a method that returns the first element of an array?\n\n```js\nmodule.exports = function firstElement(arr) {\n\treturn arr[1];\n};\n```\n\nYou already spotted a bug, huh? Let's keep it simple for now.\n\nInstall and initialize Jest, an open-source testing framework maintained by Facebook. When initializing, you should check every question with `y`.\n\n```bash\nnpm i --save-dev jest\nnpx jest --init\n```\n\nNext up, we need to define our first test. Conventionally, we create a folder named `__tests__` in the directory of the module we want to test. inside it, there should be a file named `<module>.test.js`. Something like this:\n\n```bash\n▶ tree\n.\n├── package.json\n└── src\n    ├── __tests__\n    │   └── firstElement.test.js\n    └── firstElement.js\n```\n\nJest provides global functions that do not need to be imported in a file. A simple test can look like this:\n\n```js\nconst firstElement = require(\"../firstElement.js\");\n\ntest(\"firstElement gets first element of array\", () => {\n\texpect(firstElement([1, 2])).toBe(1);\n});\n```\n\n`expect` is another word for `assert`. If you ask me, \"Expect firstElement of [1, 2] to be 1\" sounds reasonably english, doesn't it? After defining the test, all there is to do left is to run the `npm test` command, which has been created for us by running `npx jest --init` earlier.\n\n```bash\n▶ npm test\n> jest\n\n FAIL  src/__tests__/firstElement.test.js\n  ✕ firstElement (6ms)\n\n  ● firstElement\n\n    expect(received).toBe(expected) // Object.is equality\n\n    Expected: 1\n    Received: 2\n\n      2 |\n      3 | test('firstElement', () => {\n    > 4 |   expect(firstElement([1, 2])).toBe(1);\n        |                                ^\n      5 | });\n      6 |\n\n      at Object.<anonymous>.test (src/__tests__/firstElement.test.js:4:32)\n\nTest Suites: 1 failed, 1 total\nTests:       1 failed, 1 total\nSnapshots:   0 total\nTime:        1.1s\nRan all test suites.\nnpm ERR! Test failed.  See above for more details.\n```\n\nWhoops! Looks like we have found a bug! Let's fix it by adjusting the index of the return value in the firstElement function:\n\n```js\nmodule.exports = function firstElement(arr) {\n\treturn arr[0];\n};\n```\n\nAnd after rerunning `npm test`:\n\n```bash\n▶ npm test\n> jest\n\n PASS  src/__tests__/firstElement.test.js\n  ✓ firstElement (4ms)\n\nTest Suites: 1 passed, 1 total\nTests:       1 passed, 1 total\nSnapshots:   0 total\nTime:        0.666s, estimated 2s\nRan all test suites.\n```\n\nYay, your first unit test! Of course, there is much more to find out about the Jest framework. To see a full guide, read the [official docs](https://jestjs.io/).\n\nI have prepared a [template repository](https://github.com/garritfra/react-parcel-boilerplate) for building react apps. It also uses Jest to run tests, you don't have to worry about a thing! If you found this interesting, consider checking out my other blog posts, and/or check out my [GitHub](https://github.com/garritfra)!\n","frontmatter":{"title":"Testing isn't hard","date":"2019-11-08","tags":"javascript, guide, web, programming"}},{"slug":"quick-tip-terminal-pastebin","markdownBody":"\nEver find yourself in a situation where you simply want to save or share the output of a terminal command? Selecting, copying and pasting text from stdout always feels quite tedious, if you just want to share the contents of a file.\n\nA project called [Termbin](https://termbin.com/) tries to simplify this process. Just pipe the command you want to save to the following url on port `9999`, using Netcat:\n\n```sh\necho \"Hello, Blog!\" | nc termbin.com 9999\n```\n\nInstead of showing the output, it will be forwarded to Termbin and show the URL, under which your output will be available:\n\n```sh\n➜  blog git:(master) ✗ cat ./some_file.txt | nc termbin.com 9999\nhttps://termbin.com/faos\n➜  blog git:(master) ✗\n```\n\nSure enough, after navigating to [`https://termbin.com/faos`](https://termbin.com/faos), we will see the contents of `some_file.txt`. Neat!\n\n### ⚠️Word of Caution⚠️\n\nDo not pipe any personal information, credentials or any other private data into termbin. It will be instantly available to the general public, and theres no quick way to remove it.\n\nHappy Pasting!✨\n","frontmatter":{"title":"Quick Tip! Sharing terminal output with Termbin","date":"2019-12-31","tags":"quick-tip, note"}},{"slug":"patch-based-git-workflow","markdownBody":"\nIf you have ever contributed to an open source project, chances are you have opened a pull request on GitHub or a similar platform to present your code to the maintainers. While this is a very approachable way of getting your code reviewed, some projects have decided against using pull requests and instead accept patches via email.\n\n## An introduction to patches\n\nA patch is essentially a git commit expressed in plain text. It describes what commit the change is based on, and what has changed. A basic patch looks like this:\n\n```\nFrom 92132241233033a123c4fa833449d6a0d550219c Mon Sep 17 00:00:00 2001\nFrom: Bob <bob@example.com>\nDate: Tue, 25 May 2009 15:42:16 +0200\nSubject: [PATCH 1/2] first change\n\n---\n test.txt |    1 +-\n 1 files changed, 1 insertions(+), 1 deletions(-)\n\ndiff --git a/test.txt b/test.txt\nindex 7634da4..270eb95 100644\n--- a/test.txt\n+++ b/test.txt\n@@ -1 +1 @@\n-Hallo Bob\n+Hallo Alice!\n```\n\nAs you can see, it is very readable for both the reviewer and the machine.\n\n## Sending and receiving patches\n\nThe easiest way you can generate a patch from a commit is to use `git-format-patch`:\n\n```\ngit format-patch HEAD^\n```\n\nThis will generate a `.patch` file, that can be embedded into an email and sent to the maintainers. Oftentimes they will then reply to your mail with some inline comments about your code.\n\nTo simplify this process further, git has the `send-email` command, which let's you send the patch directly to someone without needing to embed it manually. I won't go into details about this, but there is a [well written guide](https://git-send-email.io/) on how to set it up.\n\nIf you have received a patch from someone, you can apply it to your tree with the `am` (apply mail) command:\n\n```\ngit am < 0001-first-change.patch\n```\n\ncheck your `git log` to see the patch in form of the latest commit.\n\n## Why even bother\n\nYou might think that this is just a silly and outdated approach to collaborative development. \"Why not simply open a pull request?\" you might ask. Some projects, especially low-level oriented ones like the Linux kernel, do not want to rely on third-party platforms like GitHub to host their code, with good reasons:\n\n1. Everyone can participate! You don't need to register an account on some proprietary website to collaborate in a project that uses a patch-based workflow. You don't even have to expose your identity, if you don't want to. All you need is an email-address, and frankly most of us have one.\n2. It's plain simple! Once you get used to generating and applying patches on the command line, it is in fact easier and faster than opening a pull request in some clunky GUI. It doesn't get simpler than plain text.\n3. It is rewarding! Once you have submitted a patch to a project, there is no better feeling than getting a simple \"Applied, thanks!\" response from a maintainer. And if it's a response that contains feedback rather than an approval, it feels even better to submit that reworked code again and get it eventually applied.\n\n## Conclusion\n\nThe patch-based workflow is an alternative way to collaborate with developers. If it helps you in your day to day business depends on the projects you are contributing to, but in the end it is always good to have many tools under your belt.\n","frontmatter":{"title":"The Patch-Based Git Workflow","date":"2020-09-28","tags":"git, guide, programming"}},{"slug":"lightweight-vpn-with-wireguard","markdownBody":"\nThis blog post has been taken over from my [collection of \"Today I Learned\" articles](https://garrit.xyz/til).\n\nYou can easily set up a private network of your devices. This way you can \"talk\" to your phone, raspberry pi etc. over an **encrypted** network, with simple IP-addresses.\n\n![](https://images.unsplash.com/photo-1505659903052-f379347d056f?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=2550&q=80)\n\nFirstly, install wireguard on all of your systems. Simply install the `wireguard` package from your package manager respectively. Check out [the official installation guide](https://www.wireguard.com/install/) if you can't find the package. If you're on debian, try [this](https://wiki.debian.org/WireGuard?action=show&redirect=Wireguard) guide. There's also an app for Android, iOS and MacOS.\n\nEvery participent (Client and server) needs a key-pair. To generate this, run this command first on the server, and on all clients:\n\n```bash\nwg genkey | tee wg-private.key | wg pubkey > wg-public.key\n```\n\nIt might make sense to do this as root. This way you don't have to type `sudo` with every command.\n\n## Server Configuration\n\nYou will need to create a configuration for the server. Save this template at `/etc/wireguard/wg0.conf`, and replace the fields where needed:\n\n```conf\n[Interface]\nPrivateKey = <Server private key from wg-private.key>\nAddress = 10.0.0.1/24 # IP Address of the server. Using this IP Address, you can assign IPs ranging from 10.0.0.2 - 10.0.0.254 to your clients\nListenPort = 51820 # This is the standard port for wireguard\n\n# The following fields will take care of routing\nPostUp = iptables -A FORWARD -i %i -j ACCEPT; iptables -A FORWARD -o %i -j ACCEPT; iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE\nPostDown = iptables -D FORWARD -i %i -j ACCEPT; iptables -D FORWARD -o %i -j ACCEPT; iptables -t nat -D POSTROUTING -o eth0 -j MASQUERADE\n\n# Laptop\n[Peer]\nPublicKey = <Public Key of Laptop Client>\nAllowedIPs = 10.0.0.2/32 # The client will be reachable at this address\n\n# Android Phone\n[Peer]\nPublicKey = <Public Key of Phone Client>\nAllowedIPs = 10.0.0.3/32\n\n# ...\n```\n\nThen run `wg-quick up wg0` to start the wireguard interface with the configuration from `/etc/wireguard/wg0`.\n\n## Setting up clients\n\nSetting up clients is very similar to the server setup process. Generate a keypair on each client, save the following config to `/etc/wireguard/wg0.conf` and replace the nessessary fields:\n\n```conf\n[Interface]\nPrivateKey = <Client Private Key from wg-private.key>\nAddress = 10.0.0.2/32 # The fixed address of the client. Needs to be specified in the server config as well\n\n[Peer]\nPublicKey = <Server Public key>\nAllowedIPs = 10.0.0.0/24 # Routes all traffic in this subnet to the server. If you want to tunnel all traffic through the wireguard connection, use 0.0.0.0/0 here instead\nEndpoint = <Public Server IP>:51820\nPersistentKeepalive = 25 # Optional. Will ping the server every 25 seconds to remain connected.\n```\n\nOn every client, run `wg-quick up wg0` to start the interface using the config at `/etc/wireguard/wg0.conf`.\n\nThis whole proccess might be easier on GUIs like Android or MacOS.\n\nNow, try to ping your phone from your laptop:\n\n```\nping 10.0.0.3\nPING 10.0.0.3 (10.0.0.3) 56(84) bytes of data.\n64 bytes from 10.0.0.3: icmp_seq=1 ttl=64 time=5382 ms\n64 bytes from 10.0.0.3: icmp_seq=2 ttl=64 time=4364 ms\n```\n\n### References\n\n- [Official Documentation](https://www.wireguard.com/)\n- [https://www.stavros.io/posts/how-to-configure-wireguard/](https://www.stavros.io/posts/how-to-configure-wireguard/)\n","frontmatter":{"title":"Quick Tip! Setting up a lightweight Server-Client VPN with wireguard","date":"2020-08-19","tags":"infrastructure, guide"}},{"slug":"introducing-slashdev-space","markdownBody":"\nHi! Welcome to the new home of my blog. Let me give you a small tour of why I built it, its underlying architecture and my ambitions with this project.\n\nMy old blog was based on [Gatsby.js](https://www.gatsbyjs.com/), a static site generator built on React. Back then, I used a quick and dirty blogging template I stole from the Gatsby themes page. Gatsby themes are essentially npm packages, that you throw in your project as a dependency. While it was super easy to set up, I had a hard time configuring it to my likings, since I relied on a framework someone else has provided.\n\nThe real turning point came, when I tried to build the blog after a few months of not maintaining it. I wasn't able to compile it, since some dependency of the blog template broke. Of course I could have forked the template and fixed it to my likings, but I didn't want to maintain yet another library until the end of my blogs life. You could draw some parallels to propriotary software, where you don't have the chance to look under the hood and see what's wrong, except in this case, it was just me being lazy.\n\n## A new approach\n\nWhat I want is a project that I have full control over. I want to be able to customize styling and add features whenever I want to. Gatsby would have been able to give me all of this, but I have the feeling that it sometimes overcomplicates things too much (Having a GraphQL backend is nice, but do you really need that?). I looked at frameworks like [Hugo](https://gohugo.io/) which offers lightning fast compilation, but with it, I would have been tied to \"the Hugo way\" of templating and configuring the project.\n\nIn the end it was [Next.js](https://nextjs.org/) that caught my attention the most, given how simple it is. There's not much configuration involved in the setup process (although getting it to work with github pages was somewhat tedious). Each component in the `pages/` directory corresponds to a full page on the website. The `public/` directory is served statically. That's really all I needed to build a modular webpage.\n\n## Wiring things up\n\nBecause Next.js is so minimalistic, there are some parts that you have to set up by yourself. Rendering markdown files for example does not come included, it has to be done manually. Thankfully, there are some packages that can do this for you. All you have to do is write the markdown to the specific pages. It basically boils down to this:\n\n```js\nconst posts = ((context) => {\n\tconst keys = context.keys();\n\tconst values = keys.map(context);\n\n\tconst data = keys.map((key, index) => {\n\t\t// Create slug from filename\n\t\tconst slug = key\n\t\t\t.replace(/^.*[\\\\\\/]/, \"\")\n\t\t\t.split(\".\")\n\t\t\t.slice(0, -1)\n\t\t\t.join(\".\");\n\t\tconst value = values[index];\n\t\t// Parse yaml metadata & markdownbody in document\n\t\tconst document = matter(value.default);\n\t\treturn {\n\t\t\tfrontmatter: document.data,\n\t\t\tmarkdownBody: document.content,\n\t\t\tslug,\n\t\t};\n\t});\n\treturn data;\n})(require.context(\"../content/posts\", true, /\\.md$/));\n```\n\nAnother special case was setting up a RSS feed. I basically had to write a simple script that generates the feed from all posts in the `content/posts` directory, let it run during every build and throws the output in the `public/` directory, so that it can be served as `/rss.xml`. You might argue that this is quite a tedious process for such a feature, but it gives me all the flexibility I want over the features of this project.\n\n## Deployment\n\nI've considered self hosting this blog on my server. While that would have been a fun learning-experience, I wanted to stick to the simple deploy-and-forget workflow I was used to from GitHub Actions. Every push to the master branch triggers a full deployment. No manual work required. Doing it this way, I also save the time and energy to set up SSL encryption, plus it is highly scalable (not that I expect a traffic-explosion, but you never know). Setting Next.js up to deploy to GitHub Pages takes some time, because there are some pitfalls that you have to be aware of. [This article](https://dev.to/jameswallis/deploying-a-next-js-app-to-github-pages-24pn) helped me a lot.\n\n## Ambitions\n\nIn the future, I want /dev.space to become more than just a blog. I want it to become a platform for my thoughts and ideas. I'm also playing with the idea to migrate my main website (https://garrit.xyz) over to /dev.space, therefore made sure that my current setup is very future-proof and can be easily extended.\n\nFeel free to dig through the source code for this project. You can find it on my GitHub: https://github.com/garritfra/slashdev.space\n\nLet me know if there is anything that you miss on this blog. Searchable posts? Dark-mode? This is only the beginning of /dev.space!\n","frontmatter":{"title":"Introducing my new blog - slashdev.space","date":"2020-10-08","tags":"meta"}},{"slug":"fighting-array-functions-with-es6","markdownBody":"\nYesterday, I came across an interesting bug regarding JavaScript Arrays, and I wanted to share my approach to fixing it.\nAt a basic level, I wanted to pass part of an array to a function, but wanted to use the original array later on.\n\n```js\nlet arr = [1, 2, 3, 4, 5]\nlet something = arr.splice(0, 3)\ndo(something) // []\nDoSomethingWithOriginal(arr)\n```\n\nThinking that Array.prototype functions don’t mutate the array directly, I moved on with my day. This lead to a bunch of problems down the line.\nSome array methods in the EcmaScript specification are designed to mutate arrays, while others do not.\n\n### Non-mutating functions\n\n- Array.prototype.map()\n- Array.prototype.slice()\n- Array.prototype.join()\n- …\n\nThese functions do not mutate the array they are called on. For example:\n\n```js\nlet arr = [1, 2, 3, 4, 5];\nlet partOfArr = arr.slice(1, 2);\nconsole.log(partOfArr); // [2, 3]\nconsole.log(arr); // [1, 2, 3, 4, 5]\n```\n\n### Mutating functions\n\n- Array.prototype.sort()\n- Array.prototype.splice()\n- Array.prototype.reverse()\n- …\n\nThese methods mutate the array directly. This can lead to unreadable code, as the value can be manipulated from anywhere. For example:\n\n```js\nlet arr = [5, 2, 4];\narr.sort();\nconsole.log(arr); // [2, 4, 5]\n```\n\nTo me, it is very unclear, which functions do, and which don’t mutate arrays directly. But, there’s a simple trick you can use to stop letting the functions mutate arrays directly, ultimately leading to more readable and reliable code.\n\n## Enter: The ES6 Spread Operator!\n\n![Spread Operator](https://images.unsplash.com/photo-1518297056586-889f796873e0?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1225&q=80)\n\nTake a look at this snippet:\n\n```js\nlet arr = [3, 5, 1, 2, 4];\nlet sorted = [...arr].sort();\nconsole.log(arr); // [3, 5, 1, 2, 4]\nconsole.log(sorted); // [1, 2, 3, 4, 5]\n```\n\nVoilà! We have a sorted array, and the original one is also around. The spread operator(`[...arr]`) is used to create a new array with every value of arr .\nYou can use this for arrays, as well as objects:\n\n```js\nlet obj = {\n\tfield: \"example\",\n};\nlet extendedObj = {\n\t...obj,\n\tanotherField: 42,\n};\nconsole.log(extendedObj.field); // \"example\"\n```\n\n## Conclusion\n\nES6 brought us awesome features like let and const assignments, as well as arrow functions. A more unknown feature however is the spread operator. I hope you now know how to use the spread operator, and that you can adopt it for cleaner and simpler code.\n","frontmatter":{"title":"Fighting Array Functions with ES6","date":"2019-04-07","tags":"javascript, guide, programming, web"}},{"slug":"_2022-12-07-about-adhd","markdownBody":"\nI was that kid who would would throw pencils at classmates, hit a random guy in\nthe balls at the supermarket and throw large rocks through the air until they\nhit someones head. Thankfully I've been diagnosed with ADHD at a very early\nstage in my life, which gave me the opportunity to learn a lot about ADHD, and\nhow to cope with it. It hasn't always been easy, and I'm still very much\nstruggling with it in my day to day life, but just being aware that I have ADHD\nand knowing how to deal with it in different situations is invaluable to me.\n\nAlthough I don't have a problem talking about my ADHD, I did have to learn when\nto avoid this topic. Some people start to question everything I do and say when\nthey learn that the reason I nibble on my fingers is my ADHD. Most people\nhowever don't even notice that I have ADHD until they see me taking medication\nafter a sleepover.\n\nBut the true battle with ADHD takes place in my head. I just came across [this\nvideo](https://www.youtube.com/watch?v=uMK4gdR7c18), which perfectly outlines\nwhat's going on in my head. \n\n## Seeking an optimal stimulus\n\n## Task Paralysis\n","frontmatter":{"title":"About my ADHD","date":"2022-12-07","tags":"note, 100DaysToOffload, life"}},{"slug":"_2022-11-28-mistakes-i-made-writing-my-first-compiler,-and-what-i-would-do-better-next-time","markdownBody":"\nMy most ambitious side project so far has been the [Antimony Programming\nLanguage](https://github.com/antimony-lang/antimony). It's a compiler for a\nprogramming language I made up about two years ago. It can be used for basic\nprograms and algorithms, but anything beyond\n[bubblesort](https://github.com/antimony-lang/antimony/blob/master/examples/bubblesort.sb)\nand the [ackermann\nfunction](https://github.com/antimony-lang/antimony/blob/master/examples/ackermann.sb)\nwasn't feasible.\n\nUnfortunately, the language never grew out of the \"toy\" phase, so I more or less\nabandoned it. Looking back, I definitely learned a lot about compilers, and if I\nwere to write another language (which I will most likely do in the future),\nthere are some things that I would do better next time.\n\n## Statements are expressions minus flexibility\n\nThe syntax of many C-like programming languages can be broken up into two\ncategories. An `Expression` is anything that resolves a value.\n\n```c\n1 + 2       // 3\n5 * 8 + 2   // 42\n1 < 2       // true\naddOne(1)   // 2 (assuming `addOne` returns the given value plus one)\n[1, 2, 3]   // Array containing a 1, a 2 and a 3\n```\n\nA `Statement` resembles an action or branch in a program.\n\n```c\nint foo() {                 // Declare statement\n    int bar = 1;            // Assignment statement\n    if (bar >= 1) {         // Conditional statement\n        printf(\"%d\", bar);  // Function call expressions can be statements too!\n    }\n}\n```\n\nAntimony picks up on this concept, and strictly separates statements from\nexpressions. Some modern programming languages (E.g.\n[Rust](https://rust-lang.org/)) got rid of most types of statements by just\ndeclaring almost anything an expression. There are just [Declaration\nStatements](https://doc.rust-lang.org/reference/statements.html#declaration-statements)\nused for function and variable declaration, and [Expression\nStatements](https://doc.rust-lang.org/reference/statements.html#expression-statements),\nwhich can contain any arbitrary expression. This allows us to introduce a lot of\nnice features that are very hard to implement if we treated statements as actual statements\n\n```rust\nlet foo = if 1 > 2 {\n    1\n} else {\n    for {\n        break 2\n    }\n}\n\nprint(foo)  // 2\n```\n\nUnfortunately, I realized the benefits of extensively using expressions too late.\n\nTODO\n\n## Don't add types as an afterthought\n\n## Multiple backends = exponential headaches\n\n## \"I won't need an IR\" is a lie\n","frontmatter":{"title":"Mistakes I made writing my first compiler, and what I would do better next time","date":"2022-11-28","tags":"note, 100DaysToOffload"}},{"slug":"_2022-05-02-docker-in-plain-english","markdownBody":"\nRecently I saw some fellow Mastodon-users discussing resources and guides to get\ninto the docker ecosystem. Given that most of my private infrastructure is built\nupon docker and docker-compose, I thought I'd share how **I** use this tool. I\nwill try to assume no prior container-knowledge, but if anything isn't clear to\nyou, feel free to [contact me](/contact).\n\n## Docker 101\n\nFirst up: What on earth is Docker, and why should I use it?\n\nDocker is a _container runtime_. It can be used to isolate system resources in a\nreproducible manner, meaning if I containerize an application on my machine, I\ncan be sure that it will function exactly the same on all machines. The benefits\nof this are obvious: You more or less eliminate all dependencies to a specific\nenvironment, like the operating system and other software. As long as it's the\nsame CPU-architecture, this sentence holds true: If it runs docker, it can run\nyour application.\n\nThings running in a container also can't break out of this \"sandbox\". A process\nin a container is only aware of the resources around it, not on the host\nmachine. Each container is kind of like an operating system **inside** your\nactual operating system.\n\nTo describe what a container should look like, we need to write a \"recipe\" for\nit. In it, you describe a starting point from which you want to build upon, and\nthe necessary steps to achieve the desired state. This \"recipe\" is called a\n`Dockerfile`. A very simple Dockerfile might look like this:\n\n```\nFROM ubuntu\n\nRUN apt update && apt upgrade -y\n\nCMD [\"echo\", \"Hello World!\"]\n```\n\nIf you now run `docker build -t hello-world .`, docker will take this recipe and\nbuild an **image** called \"hello-world\". This image is a template that describes\nthe state of your application. In our case, we take the definition provided by\nthe \"ubuntu\" image and simply do a system update. Whenever you spawn a container\nfrom this image, it will always start from exactly this state. Note that the\ncommands in the Dockerfile do not run every time you launch a container! An\nimage is the **result** of running the commands. The final instruction, `CMD`,\nis the command to run whenever you spawn a container, but more on that later.\n\nCongrats! You just built your very first docker image. To verify that it's\nactually there, try running `docker image ls`. This will list all images on your\nsystem:\n\n```\n➜  garrit.xyz git:(master) ✗ docker image ls\nREPOSITORY               TAG             IMAGE ID       CREATED          SIZE\nhello-world              latest          6e2240011a89   8 minutes ago    109MB\n```\n\nAn image doesn't really do anything on its own. You need to tell docker to\nconstruct a container out of that image. A container is essentially an\n**instance** of that image. Try running this command:\n\n```\ndocker run hello-world\n```\n\nAnd, as instructed with the `CMD` line, you should see the words \"Hello World!\"\nprinted on the screen. You can verify that it's still there by running `docker ps -a`, which will list all containers on your system, including the one you\njust ran:\n\n```\nCONTAINER ID   IMAGE                    COMMAND                  CREATED          STATUS                      PORTS                                            NAMES\n05415bf66a91   hello-world              \"bash\"                   3 seconds ago    Exited (0) 2 seconds ago\n```\n\n\"This isn't really helpful!\", I hear you yell. You're right, so let's look at a\nreal world example.\n\n## Example: A simple Node.js application\n\nA real world use case for a docker container is run your home-built application.\nSay we have a basic Node.js app that we wanted to containerize:\n\n```\n.\n├── app.js\n├── package-lock.json\n└── package.json\n```\n\nAnd your main setup-workflow for this application looks something like this:\n\n```\nnpm install\nnpm start\n```\n\nRemember that a Dockerfile is a **recipe** of how an application is built. A\ncorresponding recipe could look like this:\n\n```Dockerfile\n# Declare base image\nFROM node:16\n\n# Copy the application into the container\nCOPY . .\n\n# Install dependencies\nRUN npm install\n\n# Launch the application\nCMD [\"npm\", \"start\"]\n```\n\nLike above, you can build this Dockerfile using `docker build -t testapp .`, or\nany name you'd like to use.\n\n> **Quick Tip**: You might also want to add a `.Dockerignore` file, which lists\n> files and directories which should not be copied inside the container, just like\n> a `.gitignore` file. I usually add `node_modules` since it will be recreated\n> when building the image, and some files that are not relevant at runtime, like a\n> README.\n\nRunning `docker image ls` should now show the image you just created:\n\n```\nREPOSITORY               TAG             IMAGE ID       CREATED             SIZE\ntestapp                  latest          463e68d86eee   5 minutes ago       857MB\n```\n\nYou can now \"run\" the image, which will result in a spawned container. Since\nContainers run in their own environment, they won't be able to receive any\nkeystrokes by default, so you can't stop the application. To fix this, you can\nuse the `-it` flags, which will establish an interactive session to the process\ninside the container. This makes it easier to stop the container after it is\ncreated:\n\n```\ndocker run -it testapp\n```\n\nAnd voila! You should see the output of your application in the terminal. If\nyou've done some Node.js, this output might be familiar:\n\n```\n➜  testapp git:(master) ✗ docker run -ti testapp\n\n> testapp@1.0.0 start\n> node app.js\n\nExample app listening at http://:::8080\n```\n\nYou'll soon discover that you can't access port 8080 on your machine. Docker has\na powerful networking engine, and each container has its own IP. You _could_\nfigure out the IP of your container and access it like that. A simpler approach\nthough is to just bind a port of your host machine to the container. For\nexample, let's bind our port 4000 to port 8081 of the container. This can be\ndone using the `-p` flag of the cli:\n\n```\ndocker run -p 4000:8081 -it testapp\n```\n\n> **Quick Tip**: To remember the order of the container- and the host-port, I\n> always think of the container as laying on my desk. First, I grab the cable (the\n> host machine) and then plug it into the container. Weird analogy, I know. But it\n> really helped me make sense of this!\n\nIf you now access `http://localhost:4000` on your host machine, you should see\nyour application!\n\n## Docker Compose 101\n\nNow that we've looked at the basics of Docker, let's talk about Docker Compose.\nDocker Compose is a tool that allows you to define and manage multi-container\napplications. This means you can use a single docker-compose.yml file to define\nthe services and dependencies of your application, and then use Docker Compose\nto start and stop all of the containers at once.\n\nUsing Docker Compose can save you a lot of time and hassle, especially if you\nhave a complex application with multiple components that need to work together.\nWith Docker Compose, you can specify the dependencies between your containers,\nas well as the ports, volumes, and other settings that they need to run\nproperly. This makes it much easier to manage and maintain your application, and\nallows you to make changes to your environment quickly and easily.\n\nHere is an example docker-compose.yml file for a simple Node.js application:\n\n```yaml\nversion: '3'\nservices:\n  app:\n    build: .\n    ports:\n      - 3000:3000\n    volumes:\n      - .:/usr/src/app\n    command: npm start\n```\n\nIn this file, we define a single service called \"app\" that uses the Dockerfile\nin the current directory to build an image. We then map port 3000 on the host\nmachine to port 3000 on the container, mount the current directory as a volume,\nand specify the command to run when the container is started.\n\nTo start the containers defined in this docker-compose.yml file, you can run the following command:\n\n```\ndocker-compose up\n```\n\nThis will build the images, create the containers, and start all of the\nservices. You can then access your application at http://localhost:3000.\n\nTo stop the containers and remove them, you can run the following command:\n\n```\ndocker-compose down\n```\n\nThis will stop and remove the containers, as well as the networks and volumes\nthat we've created.\n\n## How I deploy my services\n\n- Walkthrough of a simple deployment (miniflux?)\n- Traefik\n- Local volumes\n- Permissions\n\n## Conclusion\n\n- Image size optimizations\n\nThis is post 030 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Docker in plain english","date":"2022-05-02","tags":"guide, docker, infrastructure, 100DaysToOffload"}},{"slug":"_2021-01-25-kotlin-refactor-strategies","markdownBody":"\nThe last couple of days, I had the opportunity to refactor some of the Kotlin code in our massive android project. The app is quite old, and a lot of legacy code has accumulated over the years. At one point, we decided to migrate a lot of our view-logic from Java to Kotlin. Instead of cleaning the code up, this lead to an even bigger buildup of technical debt. The problem was that we mainly used the migration-tools that Android Studio provides, which, at that time where not very sophisticated. Sometimes we had to manually fix some things which, since it is such a massive codebase, we didn't really take the time to clean every converted file up.\n\nOnce the number of warnings approached infinity, we finally decided to dedicate a chunk of each sprint to house-cleaning in our codebase. My recent refactoring involved cleaning up unsafe usages, and I want to share some of my approaches to this kind of refactoring.\n\n## Obvious fixes\n\nI encountered this kind of pattern a number of times during my refactoring:\n\n```kt\ntextView!!.setOnClickListener { showDialog() }\n```\n\nIf you see something like this, your alarm bells should immediately ring. If textView is `null`, the app would crash. The same behavior is happening with java, so a switch from Java to kotlin would be pointless.\n\nThe simple fix is to replace the `!!` operator with a `?`, which only executes the line of code if the element is not null.\n\n```kt\ntextView?.setOnClickListener { showDialog() }\n```\n\nOf course, if the view _is_ null even though you're expecting a value, you should probably do something about this. A simple way to handle this is to add some \"sanity checks\" to your code. Create an `assert` function that throws an `AssertionError` if a given condition is false. Unfortunately, android does not use the builtin `assert` function that kotlin provides, so you will have to write your own. The neat thing about `assert` is that you can toggle this behavior, meaning you can enable assertion errors in the debug build, but disable it in the final production build to prevent the app from randomly crashing.\n\n```kt\nassert(textView != null)\ntextView?.setOnClickListener { showDialog() }\n```\n\n## More complex cases\n\nLet's take a look at this code: ...\n\nThis is post 007 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Fixing nullability issues when migrating to Kotlin","date":"2021-01-23"}},{"slug":"_2021-01-22-library-of-babel","markdownBody":"\nWhat if I told you that everything you ever have said, thought and wrote, and everything you will say, think or write, has already been written down? Let me share my discovery of a construct called the _library of babel_.\n\nThe library of babel is a\n\n## Real world use cases\n\nThis is post 006 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"The library of babel","date":"2021-01-22"}},{"slug":"_2021-01-14-diy-software","markdownBody":"\nWe often take free software as granted. We think of many programs are a \"black box\" that are supposed to _just work_. I think, this goes against the very nature of free and open source software.\n\nThis is post 004 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"DIY Software","date":"2021-01-14"}},{"slug":"2023-03-11-a-software-requirements-checklist","markdownBody":"\r\nI just found a [great post](https://www.etsy.com/codeascraft/a-checklist-manifetsy) on the Etsy Engineering blog suggesting a possible checklist for new product requirements. In reality, this checklist is very hard to fulfill, but it's a nice reminder of what a well thought out requirement could look like.\r\n\r\n### Scope\r\n\r\n- Is the feature meant to be very polished and finished or are we just trying to get user feedback as an MVP?\r\n- If we are running a MVP, is the current feature a true MVP? How can we simplify or cut scope?\r\n\r\n### Eligibility\r\n\r\n- What populations should be included or excluded from the experiment? When should users see this feature? (Which pages, signed in/signed out, mobile, desktop, etc.)\r\nWhere/when should bucketing occur?\r\n- Will the experiment conflict with any other experiments? Do the experiments need to run exclusively?\r\n- What countries should the experiment run in (can impact translations)?\r\n\r\n### A11Y\r\n\r\n- Is there any special accessibility work this feature will require? If extra work is anticipated, check in early with our a11y team.\r\n- When testing and developing we should keep two users in mind - a keyboard user and a voice over user, do we need to add other code for these users?\r\n\r\n### Translations\r\n\r\n- Are there any strings to be translated that should be submitted ASAP?\r\n- Do we need to translate any labels for a11y?\r\n\r\n### Observability\r\n\r\n- How will we know that the feature is working? Are there existing graphs we can use or do we need new ones?\r\n- Should any of these metrics have a threshold or alerting?\r\n- Are we missing any key events to obtain user feedback?\r\n- How will we compare our control and variant?\r\n\r\n### Performance\r\n\r\n- Is there anything in my experiment that could degrade performance of the site?\r\n- Do I need an operational experiment to verify that I’m not impacting performance?\r\n\r\n### Error States\r\n\r\n- Do we have designs for loading states?\r\n- Do we have designs for unsuccessful requests and error handling?\r\n- Do we have informative logging when there are errors?\r\n\r\n### QA\r\n\r\n- What set of browsers and devices should we test our new feature against?\r\n- Which user perspectives do we need to test?\r\n\r\n### Ramping\r\n\r\n- What will our ramping strategy be?\r\n\r\n---\r\n\r\nCheck out the [original post](https://www.etsy.com/codeascraft/a-checklist-manifetsy) for a full writeup and the intentions behind this checklist.\r\n\r\n---\r\n\r\nThis is post 056 of [#100DaysToOffload](https://100daystooffload.com/).\r\n","frontmatter":{"title":"A software requirements checklist","date":"2023-03-11","tags":"100DaysToOffload, note, quote, practices"}},{"slug":"2023-03-10-debugging-ecs-tasks","markdownBody":"\r\nI just had to debug an application on AWS ECS. The whole procedure is documented in more detail in the [documentation](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-exec.html), but I think it's beneficial (both for my future self and hopefully to someone out there) to write down the proccess in my own words.\r\n\r\nFirst of all, you need access to the cluster via the [CLI](https://aws.amazon.com/de/cli/). In addition to the CLI, you need the [AWS Session Manager plugin for the CLI](https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with-install-plugin.html). If you're on MacOS, you can install that via [Homebrew](https://formulae.brew.sh/cask/session-manager-plugin):\r\n\r\n```\r\nbrew install --cask session-manager-plugin\r\n```\r\n\r\nNext, you need to allow the task you want to debug to be able to execute commands. Since I'm using Terraform, this was just a matter of adding the [`enable_execute_command`](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/ecs_service#enable_execute_command) attribute to the service:\r\n\r\n```tf\r\nresource \"aws_ecs_service\" \"my_service\" {\r\n  name            = \"my-service\"\r\n  cluster         = aws_ecs_cluster.my_cluster.id\r\n  task_definition = aws_ecs_task_definition.my_task_definition.id\r\n  desired_count   = var.app_count\r\n  launch_type     = \"FARGATE\"\r\n  enable_execute_command = true # TODO: Disable after debugging\r\n}\r\n```\r\n\r\nYou may also need specify an execution role in the task definition:\r\n\r\n```tf\r\nresource \"aws_ecs_task_definition\" \"my_task_definition\" {\r\n  family              = \"my-task\"\r\n  task_role_arn       = aws_iam_role.ecs_task_execution_role.arn\r\n  execution_role_arn  = aws_iam_role.ecs_task_execution_role.arn  # <-- Add this\r\n}\r\n```\r\n\r\nMake sure that this role has the correct access rights. There's a nice [troubleshooting guide](https://aws.amazon.com/de/premiumsupport/knowledge-center/ecs-error-execute-command/) going over the required permissions.\r\n\r\nIf you had to do some modifications, make sure to roll out a new deployment with the fresh settings:\r\n\r\n```\r\naws ecs update-service --cluster my-cluster --service my-service --force-new-deployment\r\n```\r\n\r\nNow, you should be able to issue commands against any running container!\r\n\r\n```\r\naws ecs execute-command --cluster westfalen --task <task-id-or-arn> --container my-container --interactive --command=\"/bin/sh\"\r\n```\r\n\r\nI hope this helps!\r\n\r\n---\r\n\r\nThis is post 055 of [#100DaysToOffload](https://100daystooffload.com/).\r\n","frontmatter":{"title":"Debugging ECS Tasks","date":"2023-03-10","tags":"100DaysToOffload, infrastructure, aws, guide, note, terraform"}},{"slug":"2023-03-08-terraform-and-kubernetes-are-fundamentally-different","markdownBody":"\r\nOn the surface, Infrastructure as Code tools like [Terraform](https://www.terraform.io/) or [CloudFormation](https://aws.amazon.com/de/cloudformation/) may seem to behave similar to [Kubernetes](https://kubernetes.io/) YAMLs, but they are in fact fundamentally different approaches to cloud infrastructure.\r\n\r\nTerraform tries to provide a declarative way to express imperative actions. If you tell Terraform that you need an EC2 instance, it will notice that no such resource exists and instruct the AWS API to create one. If you don't need the instance anymore and remove the resource definition from your code, Terraform will also pick that up and instruct the AWS API to delete the instance. This works well in most cases, but every once in a while the declarative state may get out of sync with the real world, resulting in errors that are hard to debug and resolve.\r\n\r\nKubernetes on the other hand is a fully declarative system. In a [previous post](/posts/2022-09-22-kubernetes-is-a-domain-specific-database) I touched on how Kubernetes constantly compares the *desired* state with the *actual* state of the resources and tries to match the two. Although it is theoretically possible to issue imperative actions, Kubernetes is built from the ground up to be declarative.\r\n\r\n---\r\n\r\nThis is post 054 of [#100DaysToOffload](https://100daystooffload.com/).\r\n","frontmatter":{"title":"The fundamental difference between Terraform and Kubernetes","date":"2023-03-08","tags":"note, infrastructure, terraform, kubernetes, 100DaysToOffload"}},{"slug":"2023-03-04-pods-vs.-containers","markdownBody":"\r\nIn Kubernetes, pods and containers are often confused. I found a [great article](https://iximiuz.com/en/posts/containers-vs-pods/) going over the differences of the two terms.\r\n\r\n> Containers and Pods are alike. Under the hood, they heavily rely on Linux namespaces and cgroups. However, Pods aren't just groups of containers. A Pod is a self-sufficient higher-level construct. All pod's containers run on the same machine (cluster node), their lifecycle is synchronized, and mutual isolation is weakened to simplify the inter-container communication. This makes Pods much closer to traditional VMs, [bringing back the familiar deployment patterns like sidecar or reverse proxy](https://www.mirantis.com/blog/multi-container-pods-and-container-communication-in-kubernetes/).\r\n\r\nIn my own words: Containers are made up of Linux namespaces and cgroups. Pods can be thought of as a cgroup of cgroups (though not really), mimicing the behavior of a virtual machine that runs multiple containers with a synchronized lifecycle. The containers in a pod are losely isolated, making it easy to communicate between each other. Containers in a pod can however set individual resource requests, enabled by Linux namespaces.\r\n\r\nI'd highly encourage you to check out [the original article](https://iximiuz.com/en/posts/containers-vs-pods/) if you want to learn more about this topic.\r\n\r\n---\r\n\r\nThis is post 053 of [#100DaysToOffload](https://100daystooffload.com/).\r\n","frontmatter":{"title":"Pods vs. Containers","date":"2023-03-04","tags":"note, kubernetes, infrastructure, docker, 100DaysToOffload"}},{"slug":"2023-03-03-notes-on-containerizing-php-applications","markdownBody":"\r\nI was recently tasked with building a rudimentary infrastructure for a PHP application. Coming from a Node.js-driven world where every human and their grandmother has a blog post about containerizing your application, it was very interesting to see where PHP differs to other applications.\r\n\r\nOne major gotcha for me was that PHP code is executed on **request-time**, meaning a new process is spawned for each incoming request. Most other languages have dedicated runtimes that handle incoming requests. This unique approach is very flexible and scalable, but it comes with the implication that there is a **separate webserver** that calls into the PHP interpreter when it needs to.\r\n\r\nIn Node.js (and most other languages), you can \"just run the app\", as demonstrated by this Dockerfile:\r\n\r\n```dockerfile\r\nFROM node:18.14.2-alpine3.17 AS build\r\n\r\nWORKDIR /usr/src/app\r\n\r\nCOPY package*.json ./\r\n\r\nRUN npm ci\r\n\r\nCOPY . .\r\n\r\nEXPOSE 3000\r\n\r\nCMD [ \"node\", \"server.js\" ]\r\n```\r\n\r\nPHP on the other side is rarely used on its own. Most of the time, it needs a webserver alongside it:\r\n\r\n```dockerfile\r\nFROM php:8.1-apache-bullseye\r\n\r\n# <snip>\r\n\r\nCOPY . /var/www/html\r\nWORKDIR /var/www/html\r\n\r\n# <snip>\r\n```\r\n\r\nAs you can see, I'm using the official PHP docker image. The PHP maintainers know that adding a webserver alongside PHP is a very common pattern, so most of the variants of the image ship with a webserver. In this example I'm using Apache, but we might as well use NGINX or some other webserver. There's also the option to use [FPM](https://www.php.net/manual/de/install.fpm.php) as a FastCGI implementation and a webserver in a **separate** container.\r\n\r\nGrasping this took me some time, but after it clicked it made many things a lot clearer.\r\n\r\n## More complete Dockerfile example\r\n\r\nThe Dockerfile above is meant to demonstrate how PHP applications differ from other languages. The following is a more complete example you can use to containerize your PHP application. In this case it’s a Laravel app, so your mileage may vary.\r\n\r\n```dockerfile\r\nFROM php:8.1-apache-bullseye\r\n\r\nRUN apt-get clean && \\\r\n    apt-get update && \\\r\n    apt-get install --fix-missing -y \\\r\n        zip && \\\r\n    docker-php-ext-install \\\r\n        pdo \\\r\n        pdo_mysql \\\r\n        bcmath\r\n\r\nCOPY --from=composer:2 /usr/bin/composer /usr/bin/composer\r\n\r\nCOPY . /var/www/html\r\nWORKDIR /var/www/html\r\n\r\nENV APACHE_DOCUMENT_ROOT /var/www/html/public\r\n\r\nRUN composer install --no-dev --optimize-autoloader --no-interaction && \\\r\n    sed -ri -e 's!/var/www/html!${APACHE_DOCUMENT_ROOT}!g' /etc/apache2/sites-available/*.conf && \\\r\n    sed -ri -e 's!/var/www/!${APACHE_DOCUMENT_ROOT}!g' /etc/apache2/apache2.conf /etc/apache2/conf-available/*.conf && \\\r\n    php artisan config:cache && \\\r\n    php artisan view:cache && \\\r\n    php artisan route:cache && \\\r\n    php artisan storage:link && \\\r\n    chmod 777 -R /var/www/html/storage/ && \\\r\n    chown -R www-data:www-data /var/www/ && \\\r\n    a2enmod rewrite\r\n```\r\n\r\n---\r\nThis is post 052 of [#100DaysToOffload](https://100daystooffload.com/).\r\n","frontmatter":{"title":"Notes on containerizing PHP applications","date":"2023-03-03","tags":"note, infrastructure, docker, php, 100DaysToOffload"}},{"slug":"2023-02-24-visual-distractions","markdownBody":"\r\nEverywhere we look, we're bombarded with flashy symbols trying to grab our attention. This is even the case where we **think** that we're in control of what we're looking at. I made two simple changes that reduce visual distractions in my life.\r\n\r\n## Android App Icons\r\n\r\nApp icons play a serious role in how we interact with our phone. Over the years, there has been a constant battle for the most flashy icon on our home screen. But there's a cure: newer versions of Android [let you choose a color theme for apps that implement it](https://www.lifewire.com/change-color-of-apps-on-android-phones-5213663). It's by far not supported by every app out there, but in my case 90% of the app icons now have the same color. I feel way more comfortable looking at my phone, knowing that less things are trying to grab my attention right when I unlock my phone.\r\n\r\nWith this change, I found that I am more mindful about what app icon I tap on, since I was used to each icon having a different color. This makes it harder for my muscle memory to develop bad habits.\r\n\r\n## RSS-Reader Favicons\r\n\r\nIf you're using an RSS reader, chances are you're used to seeing a favicon next to the articles. I had the feeling that I was drawn more towards the favicon than the headline of the article, so I started looking for ways to disable favicons all together.\r\n\r\nMiniflux provides a way to override the stylesheet of the feed in the settings. Simply append the following code-snippet and the favicons will be history:\r\n\r\n```css\r\n.item-title img, .entry-website img {\r\n  display: none;\r\n}\r\n```\r\n\r\nOf course every reader is different, so you might want to look into the documentation of your reader of choice.\r\n\r\n## Conclusion\r\n\r\nThese changes might seem insignificant, but I found that they made a huge difference in how I interact with my phone. The suggestions above might not apply to your life, but I'd like to encourage you to keep an eye out for unnecessary visual distraction in your life. Try to avoid it as much as possible.\r\n\r\n---\r\n\r\nThis is post 051 of [#100DaysToOffload](https://100daystooffload.com/).\r\n","frontmatter":{"title":"Visual Distractions","date":"2023-02-24","tags":"note, guide, life, 100DaysToOffload"}},{"slug":"2023-02-22-dockerignore-troubles","markdownBody":"\r\nI commonly used to create a `.Dockerignore` file next to my `Dockerfile`. After countless hours of ignoring the problems in my setup, I found out that the uppercase `.Dockerignore` doesn't get picked up by Docker on MacOS. Only the lowercase `.dockerignore` is valid.\r\n\r\nI didn't find official documentation on this, but I think it's because both MacOS and Linux are case-sensitive, while Windows isn't. I don't remember why I got used to the `.Dockerignore` convention, but I swear I saw someone using it in the wild. Or it's my (un)logical reasoning that, because `Dockerfile` is uppercased, `.Dockerignore` should be uppercased as well.\r\n\r\nEither way, stay away from `.Dockerfile`s and stick to `.dockerfile`s.\r\n\r\nThis is post 050 of [#100DaysToOffload](https://100daystooffload.com/).\r\n","frontmatter":{"title":"Dockerignore troubles","date":"2023-02-22","tags":"note, docker, web"}},{"slug":"2023-02-21-what's-next-for-modern-infrastructure","markdownBody":"\r\nModern infrastructure is incredibly complex. I identified 4 main \"levels\" of infrastructure abstraction:\r\n\r\n## Level 1: A website on a server\r\n\r\nThis is the most straight forward way to host a website. A webserver hosted on bare metal or a VM.\r\n\r\n## Level 2: Multiple servers behind a load balancer\r\n\r\nAt this stage, you start treating servers as cattle rather than pets. Servers may be spun up and down at will without influencing the availability of the application.\r\n\r\n## Level 3: An orchestrated cluster of servers\r\n\r\nInstead of a server serving a specific purpose (e.g. webserver, DB server, etc.), a server becomes a worker for arbitrary workloads (see Kubernetes, ECS).\r\n\r\n## Level 4: Multicluster service mesh\r\n\r\nIf an organization manages multiple clusters (e.g. multiple application teams), they can be tied together into a [service mesh](https://istio.io/latest/docs/reference/glossary/#service-mesh) to better optimize communication and observability.\r\n\r\n## Level 5: ???\r\n\r\nHistory shows that we never stop abstracting. Multicluster service meshes are about the most abstract concept many people (including myself) can comprehend, but I doubt that this is the end of this journey. So, what's next for modern infrastructure?\r\n\r\nThis is post 049 of [#100DaysToOffload](https://100daystooffload.com/).\r\n","frontmatter":{"title":"What's next for modern infrastructure?","date":"2023-02-21","tags":"note, infrastructure, kubernetes"}},{"slug":"2023-02-20-what-problem-does-kubernetes-solve","markdownBody":"\r\nThis is a common question that many people (including me) ask themselves.\r\n\r\nI recently came across a great [post](https://blog.adamchalmers.com/kubernetes-problems/) which explains the problem really well:\r\n\r\n> Kubernetes exists to solve one problem: how do I run m containers across n servers?\r\n\r\nThe post also nails the answer to **how** Kubernetes solves this problem:\r\n\r\n> It's a big abstract virtual computer, with its own virtual IP stack, networks, disk, RAM and CPU. It lets you deploy containers as if you were deploying them on one machine that didn't run anything else. Clusters abstract over the various physical machines that run the cluster.\r\n\r\nI'd highly encourage you to read through the article if you want to learn more about why Kubernetes exists.\r\n\r\nThis is post 048 of [#100DaysToOffload](https://100daystooffload.com/).\r\n\r\n","frontmatter":{"title":"What problem does Kubernetes solve?","date":"2023-02-20","tags":"note, kubernetes, infrastructure"}},{"slug":"2023-02-19-til-about-css-insets","markdownBody":"\r\nJust a quick tip that I thought is worth sharing. Instead of declaring:\r\n\r\n```css\r\n.foo { \r\n  top: 0;\r\n  right: 0;\r\n  bottom: 0;\r\n  left: 0;\r\n}\r\n```\r\n\r\nyou can just use:\r\n\r\n```css\r\n.foo {\r\n  inset: 0;\r\n}\r\n```\r\n\r\nIt's supported everywhere computers are sold!\r\n\r\nMDN: https://developer.mozilla.org/en-US/docs/Web/CSS/inset\r\n\r\nSource: https://front-end.social/@estelle/109878532782943511\r\n\r\nThis is post 047 of [#100DaysToOffload](https://100daystooffload.com/).\r\n","frontmatter":{"title":"TIL about CSS Insets","date":"2023-02-19","tags":"web, note, til, quick-tip, css, 100DaysToOffload"}},{"slug":"2023-02-14-openssf-best-practices","markdownBody":"\nThe Open Source Security Foundation (OpenSSF) provides [a list of best\npractices](https://bestpractices.coreinfrastructure.org/en/criteria/0) for open\nsource projects. Although this list is tailored towards free and open source\nprojects, I believe that this list is valuable for *all* software projects.\nHere's a breakdown of all practices that I consider generic to all projects, no\nmatter its license, alongside some personal notes.\n\n## Basics\n\n### Basic project website content\n\nThe project website MUST succinctly describe what the software does (what problem does it solve?).\n\n> A website might not always apply, but a README is a good place to put this information.\n\nThe information on how to contribute MUST explain the contribution process (e.g., are pull requests used?)\n\n> This information is also best placed in the README.\n\nThe information on how to contribute SHOULD include the requirements for acceptable contributions (e.g., a reference to any required coding standard)\n\n> -> README.\n\n### Documentation\n\nThe project MUST provide basic documentation for the software produced by the\nproject.\n\nThe project MUST provide reference documentation that describes the external\ninterface (both input and output) of the software produced by the project.\n\n> I wouldn't consider reference documentation a requirement, but it's nice to\nhave.\n\n### Other\n\nThe project sites (website, repository, and download URLs) MUST support HTTPS using TLS.\n\n## Change Control\n\n### Public version-controlled source repository\n\nThe project MUST have a version-controlled source repository ~~that is publicly readable and has a URL~~.\n\nThe project's source repository MUST track what changes were made, who made\nthe changes, and when the changes were made.\n\nTo enable collaborative review, the project's source repository MUST include interim versions for review between releases; it MUST NOT include only final releases.\n\n> In some cases, code can't or shouldn't be versioned. For most website projects,\nreview environments in merge requests (Vercel, Netlify, GitLab) could be\nconsidered.\n\nIt is SUGGESTED that common distributed version control software be used (e.g., git) for the project's source repository.\n\n### Unique version numbering\n\nThe project results MUST have a unique version identifier for each release\nintended to be used by users.\n\n> Commit hashes can be used as unique version numbers in some cases.\n\nIt is SUGGESTED that the Semantic Versioning (SemVer) or Calendar Versioning\n(CalVer) version numbering format be used for releases. It is SUGGESTED that\nthose who use CalVer include a micro level value.\n\n> As mentioned above, projects that are constantly in motion (e.g.\n[darktheme.club](https://darktheme.club/)) might want to consider using commit\nhashes for version numbers instead.\n\nIt is SUGGESTED that projects identify each release within their version\ncontrol system. For example, it is SUGGESTED that those using git identify each\nrelease using git tags.\n\n> Git tags are often neglected during development, but can be very useful.\n\n### Release notes\n\nThe project MUST provide, in each release, release notes that are a\nhuman-readable summary of major changes in that release to help users determine\nif they should upgrade and what the upgrade impact will be. The release notes\nMUST NOT be the raw output of a version control log (e.g., the \"git log\" command\nresults are not release notes). Projects whose results are not intended for\nreuse in multiple locations (such as the software for a single website or\nservice) AND employ continuous delivery MAY select \"N/A\".\n\n> I wrote [a post](/posts/2021-02-20-changelogs) about changelogs a while back.\n\n\nThe release notes MUST identify every publicly known run-time vulnerability\nfixed in this release that already had a CVE assignment or similar when the\nrelease was created. This criterion may be marked as not applicable (N/A) if\nusers typically cannot practically update the software themselves (e.g., as\nis often true for kernel updates). This criterion applies only to the\nproject results, not to its dependencies. If there are no release notes or\nthere have been no publicly known vulnerabilities, choose N/A.\n\n## Reporting\n\n### Vulnerability report process\n\nIf private vulnerability reports are supported, the project MUST include how\nto send the information in a way that is kept private.\n\n> For proprietary projects, it's often a good idea to have a public \"Report an issue\"\nfeature.\n\n## Quality\n\n### Working build system\n\nIf the software produced by the project requires building for use, the\nproject MUST provide a working build system that can automatically rebuild\nthe software from source code.\n\nIt is SUGGESTED that common tools be used for building the software.\n\n### Automated test suite\n\nThe project MUST use at least one automated test suite ~~that is publicly\nreleased as FLOSS (this test suite may be maintained as a separate FLOSS\nproject)~~. The project MUST clearly show or document how to run the test\nsuite(s) (e.g., via a continuous integration (CI) script or via documentation in\nfiles such as BUILD.md, README.md, or CONTRIBUTING.md).\n\nA test suite SHOULD be invocable in a standard way for that language.\n\nE.g. `npm run test`, `cargo test`, etc.\n\nIt is SUGGESTED that the test suite cover most (or ideally all) the code\nbranches, input fields, and functionality.\n\n> Write tests if they are useful, not for the sake of having 100% test coverage.\n\nIt is SUGGESTED that the project implement continuous integration (where new\nor changed code is frequently integrated into a central code repository and\nautomated tests are run on the result).\n\n### New functionality testing\n\nThe project MUST have a general policy (formal or not) that as major new\nfunctionality is added to the software produced by the project, tests of that\nfunctionality should be added to an automated test suite.\n\nThe project MUST have evidence that the test policy for adding tests has been\nadhered to in the most recent major changes to the software produced by the\nproject.\n\n> This is often covered if you have a CI pipeline.\n\nIt is SUGGESTED that this policy on adding tests (see test_policy) be\ndocumented in the instructions for change proposals.\n\n> Consider adding a checkbox to your merge request template. For reference, here's\na checklist that I often use in templates:\n>\n> ```\n> # Checklist:\n>\n> - [ ] documented in the changelog\n> - [ ] sufficiently tested\n> - [ ] sufficiently documented\n> ```\n\n### Warning flags\n\nThe project MUST enable one or more compiler warning flags, a \"safe\"\nlanguage mode, or use a separate \"linter\" tool to look for code quality\nerrors or common simple mistakes, if there is at least one FLOSS tool that\ncan implement this criterion in the selected language.\n\nThe project MUST address warnings.\n\n> Ensure this by disallowing warnings in your CI pipeline.\n\nIt is SUGGESTED that projects be maximally strict with warnings in the\nsoftware produced by the project, where practical.\n\n## Security\n\n### Secure development knowledge\n\nThe project MUST have at least one primary developer who knows how to design\nsecure software.\n\nAt least one of the project's primary developers MUST know of common kinds of\nerrors that lead to vulnerabilities in this kind of software, as well as at\nleast one method to counter or mitigate each of them.\n\n> Easier said than done, but be vocal if you're hesitant towards a feature or\nimplementation path.\n\n### Use basic good cryptographic practices\n\nThe software produced by the project MUST use, by default, only cryptographic\nprotocols and algorithms that are publicly published and reviewed by experts (if\ncryptographic protocols and algorithms are used).\n\nIf the software produced by the project is an application or library, and its\nprimary purpose is not to implement cryptography, then it SHOULD only call on\nsoftware specifically designed to implement cryptographic functions; it SHOULD\nNOT re-implement its own.\n\n> Don't reinvent the wheel. Not just for cryptography.\n\nThe security mechanisms within the software produced by the project MUST use\ndefault keylengths that at least meet the NIST minimum requirements through the\nyear 2030 (as stated in 2012). It MUST be possible to configure the software so\nthat smaller keylengths are completely disabled.\n\nThe default security mechanisms within the software produced by the project\nMUST NOT depend on broken cryptographic algorithms (e.g., MD4, MD5, single DES,\nRC4, Dual_EC_DRBG), or use cipher modes that are inappropriate to the context,\nunless they are necessary to implement an interoperable protocol (where the\nprotocol implemented is the most recent version of that standard broadly\nsupported by the network ecosystem, that ecosystem requires the use of such an\nalgorithm or mode, and that ecosystem does not offer any more secure\nalternative). The documentation MUST describe any relevant security risks and\nany known mitigations if these broken algorithms or modes are necessary for an\ninteroperable protocol.\n\nThe default security mechanisms within the software produced by the project\nSHOULD NOT depend on cryptographic algorithms or modes with known serious\nweaknesses (e.g., the SHA-1 cryptographic hash algorithm or the CBC mode in\nSSH).\n\nThe security mechanisms within the software produced by the project SHOULD\nimplement perfect forward secrecy for key agreement protocols so a session key\nderived from a set of long-term keys cannot be compromised if one of the\nlong-term keys is compromised in the future.\n\nIf the software produced by the project causes the storing of passwords for\nauthentication of external users, the passwords MUST be stored as iterated\nhashes with a per-user salt by using a key stretching (iterated) algorithm\n(e.g., Argon2id, Bcrypt, Scrypt, or PBKDF2). See also OWASP Password Storage\nCheat Sheet).\n\nThe security mechanisms within the software produced by the project MUST\ngenerate all cryptographic keys and nonces using a cryptographically secure\nrandom number generator, and MUST NOT do so using generators that are\ncryptographically insecure.\n\n### Secured delivery against man-in-the-middle (MITM) attacks\n\nThe project MUST use a delivery mechanism that counters MITM attacks. Using\nhttps or ssh+scp is acceptable.\n\nA cryptographic hash (e.g., a sha1sum) MUST NOT be retrieved over http and\nused without checking for a cryptographic signature.\n\n### Publicly known vulnerabilities fixed\n\nThere MUST be no unpatched vulnerabilities of medium or higher severity that\nhave been publicly known for more than 60 days.\n\n> This can be ensured using\n[Dependabot](https://docs.github.com/en/code-security/dependabot/working-with-dependabot)\nor [Renovate](https://docs.renovatebot.com/)\n\nProjects SHOULD fix all critical vulnerabilities rapidly after they are reported.\n\n> Again, use automatic dependency updating mechanisms for this.\n\n### Other security issues\n\nThe public repositories MUST NOT leak a valid private credential (e.g., a\nworking password or private key) that is intended to limit public access.\n\n## Analysis\n\n### Static code analysis\n\nAt least one static code analysis tool (beyond compiler warnings and \"safe\"\nlanguage modes) MUST be applied to any proposed major production release of the\nsoftware before its release, if there is at least one ~~FLOSS~~ tool that implements\nthis criterion in the selected language.\n\nIt is SUGGESTED that at least one of the static analysis tools used for the\nstatic_analysis criterion include rules or approaches to look for common\nvulnerabilities in the analyzed language or environment.\n\nAll medium and higher severity exploitable vulnerabilities discovered with\nstatic code analysis MUST be fixed in a timely way after they are confirmed.\n\nIt is SUGGESTED that static source code analysis occur on every commit or at\nleast daily.\n\n### Dynamic code analysis\n\nIt is SUGGESTED that at least one dynamic analysis tool be applied to any\nproposed major production release of the software before its release.\n\nIt is SUGGESTED that if the software produced by the project includes\nsoftware written using a memory-unsafe language (e.g., C or C++), then at\nleast one dynamic tool (e.g., a fuzzer or web application scanner) be\nroutinely used in combination with a mechanism to detect memory safety\nproblems such as buffer overwrites.\n\n> This is often overlooked.\n\nIt is SUGGESTED that the project use a configuration for at least some dynamic\nanalysis (such as testing or fuzzing) which enables many assertions. In many\ncases these assertions should not be enabled in production builds.\n\nAll medium and higher severity exploitable vulnerabilities discovered with\ndynamic code analysis MUST be fixed in a timely way after they are confirmed.\n\n---\n\nThis is post 046 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"OpenSSF Best Practices","date":"2023-02-14","tags":"note, 100DaysToOffload, practices"}},{"slug":"2023-01-02-welcome-2023","markdownBody":"\n2022 has been an eventful year for me. We moved to a new apartment, went to a\ncouple of weddings (a good indication that we're getting older!) and finally\nstarted traveling more often.\n\nThis year, I launched two projects: [darktheme.club](https://darktheme.club) and\n[seeking-maintainers.net](https://seeking-maintainers.net). My goal for 2023 is\nto make two other projects ready for production:\n\n[Fling](https://github.com/garritfra/fling) is a household management app that\nI've been [dogfooding](https://en.wikipedia.org/wiki/Eating_your_own_dog_food)\nwith my girlfriend for over a year. I just recently implemented an account\nsystem. You can in fact already download the app on the [play\nstore](https://play.google.com/store/apps/details?id=de.garritfra.fling) and\ntest it out, though in it's current state it's just a boring app for to-do and\nshopping lists.\n\nAnother project I'm planning to launch is\n[Kubinity](https://docs.kubinity.com/). The idea behind this project is to give\nusers access to Kubernetes by allocating a namespace in a shared cluster. This\nallows for ultra cheap kubernetes hosting without the burden of maintaining and\nscaling a cluster. It also comes with free HA!\n\n## Predictions for 2023\n\n- Meta will change course (again), and kill the \"Metaverse\"\n- Cheap matter devices will enter the market. As a reference point: I predict\nthat there will be a matter-ready smart plug for under 20€ on Amazon.\n- AI generated content will be regulated in some way\n- Twitter is still around, but it's still not profitable\n- I still won't have finished my\n[#100DaysToOffload]([/posts/2021-01-11-100daystooffload](https://100daystooffload.com/))\n[challenge from 2021](/posts/2021-01-11-100daystooffload)\n\nOverall, I'm feeling very optimistic about the upcoming year. Let's see if this\nis still true at the end of this year, but let's not loose hope too early.\n\nHappy new year!\n\nThis is post 045 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Welcome, 2023","date":"2023-01-02","tags":"note, 100DaysToOffload"}},{"slug":"2022-12-05-contributing-to-open-source-knowledge","markdownBody":"\nI wrote the initial draft for this post a few months ago, traveling through\nNorway in a rented campervan. While roaming the beautiful landscapes, I spent a\nlot of time thinking. Reading books while traveling really is the best way to\nfind new inspiration.\n\nOn our trip, we wanted to try out an alternative to Google Maps. Most of the\nOpenStreetMap-based apps lack important features, but we recently stumbled upon\n[MagicEarth](https://www.magicearth.com/), which perfectly fills the void.\nOpenStreetMap has been 95% accurate for us. Those last 5% are mostly less famous\nhiking trails and attractions that could easily be filled in by people like you\nand me. This inspired me to write this blog post, where I share six ways that\nyou can contribute to open source knowledge right now.\n\n## OpenStreetMap\n\nAs mentioned above, I spotted some minor inconsistencies in\n[OpenStreetMap](https://openstreetmap.org) while driving through Norway. We\ntracked our hikes with an app that is able to export a GPX file, which can be\nimported to OpenStreetMap to check if the trail matches (or if it is missing),\nand took note of incorrect or sloppy roads/buildings. Back home, I plan to sit\ndown and fix up those issues.\n\nBut you don't have to be on a roadtrip to contribute to OpenStreetMap! Chances\nare you know your local surroundings pretty well. Just navigate to your\nneighborhood and see what could be improved. Maybe you know a public toilet, a\npark or a secret road that is not shown on the map? As a matter of fact, my\nprivate address was missing, so I added it via the editor. I can now use any of\nthe many OpenStreetMap-based apps to navigate home!\n\n## Wikipedia (and other wikis)\n\nI often feel like I can't contribute much to the vast knowledge of Wikipedia.\n_Other people are way smarter than me_ and whatnot. But while you might not be\nable to publish worthy edits to a well-known topic, you might know some things\nthat others haven't thought of. Is there an entry about your local town? Is\nthere an interesting member of your (past) family that others might want to read\nabout?\n\nOf course, there are other wikis beside Wikipedia. Are you using a little-known\ntool that has open source documentation in the form of a wiki? How can it be\nimproved?\n\n## Observation\n\nYou might have never heard of [observation.org](https://observation.org). It's\nan open biodiversity- and nature-database. I just recently learned about them in\nour local museum. They had a special exhibition about insects, and called out\nfor contributions to map out our local flora and fauna.\n\nThe idea is simple: snap a picture of an interesting looking insect or plant,\nupload it using the website (or one of their apps) and create an \"observation\".\nUsing this information, researchers will be able to understand the biodiversity\nof your area. The information is free to use, and anyone can contribute!\n\n## Wardriving\n\nWardriving is a fun and useful way to contribute to open source knowledge. By\ndriving around with a device that can detect and record wireless networks, you\ncan help to map out the wireless coverage in your area. This information can be\nused by researchers, network operators, and other interested parties to\nunderstand the availability and quality of wireless networks.\n\nOne popular tool for wardriving is [WiGLE](https://wigle.net/). WiGLE allows you\nto easily collect and share information about wireless networks, and contribute\nto the global wireless map. To get started with WiGLE, you will need a device\nthat can detect and record wireless networks. This can be a smartphone, laptop,\nor dedicated wardriving device. You will also need to download and install the\nWiGLE app, and some basic knowledge of how to use it.\n\nOnce you have set up WiGLE, you can start driving around and mapping out the\nwireless networks in your area. As you collect data, it will be automatically\nuploaded to the WiGLE database, where it can be used by researchers and other\ninterested parties. Wardriving with WiGLE is a fun and easy way to help advance\nscientific research and understanding.\n\n## folding@home\n\nAnother way to contribute to open source knowledge is to participate in the\n[folding@home](https://foldingathome.org/) project. folding@home is a\ndistributed computing project that uses the idle processing power of volunteers'\ncomputers to perform scientific calculations and simulations. These calculations\nare used to study a wide range of topics, including protein folding, drug\ndesign, and the origins of the universe.\n\nBy joining the folding@home network, you can help to advance scientific research\nand discovery. The project is open to anyone, and you can participate using your\npersonal computer, laptop, or even your smartphone. All you need to do is\ndownload and install the folding@home software, and then select the types of\ncalculations that you want to contribute to.\n\nThe folding@home project is a great way to put your idle computing power to good\nuse, and to contribute to the global effort to advance scientific knowledge. To\nlearn more, visit the [folding@home website](https://foldingathome.org/).\n\n## Blog posts\n\nWriting a blog post is a fun and engaging way to contribute to open source\nknowledge. You don't need to be a professional writer or have a formal writing\nstyle. Just jot down some notes about a topic that you are passionate about, and\nshare your experiences and expertise with others.\n\nNot only will you be helping others to learn from your experiences, but writing\na blog post can also be beneficial for yourself. Capturing your thoughts and\nideas in writing can help you to better understand and organize your own\nknowledge. It can also be a great way to reflect on your experiences and to\nlearn from your successes and failures.\n\nIf you're interested in blogging, you might want to check out the\n[100DaysToOffload](https://100daystooffload.com/) project!\n\n# Wrapping up\n\nAs you can see, there are many ways that you can contribute to open source\nknowledge, even if you don't have a lot of time or expertise. By participating\nin projects like OpenStreetMap, Wikipedia, observation.org, and folding@home, or\nby sharing your experiences and expertise through blog posts, you can make a\nreal difference in the community.\n\nWhy not give it a try? You might be surprised by how much you can learn and how\nmuch you can help others. And who knows, you might even have some fun along the\nway! Thanks for reading, and happy contributing!\n\nThis is post 044 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"6 ways you can contribute to open knowledge right now","date":"2022-12-05","tags":"note, guide, practices, 100DaysToOffload"}},{"slug":"2022-11-24-smart-move-google","markdownBody":"\nFor as long as I can remember, `maps.google.com` was the defacto domain for\nGoogle Maps. Also for as long as I can remember, I allowed this domain to use\nthe location services of my browser.\n\nYesterday I was asked to allow the usage of location services for Google Maps\nseemingly out of nowhere. Of course I accepted. After all, I just wanted to\ncheck a route to a local business and I was in a hurry. Back home I opened\nGoogle Maps again, and noticed that `maps.google.com` now redirects to\n`google.com/maps`. This implies that the permissions I give to Google Maps now\napply to **all** of Googles services hosted under this domain. So far I only\nidentified Google Flights to have made the same switch (`google.com/flights`),\nthough I'm sure they're just beginning to transfer their services to the main\n`google.com` domain.\n\nCongratulations, you now have permission to geo-track me across **all** of your\nservices.\n\nSmart move, Google.\n\nThis is post 043 of [#100DaysToOffload](https://100daystooffload.com/).\n\n---\n\nThis post generated some interesting discussions on\n[HackerNews](https://news.ycombinator.com/item?id=33729345).\n\n### Backlinks\n\n- [Google Bypasses Privacy, Puts Users’ Data on the Map](https://analyticsindiamag.com/google-bypasses-privacy-puts-users-data-on-the-map/)\n- [Last Week in Local - December 5, 2022](https://open.spotify.com/episode/2EfB2yqlh7B62701mzc82v)\n- [Softantenna](https://softantenna.com/blog/google-smart-move/) ([English](https://softantenna-com.translate.goog/blog/google-smart-move/?_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=de&_x_tr_pto=wapp))\n- [Google kann euren Standort unter Umständen mit allen Services verfolgen](https://stadt-bremerhaven.de/google-kann-euren-standort-unter-umstaenden-mit-allen-services-verfolgen/) ([English](https://stadt--bremerhaven-de.translate.goog/google-kann-euren-standort-unter-umstaenden-mit-allen-services-verfolgen/?_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=de&_x_tr_pto=wapp))\n- [zive.cz](https://www.zive.cz/clanky/google-vymyslel-figl-jak-nas-plosne-sledovat-stacilo-mapam-zmenit-adresu/sc-3-a-219564/default.aspx) ([English](https://www-zive-cz.translate.goog/clanky/google-vymyslel-figl-jak-nas-plosne-sledovat-stacilo-mapam-zmenit-adresu/sc-3-a-219564/default.aspx?_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=de&_x_tr_pto=wapp))\n","frontmatter":{"title":"Smart Move, Google","date":"2022-11-24","tags":"note, 100DaysToOffload, practices"}},{"slug":"2022-11-11-cursed-user-agents","markdownBody":"\n> **Warning**: This is a rather ranty post. I just needed a place to dump my emotions about this topic. Please take it with a grain of salt. :)\n\nI'm currently [fiddling\naround](https://github.com/garritfra/ua-parser-js/pull/8) with User-Agents of\nSmart TVs, or more specifically [HbbTV](https://www.hbbtv.org/). Interpreting\nthem is an absolute nightmare, so let me rant about interesting edge-cases I\ndiscovered along the way.\n\nTo set the mood: User-Agents in this field have a standardized format, yet many\nvendors seem to do their own thing, making it impossible to build one parser to\nrule them all. For reference, here's what the HbbTV section in a user agent\nSHOULD look like:\n\n```\nHbbTV/<version> (<capabilities>; <vendorName>; <modelName>; <softwareVersion>; [<hardwareVersion>]; <familyName>; <reserved>)\n```\n\n## The \"we'll update that later\"\n\n```\nHbbTV/1.1.1 (; Loewe; MB180; 1.0; 1.0;) NetFront/4.1\n```\n\n1.0 for both software and hardware versions suspiciously looks like a working\ntitle. At least we get some information about the vendor and the model.\n\n## The Overcommitted\n\n```\nMozilla/5.0 (Linux armv7l) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36 OPR/40.0.2207.0 OMI/4.9.0.237.DOM3-OPT.245 Model/Vestel-MB211 VSTVB MB200 HbbTV/1.2.1 (; JVC; MB211; 3.19.4.2; _TV_NT72563_2017 SmartTvA/3.0.0\n```\n\nIt's nice that we get a lot of information about the device, yet no one seemed\nto check if the string actually fits into storage.\n\n## The Lazy Boy\n\n```\nHbbTV/1.1.1 (;;;;;) Maple;2011\n```\n\nTo be fair, this is one of the earliest HbbTV devices ever. No one knew that\nthis technology would stand the test of time. Apparently not even Samsung.\n\n## Conclusion\n\nI learned that building a generic parser for user agents isn't easy, especially\nif the devices you work with could be over a decade old.\n\nOne thing that all devices do seem to get right though is the presence of the\nHbbTV section in the User-Agent. I did not encounter a single device without it.\n\nThis is post 042 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Cursed User-Agents","date":"2022-11-11","tags":"note, 100DaysToOffload, web"}},{"slug":"2022-11-03-reselling-managed-kubernetes","markdownBody":"\nI'm currently working on a side project involving reselling Kubernetes clusters.\nWhat I discovered is that it's impossible to resell *managed* Kubernetes, as in\n[EKS](https://aws.amazon.com/de/eks/) and\n[GKE](https://cloud.google.com/kubernetes-engine/).\n\nThe only possible scenario where reselling Kubernetes to your and the\nend-customers advantage is to manage the nodes yourself. The reason is the\nscaling of cost per CPU.\n\nWhen renting VMs, the price per CPU often varies for the size of the machine. \nThis leaves the reseller flexibility in the choice of resources. To give you an\nexample, here is the pricing for virtual machines at\n[Hetzner](https://www.hetzner.com/), and the price per CPU:\n\n| Product         | Number of vCPUs    | Price (in €/month) | Price per vCPU (in €/month) |\n|-----------------|--------------------|--------------------|-----------------------------|\n| CX11            | 1                  | 4.51               | 4.51                        |\n| CPX11           | 2                  | 5.18               | 2.59                        |\n| CPX21           | 3                  | 8.98               | 2.99                        |\n| CPX31           | 4                  | 16.18              | 4.05                        |\n| CPX41           | 8                  | 29.99              | 3.75                        |\n| CPX51           | 16                 | 65.33              | 4.08                        |\n\nWhen comparing this to a managed Kubernetes product like\n[CIVO](https://www.civo.com), we see that the price per CPU stays constant:\n\n| Product         | Number of vCPUs    | Price (in €/month) | Price per vCPU (in €/month) |\n|-----------------|--------------------|--------------------|-----------------------------|\n| Extra Small     | 1                  | 5                  | 5                           |\n| Small           | 2                  | 10                 | 5                           |\n| Medium          | 4                  | 20                 | 5                           |\n| Large           | 8                  | 40                 | 5                           |\n\nThis pricing model is nice and predictable for the customer, but it makes it\nimpossible to justify a resell product. If CIVO charges 5€/month per vCPU, we\nwould need to charge extra to be profitable, which in turn overcuts the\ncompetition.\n\nWhen choosing Hetzner (or any other platform offering VMs), we are still able to\nundercut the competition and even optimize how the resources are laid out on\nthe nodes. The obvious downside of course being that we have to manage the\nclusters ourselves.\n\n## Share your thoughts\n\nReselling Kubernetes is tricky. I'm currently sketching out ideas for an\nalternative way to sell Kubernetes hosting at an ultra cheap price. The project\nis still in its infancy but if you're interested, you're more than welcome to\nshare your thoughts in our [Matrix\nRoom](https://matrix.to/#/!cTXkqtlnbHScIxnlqO:matrix.org?via=matrix.org&via=envs.net)!\n\nThis is post 041 of [#100DaysToOffload](https://100daystooffload.com/).","frontmatter":{"title":"Reselling Kubernetes","date":"2022-11-03","tags":"note, 100DaysToOffload, kubernetes, infrastructure"}},{"slug":"2022-10-05-simple-guestbook","markdownBody":"\n> **TL;DR**: Click [here](/guestbook) to view the guestbook.\n\nFor a while now, I wanted to have a quick way to update the pages on my website.\n\nGitHub has the\n[\".\"](https://docs.github.com/en/get-started/using-github/keyboard-shortcuts#source-code-editing)\nhotkey, which opens a web based editor for the file you're currently viewing.\nThis site now has this feature as well! To try it out, just hit `.`, and you'll\nbe redirected to the file editor for this page.\n\nTo see how I implemented this feature, you take a look at\n[this](https://github.com/garritfra/garrit.xyz/commit/658efa3a3ebfebebbf74d0eb6aae6c1cc9566516)\ncommit. It boils down to this snippet of code:\n\n```js\nwindow.addEventListener(\"keypress\", (e) => {\n\tif (e.key === \".\") {\n\t\tconst baseUrl = \"https://github.com/garritfra/garrit.xyz/edit/main/content\";\n\t\tconst filePath = window.location.pathname;\n\t\tconst url = `${baseUrl}${filePath}.md`;\n\n\t\twindow.location.href = url;\n\t}\n});\n```\n\nPretty simple, huh?\n\nSince this doesn't work on mobile devices, I also added [a custom 404\npage](https://github.com/garritfra/garrit.xyz/commit/8c374a8bc0b66192c454300489fee52e7299c9dd#diff-2cbafea0c9dff483ebab9ad670b1cdb3eb7aac552f9c161e42fee84c2efe3a69)\nwhich also redirects to the editor, if the filepath ends with in `/edit`.\n\nLet's have some fun and put this feature to use. I added a simple\n[guestbook](/guestbook) to this site, which is open to receive pull requests.\nI'd love to hear from you!\n\nThis is post 040 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"A simple guestbook","date":"2022-10-05","tags":"note, meta, 100DaysToOffload, guide"}},{"slug":"2022-09-30-debugging-docker-images","markdownBody":"\nDocker builds images incrementally. Every line in a Dockerfile will generate a\nnew image that builds on top of the last one. This can be really handy if\nsomething is not right in your build.\n\nSince version 18.09 Docker has added a new backend for building images,\n[buildkit](https://github.com/moby/buildkit#buildkit). Unfortunately, buildkit\ndoes not let you view the IDs of the intermediate containers it uses under the\nhood. To work around that, you can opt out of buildkit by running a build with\nbuildkit disabled:\n\n```sh\nDOCKER_BUILDKIT=0 docker build --pull --rm -t myproject:latest .\n```\n\nYou should now see the IDs of the intermediate containers:\n\n```sh\nSending build context to Docker daemon  87.84MB\nStep 1/16 : FROM node:16.15.1-alpine3.16 AS development\n16.15.1-alpine3.16: Pulling from library/node\nDigest: sha256:c785e617c8d7015190c0d41af52cc69be8a16e3d9eb7cb21f0bb58bcfca14d6b\nStatus: Image is up to date for node:16.15.1-alpine3.16\n ---> e548f8c9983f\nStep 2/16 : WORKDIR /usr/src/app\n ---> Using cache\n ---> 34e5c9bdb910\nStep 3/16 : COPY package*.json ./\n ---> Using cache\n ---> 626e4ae998fc\nStep 4/16 : RUN npm install glob rimraf\n ---> Using cache\n ---> 2d036b8354e0\nStep 5/16 : RUN npm install\n ---> Using cache\n ---> 948709b4957f      <-- HERE\nStep 6/16 : COPY . .\n...\n```\n\nAs mentioned, these IDs are valid docker images, so you can just launch them\nand attach a shell like every other image:\n\n```sh\ndocker run -ti --rm 948709b4957f\n```\n\nIf you're not seeing a regular shell, but a Node.js REPL for example, this\nmight be because the `ENTRYPOINT` of that image was set to the binary of that\nREPL. To work around that, you can override the entrypoint:\n\n```sh\ndocker run -ti --rm --entrypoint=/bin/sh 948709b4957f\n```\n\n## When is this helpful?\n\nIf your build fails at a particular step, you can attach a shell to the **last\nworking** step, inspect the filesystem, and execute the failing command manually.\n\nThat's all!\n\nThis is post 039 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Debugging Docker images","date":"2022-09-30","tags":"note, 100DaysToOffload, guide, programming, docker"}},{"slug":"2022-09-26-self-hosted-software-im-thankful-for","markdownBody":"\nSelf-hosting software is not just rainbows and sunshine. I used to self-host a lot of my tools, but after some time the burden of maintaining those tools made me switch to hosted alternatives.\n\nHowever, there are a few projects that I stuck with over the years, and which I think deserve a deep appreciation.\n\n## Miniflux\n\n[Miniflux](https://miniflux.app/) is a very minimal, self-hostable RSS reader. It's been rock-solid since they day I started using it. The data for the application entirely lives In a Postgres database, which makes migrating the application to new infrastructure setups an absolute breeze. I've been meaning to support the author for quite some time now, but the cost of maintaining an instance yourself is basically zero, so I've yet to find the time to switch to their paid hosted instance.\n\n## Plausible Analytics\n\n[Plausible](https://plausible.io/) is another tool that just keeps on running. I haven't had any issues with it whatsoever, and I can't remember the last time I had to do a manual intervention. Just like Miniflux, there's a paid instance, which supports the author, and just like Miniflux, the software is so good that I haven't had a reason to switch to it yet. Oh, the irony.\n\n## BirdsiteLIVE\n\nWhile my instance of [BirdsiteLIVE](https://birdsite.slashdev.space/) is currently in a bad shape, this is not at all the fault of the software. There are limitations to the amount of Twitter API requests you can make, and I did a poor job managing the users on that instance. It's currently very overloaded and just very few tweets make it through. I will have to set aside some time to fix this, but the software itself has been rock solid since the day I started using it.\n\n## Synapse (Matrix)\n\nI was hesitant to mention [Matrix](https://matrix.org/) on this list. I had my ups and downs with Synapse (their Python implementation of a Matrix server), but the fact that my instance is still running after multiple infrastructure transitions and even a migration from SQLite to Postgres says something about the quality of the software. I have a feeling that Synapse is fairly resource-hungry, but if you feed it with enough RAM and disk, it will keep running indefinitely.\n\n## Homeassistant\n\nYou can throw [Homeassistant](https://www.home-assistant.io/) on a Raspberry Pi and everything works out of the gate. I even migrated my instance from a RPi 3 to a RPi 4 via their backup and restore functionality. It's absolutely flawless.\n\n## Dead projects\n\nI think it's fair to also mention the software that I no longer self-host.\n\n### E-Mail\n\nJust don't roll your own email.\n\n### Mastodon\n\nToo power hungry for my taste. No easy way to host inside docker, which made it a pain to keep running. I'm very happy with [Fosstodon](https://fosstodon.org/), and don't see a reason to switch to a self-hosted instance any time soon.\n\n### FreshRSS\n\nI tried replacing Miniflux once, but failed. Nothing beats Miniflux.\n\n### Prometheus + Grafana\n\nMonitoring **_inside_** your infra works until the infra goes down, at which point you're essentially driving blindfolded. I switched to Grafana Cloud, which includes a very generous free tier.\n\nThis is post 038 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Self-hosted software I'm thankful for","date":"2022-09-26","tags":"note, 100DaysToOffload, infrastructure"}},{"slug":"2022-09-22-kubernetes-is-a-domain-specific-database","markdownBody":"\nI just finished listening to [an\nepisode](https://kubernetespodcast.com/episode/129-linkerd/) of the Kubernetes\npodcast. In it, [Thomas Rampelberg](https://saunter.org/) makes an analogy that\nI think is worth sharing:\n\n> \"[...] Kubernetes is really a domain-specific database. And you need to look at it\n> that way. The YAML is literally writing a select statement or an insert\n> statement for a database. That's what the YAML is. And it's awesome that it is\n> already configured for how it is. And it's awesome that it's got a schema. But\n> the YAML is you writing an insert statement into Kubernetes. [...]\"\n\nThe Kubernetes API abstracts two types of states: desired state and actual\nstate. Whenever you apply a manifest, you update the _desired state_ of the\ncluster, just like you do in a regular, non domain-specific database like\nPostgreSQL or Redis. Kubernetes then frequently compares the desired state with\nthe _actual_ state of the cluster. If they don't match, Kubernetes will do\nwhatever it does to match these two states. Usually, this data is persisted\nusing a key-value database like [etcd](https://etcd.io/) running in a cluster,\nthough one could theoretically also hook up an external MySQL or Postgres\ndatabase for this purpose.\n\nI found this great diagram by [Tim\nDowney](https://downey.io/blog/desired-state-vs-actual-state-in-kubernetes/),\nshowing an oversimplified analogy of this pattern:\n\n![Thermostat\nExample](/assets/posts/2022-09-22-kubernetes-is-a-domain-specific-database/desired-state-hvac-diagram.png)\n\nYou _insert_ your desired state into the system, and the system adjusts the\nactual state to match the desired state. In the case of thermostats the state is\na temperature. In Kubernetes, it's [resource\nobjects](https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/)\n\nThis is post 037 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Kubernetes is a domain specific database","date":"2022-09-22","tags":"note, 100DaysToOffload, infrastructure, kubernetes"}},{"slug":"2022-08-31-whats-on-my-feed","markdownBody":"\nRSS is - in my humble opinion - one of the best ways to consume content today.\nIts only downside is that it's quite difficult to find new interesting feeds.\nIf you're looking for inspiration, here's a list of the RSS-feeds I'm subscribed\nto as of August, 2022. Some of the entries may intersect with my\n[blogroll](/blogroll).\n\nLet's not keep this one-sided! If you're also using RSS and have a blog, why not\npublish a list of **your** favorite feeds?\n\n- **HackerNews** ([Pretty](https://news.ycombinator.com/), [RSS](https://hnrss.org/frontpage)) - This one's obvious!\n- **Matt Rickard** ([Pretty](https://matt-rickard.com/), [RSS](https://matt-rickard.com/rss)) - Highly interesting posts about software and startups\n- **Drew DeVault** ([Pretty](https://drewdevault.com), [RSS](https://drewdevault.com/blog/index.xml)) - While Drew's opinions are highly controversial, I do think it's valuable to read his posts\n- **Wandering Thoughts** ([Pretty](https://utcc.utoronto.ca/~cks/space/blog/), [RSS](https://utcc.utoronto.ca/~cks/space/blog/?atom)) - Interesting articles about Linux, SystemD and DevOps\n- **Kev Quirk** ([Pretty](https://kevq.uk), [RSS](https://kevq.uk/feed/)) - Kev's posts are always a fun read!\n- **Seth Godin** ([Pretty](https://seths.blog), [RSS](https://feeds.feedblitz.com/sethsblog)) - I don't actively follow this blog, but it occasionally gives me food for inspiration\n- **The New Oil** ([Pretty](https://blog.thenewoil.org/), [RSS](https://blog.thenewoil.org/feed/)) - Some occasional technology reviews\n- **Tiny Projects/Ben Stokes** ([Pretty](https://tinyprojects.dev/), [RSS](https://tinyprojects.dev/feed.xml)) - Ben doesn't post as much as he used to, but his posts are full of great ideas!\n- **Tagesschau** ([Pretty](https://www.tagesschau.de), [RSS](https://www.tagesschau.de/xml/rss2/)) - My source of german news\n- **t3n** ([Pretty](https://t3n.de), [RSS](https://t3n.de/rss.xml)) - Another german news source, focused around technology and NewWork topics. I thought about removing the description though, as the articles are somewhat poorly written. Still looking for alternatives!\n- **xkcd** ([Pretty](https://xkcd.com/), [RSS](https://xkcd.com/atom.xml)) - come on, did you really think I wasn't subscribed to Randall?\n\nWrote a \"What's on my feed?\" entry yourself? Let me know, and I'll link it here!\n\nThis is post 036 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"What's on my feed? (2022)","date":"2022-08-31","tags":"note, 100DaysToOffload, update"}},{"slug":"2022-06-29-the-only-true-answer-to-tabs-vs-spaces","markdownBody":"\nI recently dove into a new project at work. We're starting from a blank page,\nso of course this classic question came up:\n\n> \"So should we use tabs or spaces for our formatting?\"\n\nOne of my teammates explained to us why the only logical answer to this is\n\"Tabs\", and you'll soon know why.\n\n## The problem\n\nMost formatters, by default, use either two or four spaces for indentation by\ndefault. The [Prettier](https://prettier.io/) formatter does this, and it\nsomewhat became the norm for JavaScript projects. This has one huge downside\nthough: everyone on the team has to agree, or live with this standard.\n\nNowadays, almost all editors come with the ability to change the preferred\nindentation settings, which will be overridden by the settings of the\nformatter. I prefer an indentation of 4 spaces, which is reflected in all of my\ncode. If I'm working on a project that uses an indentation of 2 spaces via\nprettier, my preference will be overridden when formatting the code.\n\n## Just use tabs\n\nThe solution to this problem is simple: Create a `.editorconfig` file and set\nthe indentation style to tab, without a width:\n\n```editorconfig\nroot = true\n\n[*]\nend_of_line = lf\ncharset = utf-8\nindent_style = tab\n```\n\nAlmost all editors will be able to pick this file up and configure some\nproject-wide settings. If your editor is configured to use a indent width of 4,\nthis setting will be respected. If you're a maniac that indents their code with\n8 spaces, you'll be pleased to see that you can finally use this style in your\ncode, without forcing anyone else to do as you do.\n\nEven GitHub, GitLab and friends are able to respect this setting, giving\neveryone the opportunity to view code in their preferred style.\n\nI hope you now know why using a single tab of indentation makes the most sense if\nyou're working in a team. Let me know your thoughts!\n\nThis is post 035 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"The only true answer to 'tabs vs spaces'","date":"2022-06-29","tags":"note, guide, 100DaysToOffload, programming, practices"}},{"slug":"2022-06-10-a-list-of-bugs-in-macos","markdownBody":"\nI've been using MacOS for years, and I was always happy with it. However, it\nhas some really annoying problems that don't seem to get attention by Apple.\nThis is a list of things that personally bother me. I will try to update it\nwhenever I encounter new issues.\n\n- ~~When plugging in a external monitor, some native Windows can't be used with a\n  mouse anymore. Restart required to fix.~~ (This seems to be fixed)\n- Dash-to-dock sometimes doesn't work for fullscreen-applications.\n- When opening a fullscreen window from the workspace-overview, the dock\n  sometimes stays visible. Has to be manually hovered to hide.\n- When using a chromium-based browser on an external monitor in\n  fullscreen-mode, the menu-bar sometimes doesn't appear when hovered.\n- When switching from the internal to an external monitor, the workspace\n  alignment is not how I left it.\n\nThis is post 034 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"A list of bugs in MacOS","date":"2022-06-14","tags":"note, 100DaysToOffload, MacOS"}},{"slug":"2022-06-02-tar-commands","markdownBody":"\n> **Update**: [@kaushalmodi@mastodon.technology](https://mastodon.technology/@kaushalmodi)\n> replied to this post with a way more complete cheat sheet. If you don't want\n> to hear me rant about tar and actually want to get good at it, go read their\n> post instead:\n>\n> https://scripter.co/disarming-the-tar-bomb-in-10-seconds/\n\nGod dammit. I can't tell you how often I had to look up how to create or\nextract a tar archive on linux. There are SOO many options!!\n\n![xkcd 1168](https://imgs.xkcd.com/comics/tar_2x.png)\n\n## Let's settle this once and for all\n\nHere's how you create an archive:\n\n```\ntar cf archive.tar directory\n```\n\nTry to remember \"Create File\".\n\nAnd here's how you extract an archive:\n\n```\ntar xf archive.tar\n```\n\nFor this one, try to remember \"(e)Xtract File\".\n\nAnd if there's some other compressions in the mix: keep looking it up! A more\ncomprehensive cheat sheet can be found here:\nhttps://simplecheatsheet.com/linux-tar-files/\n\nSorry for this dumb post. I'm sure you can relate to my feelings. ;)\n\nThis is post 033 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Here's the tar command you keep looking for","date":"2022-06-02","tags":"note, 100DaysToOffload, linux"}},{"slug":"2022-05-31-database-server-migration-cheat-sheet","markdownBody":"\nI just finished migrating a postgres database to a new host. To remember how to\ndo it next time, I'm writing down the commands I used here.\n\nI usually just shut down the database and then copy the local directory where\nthe volume was mounted onto the new host. This time though, I seemed to be\ngetting some I/O errors, so I had to do it the \"right\" way.\n\nTo be fair, this note is based on\n[this](https://www.netguru.com/blog/how-to-dump-and-restore-postgresql-database)\nguide. I modified it to fit my workflow with docker.\n\n## Creating a dump\n\nLog into the old host:\n\n```\nssh <user>@host\n```\n\nConnect to the postgres-container:\n\n```\ndocker exec -ti myservice_db_1 /bin/bash\n```\n\nCreate a dump. You can name your dump as you wish - I'm using dates to\ndistinguish multiple dumps:\n\n```\npg_dump -U db_user db_name > db_name_20220531.sql\n```\n\nCopy the dump to the host machine:\n\n```\ndocker cp myservice_db_1:/db_name_20220531.sql ~/\n```\n\n## Moving the dump to the new host\n\nThe easiest way to get the dump off of the old server and onto the new one is to\nuse your local machine as a middleman.\n\nFirst, download the dump to your machine:\n\n```\nscp <user>@<host>:~/db_name_20220531.sql .\n```\n\nThen, do the same thing but reversed, with the new host:\n\n```\nscp ./db_name_20220531.sql <user>@<host>:~/\n```\n\n## Restoring the dump\n\nFirst, connect to the new host:\n\n```\nssh <user>@<host>\n```\n\nAssuming the docker service is already running on the new host, attach to the\ndb-container, just like above:\n\n```\ndocker exec -ti myservice_db_1 /bin/bash\n```\n\nThis time, we have to do some fiddling on the database, so attach a session to\npostgres using their cli:\n\n```\npsql -U my_user\n```\n\nBefore \"resetting\" the existing DB to apply the dump, we have to connect to\nanother database. The `postgres` DB is always there, so you can use that.\n\n```\n\\c postgres\n```\n\nNow, we drop the existing DB and re-add it:\n\n```sql\ndrop database database_name;\ncreate database database_name with owner your_user_name;\n```\n\nAnd now, the moment you've been waiting for! Leave the psql-session and apply\nthe dump:\n\n```\npsql -U db_user db_name < db_name_20220531.sql\n```\n\nThat's all! You now have the exact copy of production database available on your\nmachine.\n\nThis is post 032 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Postgres Docker Container Migration Cheat Sheet","date":"2022-05-31","tags":"note, guide, 100DaysToOffload, database, docker, postgres"}},{"slug":"2022-05-24-cloning-windows-to-a-new-drive","markdownBody":"\nMy grandpa has been using his current computer for about 10 years now. After\nsuch a long time, the system has become quite slow and bulky. Back then it was\nrelatively normal to use a HDD as a primary hard drive, which adds to the slow\nexperience. It was time for an upgrade!\n\n> **TL;DR**: Use [Clonezilla](https://clonezilla.org/) on a live usb stick to\n> create an exact copy of your old drive onto your new one.\n\nI got him a 512 GB SSD, which, conveniently, is the same size of his current\nHDD. While installing the new drive alongside his existing one, I thought about\nhow to copy the existing Windows-installation.\n\nNaïvely, I thought that I could just `dd` the contents of the HDD onto the new\ndrive would work, since, _every byte is copied as is_, or at least that's what I\nthought. Turns out it wasn't that easy. I'm sure it would've worked if I was\nmore careful, but by default, `dd` just wipes over each byte, not caring if it\nmade a mistake. After very long 5 hours, I came back to the PC to see that it\nfinished copying the 512 GB (yes, it's not just copying the data, it's copying\nthe entire partition!). In a super excited mood, I restarted the PC and selected\nthe SSD as a boot medium. Aaaaand... nothing. Windows tried to repair some stuff\nbut it wasn't successful. I fiddled around with the boot partition a bit, but I\nhad to give up after an hour or so.\n\n## The second attempt\n\nAfter researching a bit (I should've done that sooner...) I stumbled across\n[Clonezilla](https://clonezilla.org/), a Linux distribution custom-built for\nthis purpose. I flashed it onto a usb-stick and started the cloning process.\nAfter just 20 minutes (!), it was done cloning the existing data. The process is\nextremely simple!\n\nBefore rebooting, I disconnected the old drive to make sure that there's no\nfunny business going on. Apparently, Windows had to self-adjust UIDs of the\ndrives, but after a short \"Preparing Windows\" animation, the system started up\nas expected. **Success**!!\n\nThe performance of the new hard drive is amazing, at least compared to the HDD\nmy grandpa had before. Plus, we can use the existing HDD to take full system\nbackups every now and then, using the same process.\n\nThis is post 031 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Cloning Windows to a new drive","date":"2022-05-24","tags":"note, guide, windows, til, 100DaysToOffload"}},{"slug":"2022-05-06-weeknote-18-2022","markdownBody":"\nIt's friday, my dudes! Here are some notes on what happened during my week.\n\n## At Work\n\nI wanted to start off my week by looking into migrating our deployment workflow\nto a GitLab Agent, as described [last week](/posts/2022-04-29-weeknote-17-2022).\nThat didn't really go well, since a database from another project bursted into\nflames (not literally) for a reason I still couldn't figure out. As a\nworkaround, I just beefed up the AWS RDS instance size until we find a proper\nsolution.\n\nWhile looking through the docs, I also read more about [Aurora\nServerless](https://aws.amazon.com/de/rds/aurora/serverless/), which would be a\nmatch made in heaven, at least for this specific project. Instead of keeping\nmultiple instances running even without any load, serverless automatically\nscales to as many resources as you need. This **drastically** improves\nperformance (according to the docs it's instantaneous, but I'll have to see that\nwith my own eyes), while keeping the price at a minimum. Once we're done with\nthat migration, I'll probably follow up with a blog post on this.\n\n## Home Automation\n\nWe recently wanted to configure a light in our bedroom to turn on early and the\nmorning, stay on for a few hours and then turn off. Same for the evening.\n\nSoo, I ordered two power plugs pre-flashed with\n[Tasmota](https://tasmota.github.io/docs/) and yanked HomeAssistant onto an old\nPi. The plugs came as a pair, so I automated another light in our home. I think\nthis will be the beginning of a deep deep rabbit hole.\n\nWe'll be moving to a new flat soon, where humidity is higher than usual. I'm\nthinking to monitor this and react with some counter-measures, like air filters.\nSky is the limit, really. More to follow...\n\n## Motorbike Maintenance\n\nI was long due to dust off my motorbike, but I finally did. Also, I did my first\noil change on a vehicle **ever**!\n\n![Motorbike oil change](/assets/posts/2022-05-06-weeknote-18-2022/motorbike_oil_change.jpeg)\n\nFeels great to finally be back on the road.\n\n## Fun Fun Fun\n\nBesides riding my motorbike, we went to a fair in our town. It was the first\ntime in years that I've been to a fair, and to ride bumper cars.\n\nAlso, I met up with a friend I haven't seen in a while. It was great taking a\nwalk and talking about life.\n\nThat's it for this week. Thanks for reading!\n\nThis is post 030 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Weeknote 18/2022","date":"2022-05-06","tags":"weeknote, 100DaysToOffload"}},{"slug":"2022-04-29-weeknote-17-2022","markdownBody":"\nThis week was quite ordinary. We enjoyed the weather during walks in the park\nand got some chores done.\n\n## At work\n\nA major news this week was that GitLab decided to remove [certificate-based\ncluster\naccess](https://docs.gitlab.com/ee/user/project/clusters/deploy_to_cluster.html#deploy-to-a-kubernetes-cluster-with-cluster-certificates-deprecated)\nto Kubernetes clusters sooner as we hoped. We're now fleshing out how to migrate\nto the newer\n[gitlab-agent](https://docs.gitlab.com/ee/user/clusters/agent/install/index.html).\n\nI also got some refactoring work done. In one of our projects, we're currently\nmigrating our GraphQL queries away from a HOC-pattern towards React hooks, using\nthe amazing\n[apollo-augmented-hooks](https://github.com/appmotion/apollo-augmented-hooks)\nlibrary for additional caching features.\n\n## Sideprojects\n\nI continued to work on a secret side-project of mine, but I don't want to spoil\nanything just yet.\n\nBUT! If you're good at writing and would like to help out writing a couple of\nparagraphs that can catch the users attention, [I'd love to hear from\nyou](/contact).\n\nThat's it for this week. Thanks for reading!\n\nThis is post 029 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Weeknote 17/2022","date":"2022-04-29","tags":"weeknote, 100DaysToOffload"}},{"slug":"2022-04-28-hello-to-all-the-new-mastodon-users","markdownBody":"\nTo everyone who just joined Mastodon following Twitter's aquisition: I wish you a\nwarm welcome!\n\nWhether you're staying or just testing the waters, you might want to get to know\nsome awesome people around here. The fediverse might seem intimidating at first.\nThat's because around here, there's no algorithm telling you whom to follow.\n\nInspired by [Mike Harley's blog\npost](https://obsolete29.com/posts/2022/04/26/hello-to-all-the-new-mastodon-users/),\nI want to share some of the people I have been following, talking to, or\ngenerally found interesting over the past ~2 years on this platform. I hope this\nwill be useful to you!\n\n- Alexey Yerin - https://fosstodon.org/@yyp\n- Andreas Gerlach - https://fosstodon.org/@appelgriebsch\n- Bascht - https://social.yakshed.org/@bascht\n- Benedikt Flöser - https://fosstodon.org/@theDoctor\n- Benjamin Hollon - https://fosstodon.org/@benjaminhollon\n- Binjamin Green - https://mas.to/@binyamin\n- Blueberry - https://fosstodon.org/@blueberry\n- Borked - https://masto.borked.sh/@borked\n- Chis - https://fosstodon.org/@RyuKurisu\n- Conny Duck - https://chaos.social/@ConnyDuck\n- Daniel Siepmann - https://fosstodon.org/@daniels\n- Daniel Stenberg - https://mastodon.social/@bagder\n- Darius Kazemi - https://friend.camp/@darius\n- Devine Lu Linvega - https://merveilles.town/@neauoire\n- Doug Belshaw - https://fosstodon.org/@dajbelshaw\n- Elias Mårtenson - https://functional.cafe/@loke\n- Fedops - https://fosstodon.org/@fedops\n- FunnyLookinHat - https://fosstodon.org/@funnylookinhat\n- Gina - https://fosstodon.org/@Gina\n- Gray - https://fosstodon.org/@gray\n- Hugo - https://soykaf.org/users/uoya\n- Hund - https://fosstodon.org/@hund\n- Jan-Lukas Else - https://fosstodon.org/@jle\n- Jens Finkhäuser - https://social.finkhaeuser.de/@jens\n- Kev Quirk - https://fosstodon.org/@kev\n- Klaus Zimmermann - https://fosstodon.org/@kzimmermann\n- Luke Harris - https://fosstodon.org/@lkhrs\n- Mike Harley - https://indieweb.social/@obsolete29\n- Murteza Yesil - https://fosstodon.org/@murtezayesil\n- Nicholas Constant - https://social.nicolas-constant.com/users/NicolasConstant\n- Nicholas Danes - https://smallcamp.art/@ndanes\n- Nikita Karamov - https://fosstodon.org/@kytta\n- Nüjtag - https://fosstodon.org/@Nujtag\n- Oppen - https://merveilles.town/@oppen\n- Paul Lammers - https://eldritch.cafe/@paullammers\n- Paul Wilde - https://fosstodon.org/@pswilde\n- Rekka Bellum - https://merveilles.town/@rek\n- Robby - https://zoinks.one/users/robby\n- Ru - https://fosstodon.org/@ru\n- Sotolf - https://social.linux.pizza/@sotolf\n- Tallship - https://pleroma.cloud/users/tallship\n- Tayo - https://fosstodon.org/@Tay0\n- TobTobXX - https://fosstodon.org/@tobtobxx\n- Tyler Childs - https://smallcamp.art/@tychi\n- Yarmo - https://fosstodon.org/@yarmo\n- eddiex - https://fosstodon.org/@0xedd1e\n- iooioio - https://fosstodon.org/@iooioio\n- muesli - https://mastodon.social/@fribbledom\n- tdarb - https://fosstodon.org/@tdarb\n- xpil - https://fosstodon.org/@xpil\n\nAnd of course - yours truly: [@garritfra@fosstodon.org](https://fosstodon.org/@garritfra)\n\n## Bonus tip\n\nIf you'd like to keep following people from twitter as if they were part of the\nfediverse, you might want to check out the\n[BirdsiteLIVE](https://fosstodon.org/web/@BirdsiteLIVE) project. I've been using\nit for about a year now. No need to be on twitter at all! If you're not into\nself-hosting, check out this list of hosted instances:\nhttps://the-federation.info/birdsitelive.\n\nStay safe, and see you around!\n\nThis is post 028 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Hello to all the new Mastodon users","date":"2022-04-28","tags":"fediverse, mastodon, 100DaysToOffload"}},{"slug":"2022-04-25-weeknote-16-2022","markdownBody":"\nThis is my first (already belated) weeknote. Even though it's the monday of the\nnext week, I thought I'd reflect on what happened last week. I already tried [a\nsimilar format](/posts/2020-12-18-update-december) on this blog in a monthly\nfashion, but didn't stick to it. With these \"weeknotes\", I will try to sit down\nfor half an hour each friday. This is heavily inspired by [Doug Belshaw's posts\nwith the same name](https://dougbelshaw.com/blog/).\n\n## At work\n\nThere's currently an effort to integrate a whole new backend into our\napplication. Fundamental features like user- and role-management will have to be\nmigrated to this new backend, which is mostly incompatible with our current\none. As I'm currently the only developer on the project, building a concept\nand realizing it will mostly be my part. I'm somewhat frightened about this\nundertaking, but I think I will learn a whole lot.\n\nI also introduced versioned deployments for our application. To keep it simple,\nreleases are versioned by date (E.g. `v2022.04.21`). Alongside this, I also\nadded a changelog describing each release. I'm still unsure whether this gives\nus a benefit or if it's just unnecessary work, but time will tell. I'm curious\nof what you - the reader - thinks of this. Feel free to reply to this post via\nthe link at the bottom of the page.\n\n## On this blog\n\nI'm playing around with ideas about comments on this static website. I yanked\nout some thoughts in this thread, but I'll have to think about it a bit more:\n\n<iframe src=\"https://fosstodon.org/@garritfra/108180821665987615/embed\"\nclass=\"mastodon-embed\" style=\"max-width: 100%; border: 0; height: 21rem\" width=\"400\"\nallowfullscreen=\"allowfullscreen\"></iframe><script\nsrc=\"https://fosstodon.org/embed.js\" async=\"async\"></script>\n\n## Plants\n\nMy girlfriend and I are in love with plants. This week, we went to a nursery and\ngot ourselves a new member to our family: a Monstera Adansonii.\n\n![Monstera Adansonii](/assets/monstera_adansonii.jpeg)\n\nI also did a little experiment with one of my eucalyptus citriodoras. I cut it\nfrom 50cm back to the first \"node\" in the trunk, at about 2cm. I'm quite happy\nwith how it evolved in just about 10 months. It's now time to develop some\nbranches. Here's it before the pruning:\n\n![Eucalyptus before pruning](/assets/eucalyptus_before_pruning.jpeg)\n\nAnd this is what it looks like now:\n\n![Eucalyptus after pruning](/assets/eucalyptus_after_pruning.jpeg)\n\nI also made cuttings from the rest of the stem. Not sure if they'll survive, but\nwhat could I lose?\n\nThat's it for this week. Thanks for reading!\n\nThis is post 027 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Weeknote 16/2022","date":"2022-04-25","tags":"weeknote, 100DaysToOffload"}},{"slug":"2022-03-24-swapping-numbers-without-temp","markdownBody":"\nEver wondered how to swap two numbers without using a temporary variable?\n\nI just found this very old note that I thought is worth sharing. The trick is\nquite old and you might already know about this, but when I started out with\nprogramming, it blew my mind.\n\nIn school, we get taught to use a temporary\nvariable to swap two numbers:\n\n```js\nlet a = 5;\nlet b = 10;\n\nlet temp = a;\n\na = b; // a = 10\nb = temp; // b = 5\n```\n\nBut by using some arithmetic, we can save us a few bytes of memory:\n\n```js\nlet a = 5;\nlet b = 10;\n\na = a + b; // a = 15 ; b = 10\nb = a - b; // a = 15 ; b = 5\na = a - b; // a = 10 ; b = 5\n```\n\nPlease **never** use this in any production code. The less we have to think\nabout a piece of code, the better it is. It's a fun thought experiment\nnevertheless!\n\n---\n\nThis is post 026 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Swapping two Numbers without Temporary Variables","date":"2022-03-24","tags":"note, programming, 100DaysToOffload"}},{"slug":"2022-03-18-fix-traefik-proxy-issues","markdownBody":"\nAfter changing my proxy from NGINX to Traefik, I noticed that some of my\nservices started misbehaving.\n\nIn particular, my instance of\n[BirdsiteLive](https://github.com/NicolasConstant/BirdsiteLive)\n([birdsite.slashdev.space](https://birdsite.slashdev.space)) had issues\nforwarding tweets to the\n[Fediverse](https://garrit.xyz/posts/2021-01-18-reasons-the-fediverse-is-better).\n\nThe only difference between my old NGINX and my Traefik config were the headers.\nI didn't think that that's what's causing the issue, but after digging around a\nbit I figured out what's wrong. I still can't wrap my head around it entirely,\nbut it has something to do with forwarding external `https` requests to internal\n`http` services, since the `x-forwarded-` headers where missing in the forwarded\nrequests.\n\nIn the world of NGINX, we can instruct the proxy to forward _all_ headers using\nthis directive:\n\n```conf\nproxy_pass_request_headers      on;\n```\n\nwhich takes care of the issue. In Traefik, it's a bit more convoluted. Traefik\ncan use a combination of \"Entrypoints\" and middleware to route traffic around.\nIn my setup, I use a `webSecure` entrypoint listening for SSL/TLS traffic, and a\n`web` entrypoint that just redirects to `webSecure`:\n\n```yaml\nentryPoints:\n  web:\n    address: :80\n    http:\n      redirections:\n        entryPoint:\n          to: \"websecure\"\n          scheme: \"https\"\n\n  websecure:\n    address: :443\n```\n\nApparently, some services send requests to the `web` entrypoint, and the\n`x-forwarded-for` headers are dropped. To prevent this, you can set the\n`proxyProtocol` and `forwardedHeaders` in the `web` entrypoint to `insecure`,\nlike so:\n\n```yaml\nentryPoints:\n  web:\n    address: :80\n    proxyProtocol:\n      insecure: true\n    forwardedHeaders:\n      insecure: true\n    # ...\n# ...\n```\n\nI'm sure there's a reason why this is marked as `insecure`, but it behaves just\nlike the NGINX counterpart, so I didn't bother digging deeper into the matter.\nMaybe one day I'll come back to properly fix this.\n\nIf you want to read more, check out\n[this](https://medium.com/@_jonas/traefik-kubernetes-ingress-and-x-forwarded-headers-82194d319b0e)\narticle on Medium. It explains the issue in more detail.\n\n---\n\nThis is post 025 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Fixing Traefik Proxy Issues","date":"2022-03-18","tags":"note, guide, infrastructure, web, 100DaysToOffload"}},{"slug":"2022-01-28-til-how-to-get-the-selected-language-of-a-browser","markdownBody":"\nToday I learned how to get the selected language of a browser.\n\nIt's super simple!\n\n```js\nconst userLang = navigator.language || navigator.userLanguage;\n```\n\n## An interesting discovery\n\nEventhough I'm using a chromium-based browser, the user-agent and some other\nfields of the `navigator` object imply that I'm running Mozilla Netscape 5.0.\nThis is a relic of the past, where the user agent heavily influenced the look\nand feel of a served website. Nowadays, all rendering engines work more or less\nequally, but back then, browsers tried to be as good as the market leader, so\nthey disquised themselves as Netscape. This podcast episode goes into more\ndetail about how this developed (jump to minute 3 to listen to this topic):\n\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/episode/71URVFdhF6pcUBRhxerDIV?utm_source=generator&t=190\" width=\"100%\" height=\"232\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\"></iframe>\n\n---\n\nThis is post 024 of [#100DaysToOffload](https://100daystooffload.com/).\n\n## Resources\n\n- [Stack Overflow Thread on getting the user language](https://stackoverflow.com/questions/8199760/how-to-get-the-browser-language-using-javascript)\n- [Full link to the Podcast episode](https://corecursive.com/internet-is-duct-tape/#)\n","frontmatter":{"title":"TIL how to get the active language of a browser","date":"2022-01-28","tags":"note, til, javascript, web, 100DaysToOffload"}},{"slug":"2021-12-31-btrfs-on-alpine","markdownBody":"\nI'm currently in the midst of migrating some of my infrastructure from the cloud\nto \"on prem\", aka a local server, aka my old PC. I wanted to try alpine linux as\nthe host OS to see how it behaves as a lightweight server distro.\n\nSo far it stands up quite nicely, it has everything you'd expect from a\nlinux-based operating system. The only problem I encountered was getting BTRFS\nto work out of the box. Here are some things you should know when using BTRFS on\nAlpine linux.\n\n### Installing BTRFS\n\nInstalling BTRFS is relatively straight forward. Simply install the package and\ntell Alpine to load the module on startup:\n\n```\napk add btrfs-progs\necho btrfs >> /etc/modules\n```\n\nTo load the module right away, you can use the following command:\n\n```\nmodprobe btrfs\n```\n\n### Mounting a volume\n\nIf you try mounting a btrfs volume via your fstab, you will get an error. This\nis because BTRFS does not know about the drives yet when the filesystems are\nmounted. To work around this, you can create an OpenRC service that runs a\n`btrfs scan` to detect the drives. To do so, create a service under\n`/etc/init.d/btrfs-scan` with the following content:\n\n```sh\n#!/sbin/openrc-run\n\nname=\"btrfs-scan\"\n\ndepend() {\n  before localmount\n}\n\nstart() {\n  /sbin/btrfs device scan\n}\n```\n\nMake the service executable and register it:\n\n```\nchmod +x /etc/init.d/btrfs-scan\nrc-update add btrfs-scan boot\n```\n\nNow, you should be able to add the volume to your `/etc/fstab`:\n\n```\nUUID=abcdef-0055-4958-990f-1413ed1186ec  /var/data  btrfs   defaults,nofail,subvol=@  0  0\n```\n\nAfter a reboot, you should be able to see the drive mounted at `/var/data`.\n\n### Resources\n\n- [Nathan Parsons - \"Using BTRFS on Alpine Linux\"](https://nparsons.uk/blog/using-btrfs-on-alpine-linux)\n- [A bug report about this problem](https://gitlab-test.alpinelinux.org/alpine/aports/-/issues/9539)\n\nThis is post 023 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"BTRFS on Alpine Linux","date":"2021-12-31","tags":"linux, infrastructure, note, guide, 100DaysToOffload"}},{"slug":"2021-12-24-notes-on-pruning-chinese-elms","markdownBody":"\nI recently bought a chinese elm bonsai. To keep it alive and healthy, I devoted\nsome time to learning how to properly prune it.\n\nChinese elms are very robust trees with strong growth. Even in the winter\nseason, I can prune it on a weekly basis. I just watched [this\nvideo](https://www.youtube.com/watch?v=Nsvc2Ll1X2A) which gives some helpful\ntips on pruning.\n\nI noticed that the author let the tree grow heavily to develop stronger\nbranches. Until now, I pruned the branches back to one leaf whenever it had\nabout 5 leaves. However, a branch should only be cut on its woody parts in order\nto develop more shoots, and a branch needs some growth in order to turn to wood.\n**Don't prune the branches too early**!\n\nSecondly, I thought that cutting the branches just behind a leaf would lead to\nmore growth, but I found that this is not true. The branches should be cut just\na bit above the leaf to prevent branch to die back and kill the leaf. It could\nalso lead to the shoot coming out straight instead of angled, which is not\ndesired.\n\nLastly I learned that branches pointing upwards should most likely be pruned,\nsince they take a lot of energy that the tree could use elsewhere, which is also\npointed out in [this video](https://www.youtube.com/watch?v=93c985zOwhs).\n\nHere's a picture of my chinese elm just how I bought it. You can clearly tell\nthat it was cheaply imported from china. It needs a lot of work, but I'm having\na lot of fun learning about this tree!\n\n![Chinese Elm](/assets/chinese_elm.jpeg)\n\nThis is post 022 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Notes on pruning chinese elms","date":"2021-12-24","tags":"bonsai, note, guide, 100DaysToOffload"}},{"slug":"2021-12-21-usetoggle-react-hook","markdownBody":"\nHere's a useful react hook for situations where you have to keep track of the\nstate of a dialog, popup, etc.:\n\n```js\nimport { useState } from \"react\";\n\nexport default (value) => {\n\tconst [state, setState] = useState(value);\n\n\tconst setStateActive = () => {\n\t\tsetState(true);\n\t};\n\n\tconst setStateInactive = () => {\n\t\tsetState(false);\n\t};\n\n\treturn [state, setStateActive, setStateInactive];\n};\n```\n\nUsage:\n\n```js\nconst SomeComponent = () => {\n\tconst [isDeleteDialogOpen, openDeleteDialog, closeDeleteDialog] =\n\t\tuseToggle(false);\n\n\treturn (\n\t\t<>\n\t\t\t<Button onClick={openDeleteDialog}>Open Delete Dialog</Button>\n\t\t\t<Dialog isOpen={isDeleteDialogOpen} onClose={closeDeleteDialog}></Dialog>\n\t\t</>\n\t);\n};\n```\n\nThis is post 021 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Quick tip! React useToggle Hook","date":"2021-12-21","tags":"javascript, react, quick-tip, 100DaysToOffload, web, programming"}},{"slug":"2021-10-04-server-side-caching-with-apollo-graphql","markdownBody":"\nI recently implemented server-side caching for one of our applications at work.\nThis guide tries to document that I've learned. It assumes that you are using\nan apollo server of version 3 or higher.\n\n### What is Server-Side Caching?\n\nThe point of server-side caching is to reduce the load of your database by\n“remembering” the results of a query for a certain period. If the exact same\nquery comes in again, that remembered result will be returned.\n\nCaching should be handled with care. You should never enable caching for your\nentire application. Instead, you should identify the bottlenecks and develop a\nstrategy to overcome them.\n\n### Enabling caching on the server\n\nThe Apollo Team has done a great job\n[documenting](https://www.apollographql.com/docs/apollo-server/performance/caching/)\nthe caching behavior of their server. To add caching to your existing\nApollo-Server, you first have to add the `responseCachePlugin` to your\nconfiguration as shown\n[here](https://www.apollographql.com/docs/apollo-server/performance/caching/#caching-with-responsecacheplugin-advanced):\n\n```js\nimport responseCachePlugin from \"apollo-server-plugin-response-cache\";\n\nconst server = new ApolloServer({\n\t// ...other options...\n\tplugins: [responseCachePlugin()],\n});\n```\n\nThen, you have to configure a cache backend. By default, Apollo Server will\nstore the caches in RAM, but I’d recommend [using\nRedis](https://www.apollographql.com/docs/apollo-server/data/data-sources/#using-memcachedredis-as-a-cache-storage-backend)\n(or Memcached, if you like), especially if your application is spread across\nmultiple instances of the same backend.\n\n```js\nconst { BaseRedisCache } = require(\"apollo-server-cache-redis\");\nconst Redis = require(\"ioredis\");\n\nconst server = new ApolloServer({\n\t// ...\n\tcache: new BaseRedisCache({\n\t\tplugins: [responseCachePlugin()],\n\t\tclient: new Redis({\n\t\t\thost: \"redis-server\",\n\t\t}),\n\t}),\n});\n```\n\n> Note that you have to use the ioredis library here. node_redis is deprecated\n> as of v2.6.0 of apollo-server-cache-redis.\n\nIf everything went well, your server should now know how to cache responses!\nThis alone won’t get you very far, since it doesn’t know what to cache.\n\n### Telling Apollo what to cache\n\nTo make a type cachable, you have to declare **cache hints**. These properties\ncan either be set in the\n[resolver](https://www.apollographql.com/docs/apollo-server/performance/caching/#in-your-resolvers-dynamic),\nor\n[statically](https://www.apollographql.com/docs/apollo-server/performance/caching/#in-your-schema-static)\nin the schema. To keep it simple, this guide will stick to the static method.\nFeel free to experiment with the dynamic approach though!\n\nTo enable cache hints, simply add the following directive to your schema (you\nonly have to do this once):\n\n```gql\nenum CacheControlScope {\n\tPUBLIC\n\tPRIVATE\n}\n\ndirective @cacheControl(\n\tmaxAge: Int\n\tscope: CacheControlScope\n\tinheritMaxAge: Boolean\n) on FIELD_DEFINITION | OBJECT | INTERFACE | UNION\n```\n\nNow you can add the `@cacheControl` directive to every type that should be cached.\n\n```gql\n# This type will be cached for 30 seconds\ntype Post @cacheControl(maxAge: 30) {\n\tid: ID!\n\ttitle: String\n\tauthor: Author\n\tcomments: [Comment]\n}\n```\n\nFor security reasons, these conditions are [very\nstrict](https://www.apollographql.com/docs/apollo-server/performance/caching/#why-are-these-the-maxage-defaults):\n\n> Our philosophy behind Apollo Server caching is that a response should only be\n> considered cacheable if every part of that response opts in to being\n> cacheable.\n\nThis means that every type needs to explicitly decide how long it should be\ncached. According to this note, the example above actually won’t be cached at\nall!\n\nHaving to specify the `maxAge` of every type would be tedious, so the authors\nhave come up with the `inheritMaxAge` property, which allows the type to\ninherit the settings from its parent. So, in order to make our example\ncachable, we have to enable cache control for all its subfields, either by\nsetting the `maxAge` explicitly or by inheriting it from the parent:\n\n```gql\ntype Post @cacheControl(maxAge: 30) {\n\tid: ID!\n\ttitle: String\n\tauthor: Author\n\tcomments: [Comment]\n}\n\ntype Author @cacheControl(inheritMaxAge: true) {\n\tid: ID!\n\tname: String\n}\n\ntype Comment @cacheControl(inheritMaxAge: true) {\n\tid: ID!\n\tbody: String\n}\n```\n\nNow, whenever you query a `Post`, it will be thrown in the cache. If you query\nthe type again within 30 seconds, the query resolver won’t execute. Instead, it\nwill be read from the cache. Keep in mind that cache hints can also be set on\n`query` and `mutation` fields. This can be handy if you want to cache the\nentire response of a request.\n\n### Gotcha 1: Multiple Response Variations\n\nThe use-case where this topic first came up required us to have different\nresponses based on the type of the logged in user. An `Admin` should see a\ndifferent result than a `Visitor`. If you ignore this fact, it could be that a\nvisitor could see the cache result of a query previously executed by an admin!\n\nThis problem can be counteracted by setting extra information in the cache key\nvia `extraCacheKeyData` (see\n[this](https://www.apollographql.com/docs/apollo-server/performance/caching/#configuring-reads-and-writes)\nparagraph):\n\n```js\nplugins: [\n    responseCachePlugin({\n        extraCacheKeyData: (ctx) => (\n            ctx.context.auth.isAdmin\n        ),\n    }),\n],\n```\n\nThis example can create two distinct caches: One for users that are marked as\nadmins, and one for regular users.\n\n### Gotcha 2: User-specific caches\n\nBesides caching for a group of users, you can also cache responses [for every\nuser\nindividually](https://www.apollographql.com/docs/apollo-server/performance/caching/#identifying-users-for-private-responses).\nYou may have noticed that you can also set a `scope` field in the cache control\ndirective. This will only cache the response if a user is logged in:\n\n```gql\ntype Post {\n\tid: ID!\n\ttitle: String\n\tauthor: Author @cacheControl(maxAge: 10, scope: PRIVATE)\n}\n```\n\nApollo determines if a user is logged in or not, based on if the `sessionId`\nfunction has returned a value other than `null`.\n\n```js\nimport responseCachePlugin from \"apollo-server-plugin-response-cache\";\nconst server = new ApolloServer({\n\t// ...other settings...\n\tplugins: [\n\t\tresponseCachePlugin({\n\t\t\tsessionId: (requestContext) =>\n\t\t\t\trequestContext.request.http.headers.get(\"sessionid\") || null,\n\t\t}),\n\t],\n});\n```\n\nI’m unsure how effective this pattern is, since every user will receive its key\nin the cache. This kind of defeats the purpose of server-side caching, which is\nmeant to reduce load on the database. If you’re trying to cache fields for\nindividual users, you might also want to take a look at client-side caching via\n[apollo-augmented-hooks](https://github.com/appmotion/apollo-augmented-hooks).\n\nThis is post 020 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Server-Side Caching with Apollo GraphQL","date":"2021-10-04","tags":"javascript, graphql, guide, 100DaysToOffload, programming"}},{"slug":"2021-09-13-fixing-an-annoying-cron-gotcha","markdownBody":"\nA while ago I went through my server and reworked my [storage\nsetup](/posts/2021-02-07-storage-setup). As discribed in that blog post, I set\nup daily backups to [Backblaze\nB2](https://www.backblaze.com/b2/cloud-storage.html) using their amazing CLI\nthrough a cron script. A day went by and I noticed that the\n[healthcheck](/posts/2021-05-15-healthchecks-io-with-docker) didn't pass.\nUnfortunately I didn't have time to fix this problem immediately, so instead I\nexecuted the command by hand every couple of days. One could argue that this in\ntotal took way more time than the actual fix, but hey, I was lazy. In the end,\nI finally dedicated some time to fix this annoying issue.\n\nIt turns out that a command executed by cron doesn't run through sh or bash,\nbut in a minimal environment without your usual environment-variables. As a\nresult, my `b2` command (and many other commands for that matter) won't run as\nexpected, if at all. A quick fix is to run your command through bash or sh\nexplicitly:\n\n```sh\nsh -c \"mycommand\"\n```\n\nAlternatively, if you want all your entries to use sh or bash, you can set the\n`SHELL` variable at the very beginning of your crontab:\n\n```sh\nSHELL=/bin/bash\n\n15 1 * * * some_command\n```\n\n[Here](https://askubuntu.com/a/23438) is an answer that goes into more detail\nabout this. Have a great day!\n\nThis is post 019 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Finally fixing that annoying Cron gotcha","date":"2021-09-13","tags":"linux, guide, 100DaysToOffload"}},{"slug":"2021-08-08-fun-with-pen-and-paper-race-cars","markdownBody":"\nI recently came across some fun games that can be played using nothing but a\npen and some paper. One of those games was called \"Race Cars\" and I wondered\nhow on earth you would play a racing game on paper, and the answer is: simple\nmath!\n\n## The core idea\n\nThe goal of the game is to cross the finish line of a hand-drawn race track\nfirst without hitting the edge. Race cars have a velocity that can be adjusted\neach move. If you're too greedy, you will crash. If you hit the breaks too\nearly, another player will win.\n\n## How it's played\n\nWe start out with a hand-drawn race track. The shape really can be anything you\nlike. Just be creative!\n\n![A race track on paper](/assets/paperracer/0.jpg)\n\nNext, each player makes a cross on the starting line. This resembles the\nplayers racecar. Right now, none of the cars is moving. They have a velocity of\n0 on the x axis and 0 on the y axis.\n\n![Two crosses on the starting line](/assets/paperracer/1.jpg)\n\nEach move, a player can accelerate or decelerate his vehicle by 1 on any axis.\nThe first move of each player is somewhat obvious. They want to accelerate\nstraight forward. On our race track, that means accelerate by 1 on the y axis.\n\n![A race car made a move](/assets/paperracer/2.jpg)\n\nNext, we can either keep on accelerating like the red player does, or \"turn\"\nour vehicle left by changing our y velocity from 0 to -1, which gives us a\nvelocity of -1, 1.\n\n![Paper race cars after two moves](/assets/paperracer/3.jpg)\n\nOn a long stretch, the red player wants to overtake blue by keeping his foot on\nthe paddle. Blue on the other hand plays it safe and hits the breaks.\n\n![Paper race cars on a long stretch](/assets/paperracer/4.jpg)\n\nSoon after, red realizes his mistake. Being so busy trying to turn, he's unable\nto hit the breaks. A crash is inevitable. Blue however continues to take the\nturn nice and slow. His humble mind brought him victory!\n\n![The red player right before crashing into a wall](/assets/paperracer/5.jpg)\n\n## It's all about fun\n\nThe concept of this game is very flexible. You can play it with a friend and\nsee who's the fastest paper racer, or by yourself and try to beat your record.\nGetting tired of just cruising around? Introduce the concept of \"items\" that\ngive you a boost or slow down the opponent (think Mario Kart!). If ordinary\ntracks are boring, try adding intersections or obstacles. Can you add a third\ndimension? The sky is the limit. Get out there and be creative.\n\nThis is post 018 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Fun with Pen and Paper: Race Cars","date":"2021-08-08","tags":"fun, guide, 100DaysToOffload"}},{"slug":"2021-05-15-healthchecks-io-with-docker","markdownBody":"\nI'm currently in the midst of improving the monitoring of my infrastructure. I\nmake heavy use of docker and docker-compose for my hosting, so it's vital to add\nmonitoring for most of the containers.\n\nI'm hosting my own instance of [healthchecks.io](https://healthchecks.io/).\nTheir solution to monitoring involves **you** having to ping **them**, instead\nof the other way around. This let's you add healthchecks to virtually anything\nthat can ping a http-endpoint.\n\ndocker-compose let's you define healthchecks to your config that, when\ncompleting sucessfully, mark the container as \"healthy\". The process of adding\nsuch a healthcheck is simple. First, create a new check in your healthchecks.io\naccount and set the ping interval to 1 minute, or a value you prefer. Then, add\nthis snippet to your docker-compose file:\n\n```yaml\napp:\n  image: nextcloud\n  ports:\n    - 127.0.0.1:8080:80\n  healthcheck:\n    test:\n      [\n        \"CMD\",\n        \"curl\",\n        \"-f\",\n        \"https://app-endpoint.tld\",\n        \"&&\",\n        \"curl\",\n        \"-fsS\",\n        \"-m\",\n        \"10\",\n        \"--retry\",\n        \"5\",\n        \"-o\",\n        \"/dev/null\",\n        \"https://healthchecks.io/ping/<UUID>\",\n      ]\n    interval: 60s\n    timeout: 10s\n    retries: 6\n```\n\nChange the first url to the url of your app. The second URL is the endpoint of\nyour healthchecks.io instance. You can obtain it from the check you configured\nearlier.\n\nThis configuration will try to ping your application and, if successful, notify\nthe healthcheck that the application is healthy. If the app is not reachable or\nthe container is down, the latter request will not be executed and your service\nis marked as \"down\".\n\nIn addition to the healthchecks of my docker containers, I also added basic\nhealthchecks to my servers cronfiles and its backup-commands.\n\nDo you have any suggestions regarding this topic? Feel free to reach out to me\nvia Matrix or email!\n\nThis is post 017 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Docker healthchecks using healthchecks.io","date":"2021-05-15","tags":"docker, 100DaysToOffload, guide"}},{"slug":"2021-04-07-pgp-guide","markdownBody":"\nIn the past week, I've been experimenting with PGP, or GPG in particular. In a nutshell, PGP is an encryption standard with a wide range of use cases. For quite some time, I didn't see the point of keeping a PGP keypair. It seemed like a burden to securely keep track of the key(s). Once you loose it, you will loose the trust of others. But after doing some research on the topic, I found that it's not actually that much of a hassle, while giving you many benefits.\n\n# The Why\n\nThe most obvious benefit is encrypting and decrypting messages and files. If you upload your public key, I can encrypt our private conversations. Nobody will be able to read what we're chatting about. If you fear that cloud providers will read through your documents, you can also go ahead and encrypt all of your data with your keypair.\n\nBut PGP is not just about encryption. A keypair also gives you a proof of identity. If I see that a piece of work is signed by you, I can be certain that you and only you have worked on this. By signing the keys of people we trust, we build a \"chain of trust\". A key with many signatures generally has a higher reputation than one without any signatures.\n\nTake Git commits for example. All it takes is a `git config user.email \"elon@spacex.com\"` and I can publish code under a different identity. But if everyone on the team signed their work, they will quickly see that a commit is missing its signature, because I'm simply not able to sign my work with Elon Musk's keypair. Only if they see a badge like this, they will know that they can trust it.\n\nYour keypair can also come in handy as a SSH key. Before I knew about PGP, I always had to install one key per machine I was working on. With PGP, you only have a single identity, and therefore you only have to install one key on your servers.\n\n# The How\n\nLet's first go over the process of setting up a keypair. For this, we will need the `gpg` command installed on our system. Usually, this is just a `<package manager> install gpg` away. Then, we will have to generate a keypair. The quickest way to get one is to use `gpg --gen-key`, but that will make some quirky assumptions about how you want to use your key.\n\nIn PGP, there is this concept of a **keyring**. A keyring has one master key and many subkeys. It is generally a good idea to have one fat master key that never expires and many small subkeys that last about a year or two. The benefit of structuring your keys like this is that you will always have your trusted keychain, and in case something goes south, E.g. your key gets compromised, you can replace that subkey and keep your identity.\n\nWith that in mind, let's create our master key. Run `gpg --full-gen-key` and follow the instructions. You probably want to use the `RSA and RSA (default)` option, and a key that is 4096 bits long (remember, this is the fat master key that never expires, so it must be secure). The comment can be left blank, unless you know what you are doing with that field. Enter a strong passphrase! If your private key were to get compromised, this passphrase is your last line of defense. Make it long, hard to crack but still rememberable. If everything went well, your key should be generated. Here's the full example output:\n\n```\nroot@c6acc9eb4fd1:/# gpg --full-gen-key\ngpg (GnuPG) 2.2.19; Copyright (C) 2019 Free Software Foundation, Inc.\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.\n\nPlease select what kind of key you want:\n   (1) RSA and RSA (default)\n   (2) DSA and Elgamal\n   (3) DSA (sign only)\n   (4) RSA (sign only)\n  (14) Existing key from card\nYour selection? 1\nRSA keys may be between 1024 and 4096 bits long.\nWhat keysize do you want? (3072) 4096\nRequested keysize is 4096 bits\nPlease specify how long the key should be valid.\n         0 = key does not expire\n      <n>  = key expires in n days\n      <n>w = key expires in n weeks\n      <n>m = key expires in n months\n      <n>y = key expires in n years\nKey is valid for? (0)\nKey does not expire at all\nIs this correct? (y/N) y\n\nGnuPG needs to construct a user ID to identify your key.\n\nReal name: Foo\nName must be at least 5 characters long\nReal name: Foo Bar\nEmail address: foo@bar.com\nComment:\nYou selected this USER-ID:\n    \"Foo Bar <foo@bar.com>\"\n\nChange (N)ame, (C)omment, (E)mail or (O)kay/(Q)uit? O\nWe need to generate a lot of random bytes. It is a good idea to perform\nsome other action (type on the keyboard, move the mouse, utilize the\ndisks) during the prime generation; this gives the random number\ngenerator a better chance to gain enough entropy.\nWe need to generate a lot of random bytes. It is a good idea to perform\nsome other action (type on the keyboard, move the mouse, utilize the\ndisks) during the prime generation; this gives the random number\ngenerator a better chance to gain enough entropy.\ngpg: key C8E4854970B7A1A3 marked as ultimately trusted\ngpg: revocation certificate stored as '/root/.gnupg/openpgp-revocs.d/4E83F95221E92EDB933F155AC8E4854970B7A1A3.rev'\npublic and secret key created and signed.\n\npub   rsa4096 2021-04-07 [SC]\n      4E83F95221E92EDB933F155AC8E4854970B7A1A3\nuid                      Foo Bar <foo@bar.com>\nsub   rsa4096 2021-04-07 [E]\n```\n\nYou could stop here and use this key, but let's instead create some subkeys under that key, to make our lives a bit easier. Take the fingerprint of the key (that large number in the output) and run `gpg --edit-key --expert <your fingerprint>`. Run `addkey` three times to add these three keys:\n\n## Signing key\n\nThis key will be used to sign your work (git commits, tags, etc.).\n\n```\ngpg> addkey\n```\n\n1. Choose option \"RSA (set your own capabilities)\", which is currently number 8.\n1. Toggle E (Encryption) so the \"Current allowed actions\" only lists Sign and confirm with Q.\n1. Choose the keysize 2048 (or whatever you prefer).\n1. Choose the key expire date 1y (or whatever you prefer).\n1. Confirm twice, then enter your passphrase.\n\n## Encryption key\n\nThis key will be used to encrypt and decrypt messages.\n\n```\ngpg> addkey\n```\n\n1. Choose option \"RSA (set your own capabilities)\", which is currently number 8.\n1. Toggle S (Sign) so the \"Current allowed actions\" only lists Encryption and confirm with Q.\n1. Choose the keysize 2048 (or whatever you prefer).\n1. Choose the key expire date 1y (or whatever you prefer).\n1. Confirm twice, then enter your passphrase.\n\n## Authentication key\n\nThis key will be used for SSH authentication.\n\n```\ngpg> addkey\n```\n\n1. Choose option \"RSA (set your own capabilities)\", which is currently number 8.\n1. Toggle S (Signing), E (Encryption) and A (Authentication) so the \"Current allowed actions\" only lists Authenticate and confirm with Q.\n1. Choose the keysize 2048 (or whatever you prefer).\n1. Choose the key expire date 1y (or whatever you prefer).\n1. Confirm twice, then enter your passphrase.\n\nNow you should have one key per use case: signing, encrypting and authentication, each with an expiration date:\n\n```\nsec  rsa4096/C8E4854970B7A1A3\n     created: 2021-04-07  expires: never       usage: SC\n     trust: ultimate      validity: ultimate\nssb  rsa4096/C5F71423813B40A0\n     created: 2021-04-07  expires: never       usage: E\nssb  rsa2048/52D4D1D19533D8A5\n     created: 2021-04-07  expires: 2022-04-07  usage: S\nssb  rsa2048/072D841844E3F949\n     created: 2021-04-07  expires: 2022-04-07  usage: E\nssb  rsa2048/42E4F6E376DD92F6\n     created: 2021-04-07  expires: 2022-04-07  usage: A\n[ultimate] (1). Foo Bar <foo@bar.com>\n```\n\nSave your key, and optionally upload it to one of the many keyservers:\n\n```\ngpg> save\n\n$ gpg --keyserver keys.openpgp.org  --send-keys foo@bar.com\n```\n\n**Pro tip**: To set a default keyserver (I use `keys.opengpg.org`, but there are many others out there!), simply add it in your `~/.gnupg/gpg.conf` file:\n\n```\nkeyserver keys.openpgp.org\n```\n\nPeople can now import your public key via `gpg --keyserver keys.opengpg.org --search-keys foo@bar.com`.\n\nWe're done with the setup, let's put our keys to use!\n\n## Code Signing\n\nTo sign your code, you will have to tell git which key to use. Edit your global git options (`~/.gitconfig`) and add these fields:\n\n```\n[commit]\n\tgpgsign = true\n[tag]\n\tgpgsign = true\n[user]\n    name = Foo Bar\n\tsigningkey = 52D4D1D19533D8A5      # Use the ID of your signing key\n\temail = foo@bar.com\n```\n\nNow, whenever you add a commit, git will sign it with your key. You will have to let your git hosting provider know that this key is yours. Go to your account settings and look for a tab that says \"Manage (GPG) keys\". Where this tab is depends on your choice of service. Next, run `gpg --export --armor <your master key id>` and copy the resulting key into the input field of your git hosting service.\n\nWhenever you push a commit, its signature will be checked against that of your account. And that's all the magic!\n\n![A signed commit](/assets/signed_commit.png)\n\n## Encrypting messages\n\nIn order to send an encrypted message to someone, you will need his public key. There are numerous ways to obtain a public key of someone. The simplest way is to ask the person for the raw key. If it's in a text file, you can import it like so:\n\n```\ncat some_key.txt | gpg --import\n```\n\nOftentimes, people will store their keys on a keyserver, just like you have probably done it. To import someones key, simply search for it on a keyserver. I'll use my key here as an example.\n\n```\ngpg --keyserver keys.openpgp.org  --search-keys garrit@slashdev.space\n```\n\nNow, your computer should know about my key. To verify that it's actually me you have imported, you can check if the output of `gpg --fingerprint garrit@slashdev.space` matches my actual fingerprint: `2218 337E 54AA 1DBE 207B 404D BB54 AF7E B093 9F3D`.\n\nOptionally, if you trust that the key is actually associated to me, you can sign it. This let's other people know that you trust me, which helps build a so called \"chain of trust\". A key which has been signed by many people is generally more trustworthy than one that has no signatures.\n\n```\ngpg --sign-key garrit@slashdev.space\n```\n\nNow, let's encrypt a message that only I will be able to read:\n\n```\nprintf \"If you can read this, you've successfully decrypted this message\" | gpg --encrypt --sign --armor -r garrit@slashdev.space\n```\n\nFeel free to send this message to my email-address, I'm happy to chat with you!\n\nDecrypting something is as easy as encrypting something. Say the encrypted message lives in `message.txt.asc`. If you are the recipient, all you have to do is to run `gpg --decrypt message.txt.asc`.\n\n## SSH\n\nYour PGP key can also be used as an SSH key to authenticate on your servers.\n\nFirst we need to add the following to `~/.gnupg/gpg-agent.conf` to enable SSH support in gpg-agent.\n\n```\nenable-ssh-support\n```\n\nNext, we'll need to tell gpg which key to use. We need to get the so called `keygrip` of your authentication key and add it to the `~/.gnupg/sshcontrol`. The keygrip can be obtained by running `gpg -K --with-keygrip`. Just copy the keygrip of the authentication key and paste it into the `~/.gnupg/sshcontrol` file.\n\nThen, we want the ssh agent to know where to look for the key. Put this in your `.bashrc` file (or corresponding config):\n\n```\nexport GPG_TTY=$(tty)\nexport SSH_AUTH_SOCK=$(gpgconf --list-dirs agent-ssh-socket)\ngpgconf --launch gpg-agent\n```\n\nThen, run `ssh-add -l` to load the key directly.\n\nTo get the public ssh key of your keypair, run this command:\n\n```\ngpg --export-ssh-key foo@bar.com\n```\n\nand add the output to the `~/.ssh/authorized_keys` file on your server. When signing in, you should be prompted to enter the passphrase of your key and then authenticated.\n\n## Closing thoughts\n\nI hope by now you see the benefits you gain from having a PGP keypair. Whether you find it useful enough to set one up is of course up to you. It is however a good practice to at least sign your git commits as a proof of identity. There are services like [Keyoxide](https://keyoxide.org) that let you keep a \"public record\" of your key, so that other people can verify your identity more easily. If you set up your key, let me know by sending an encrypted message!\n\nThis is post 016 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"A pretty good guide to pretty good privacy","date":"2021-04-07","tags":"privacy, guide, 100DaysToOffload"}},{"slug":"2021-03-13-git-builtin-lifesaver","markdownBody":"\nEveryone was in this situation at some point. You wasted a days worth of work by accidentally deleting a branch. But, all hope is not lost! Git never forgets.\n\nEvery action, be it committing changes, deleting or switching branches, is noted down by Git. To see your latest actions, you can simply run `git reflog` (It's pronounced `ref-log` but `re-flog` sounds just as reasonable):\n\n```\n5704fba HEAD@{45}: commit: docs: update changelog\nb471457 HEAD@{46}: commit: chore: refactor binop checks in parse_expression\n5f5c5d4 HEAD@{47}: commit: fix: struct imports\n76db271 HEAD@{48}: commit: chore: fix clippy warning\nac3e11c HEAD@{49}: commit: fix: circular imports\n0cbdc88 HEAD@{50}: am: lexer: handle ' or \" within the string properly\n27699f9 HEAD@{51}: commit: docs: spec: add notation\n```\n\nCommits in Git are just data that is not associated by anything. If you accidentally delete a branch, the commits will stay where they are, and you can reference them directly. To recreate your deleted branch, simply run this command:\n\n```\ngit checkout -b <branch> <sha>\n```\n\nAnd that's it! Your branch is restored. Remember to commit early and often, or prepare to loose your work!\n\nThis is post 015 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Git's built-in lifesaver","date":"2021-03-13","tags":"git, 100DaysToOffload, guide"}},{"slug":"2021-02-24-vim-terminal-strategies","markdownBody":"\nOne thing that bothered me about vim for a long time, was the lack of a terminal\ndirectly in your editor. If I'm not using Vim, I'm most definetely using VSCode\nand its built-in Terminal. After searching the webs for possible solutions, I\ncame across a couple of strategies to achive this.\n\n## Executing single commands\n\nIf you just want to issue a single command without spawning an entire shell,\nyou can just use the `:!` command:\n\n```\n:! printf \"Hello Sailor\"\n```\n\n## Vims builtin terminal\n\nI couldn't believe my eyes when I read this, but Vim ships with a builtin\nterminal! Executing `:term` will spawn it in your current buffer. How you\nintegrate it in your workflow is up to you. You could use tabs or open a\nhorizontal buffer and spawn it there. I must say that it is rather clunky to\nuse, since its literally a Vim buffer that forwards stdin and stdout to the\nbuffer, but it's there for you to use.\n\n## Vim x Tmux\n\nAnother great alternative is to set up Tmux with two windows, one for Vim and\none for your terminal, and switch between them. This works great on a minimal\nsystem, but on MacOS for example, it is easier to simply use cmd+1 and cmd+2 to\nswitch between two tabs of the Terminal application.\n\n## Pausing and resuming Vim\n\nThis one is my personal favorite. The idea comes from\n[this](https://stackoverflow.com/a/1258318/9046809) stackoverflow answer.\n\nThe plan is to pause the Vim process and resume it later. To pause Vim, you\npress `<ctrl>-z`. This sends the process in the background. Then, to resume the\nprocess, simply issue the `fg` command and Vims process resumes in the\nforeground.\n\n## Conclusion\n\nI'm sure there are many more strategies that could be added to this list. I'd be\ninterested to hear how your setup works! If you liked these techniques, you\nmight be interested in\n[@lopeztel](https://fosstodon.org/web/accounts/211905)s\n[cheat sheet](https://lopeztel.xyz/2021/02/21/my-neovim-cheatsheet/) for Vim.\n\nThis is post 014 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Strategies to use a terminal alongside (Neo)Vim","date":"2021-02-23","tags":"vim, guide, 100DaysToOffload"}},{"slug":"2021-02-20-changelogs","markdownBody":"\nToday, I finally added a proper changelog to [my current project](https://github.com/garritfra/sabre/blob/master/CHANGELOG.md). My obvious first step was to search the web for `changelog.md`, since that's the naming convention many projects are using for their changelog. I was surprised that I was immediately redirected to \"[changelog.md](https://changelog.md)\", since it is a valid domain name. This website is a great guide on the essense of a good changelog. This is also where I got most of my findings from. Let me walk you through some of the most important ones:\n\n## Changelogs are a vital part of every serious project\n\nThe whole point of a changelog is to keep track of how the project evolves over time. When working with multiple people, it helps getting everyone on the same page. Keeping a changelog reduces a possible monopoly of information, since all contributers know what is going on. Of course, users also benefit from your changelog. They will know what changes they can expect when they do an update.\n\n## Entries should have a standardized format\n\nChangelogs are mainly meant to be readable by humans. Here are some important points to watch out for when writing a changelog:\n\n- Every version of your software (major, minor and patch) should have one section and one section only\n- Recent releases should be added at the top of the changelog (reverse chronological order)\n- Each version _should_ display its release date in ISO format (YYYY-MM-DD) next to the version name\n\n## What types of changes need to be included?\n\nYou could just go ahead and throw some changes in a big list and call it a day. To make the changelog more readable though, you should categorize every change by its type. Here's an example of a set of categories that could be included:\n\n- **Features**: New features or additions\n- **Fixes**: Bugfixes\n- **Security** Important changes regarding holes in your security\n- **Documentation**: Changes or additions in your documentation should go here\n\nThis is just an example that illustrates how **I** decided to note down my changes. [changelog.md](https://changelog.md) suggests a slightly different convention, but how you're handling it doesn't really matter.\n\n## An example\n\nHere's an example of how a changelog could look like. It's taken from [Sabre](https://github.com/garritfra/sabre), a project I'm currently working on. The full changelog can be found [here](https://github.com/garritfra/sabre/blob/master/CHANGELOG.md).\n\n```md\n# Changelog\n\n## v0.4.0 (2021-02-20)\n\nThis release introduces the concept of structs, alongside many improvements to the documentation.\n\n**Features**\n\n- Assignment operators (#10)\n- Structs (#12)\n\n**Fixes**\n\nNone\n\n**Documentation**\n\n- Fixed some typose and broken links\n- Document boolean values\n- Added this changelog!\n\n## v0.3.0 (2021-02-12)\n\nThis release adds type inference to Sabre. There are also a lot of improvements in terms of documentation. The docs are now at a state that can be considered \"usable\".\n\n**Features**\n\n- Type inference\n- The `any` type\n- First attempt of LLVM backend\n\n**Fixes**\n\n- Fixed an error when printing numbers\n\n**Documentation**\n\n- Added documentation for for loops\n- Added documentation for while loops\n- Documented LLVM backend\n- Documented comments\n- Updated contributing guidelines\n```\n\n## Personal recommendations\n\nWhen releasing a new version, don't just add an entry to your changelog. You should use **git tags** whenever working with versions, to mark the exact commit of the released version.\n\nRead up on **semantic versioning**! This is the most common convention when it comes to versioning your software. ([here](https://www.geeksforgeeks.org/introduction-semantic-versioning/) is a simple guide, [here](https://semver.org/) is the official specification).\n\nI'd also advise you to keep a log of your commits in the description of the tag. Here's a command that does all of this for you:\n\n```\ngit tag -a <new release> -m \"$(git shortlog <last release>..HEAD)\"\n```\n\nSo, if you're releasing version `v0.2.0` after `v0.1.5`, you would run this command to tag your current commit with a good commit history:\n\n```\ngit tag -a v0.2.0 -m \"$(git shortlog v0.1.5..HEAD)\"\n```\n\nThis is post 013 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Writing good changelogs","date":"2021-02-20","tags":"guide, 100DaysToOffload"}},{"slug":"2021-02-17-notes-on-flutter-web","markdownBody":"\nThese are some notes I took for the evaluation of Flutter web for a potential project at work. I decided to build a frontend for [Miniflux](https://miniflux.app/), since I figured it may enclose many pitfalls an application could potentially have. You can find the current prototype [here](https://github.com/garritfra/FlutterFlux).\n\n## The Good\n\n- **Trivial to set up**: Running a Flutter application in a browser, no matter if it is an existing app or a fresh project, can be done by simply specifying the -d chrome flag.\n\n- **Same behavior compared to mobile app**: Since the app is quite literally a mobile application running in the browser, the page looks and feels like a mobile application. It gives the app a consistent look and feel across all devices. I can imagine this coming in handy for web applications that are primarily used on phones and tablets.\n\n- **Browser API integration**: It seems like many of the libraries make use of Web APIs. For example: I was able to get location updates using the location package, and store data using [localstorage](https://pub.dev/packages/localstorage). Whether the Web target is supported, is noted as a flag in the package documentation.\n\n- **Alternative Backends**: There are two [rendering backends](https://flutter.dev/docs/development/tools/web-renderers), both with its own benefits and drawbacks. The HTML renderer optimizes the page for the browser, which improves performance at the cost of consistency. The CanvasKit renderer renders WebGL using WebAssembly. This gives a consistent look and feel across all devices, at the cost of Performance and download size. If auto is specified, the renderer will be determined based on the device type. Here’s a comparison of the same app rendered with both backends:\n\n|                    HTML                    |                    CanvasKit                    |\n| :----------------------------------------: | :---------------------------------------------: |\n| ![](/assets/flutter_web_renderer_html.png) | ![](/assets/flutter_web_renderer_canvaskit.png) |\n\n## The Bad\n\n- **Still in Beta**: Flutter web requires the developer to use the beta channel of Flutter. I didn’t encounter any issues, but it could be that some features are unstable.\n\n- **No native HTML (With an exception)**: Flutter Web renders the application into its own container, which is not using semantic HTML. The resulting application is also not debuggable using standard web-dev tools, but flutters debugging features can be used. There is a workaround though. Using the [easy_web_view](https://pub.dev/packages/easy_web_view) package, I was able to embed html components as flutter widgets. The embedded code is actual HTML code that the browser itself is rendering, not Flutter. This solution is cross-platform, meaning that it also works flawlessly for mobile builds of the application. This might come in handy if the project demands to embed a javascript component like a video player. This approach could technically also improve SEO, but I’m unsure how a full-blown application only using this approach would behave.\n\n## The Ugly\n\n- **Scrolling feels sluggish**: The scrolling behavior is implemented by flutter itself and does not feel as smooth as the native scrolling behavior of modern browsers.\n\n- **SEO nearly impossible**: Since the application is a SPA and it is not using semantic HTML, it’s very difficult to do any kind of SEO. Lighthouse rated the demo application with a perfect 100 for SEO, but this is probably because it is only aware of the html that surrounds the flutter container. I didn’t find a way to Inject Metatags on a per-site basis.\n\n- **Heavy and slow on old devices**: Even a basic application like the Todo app is very heavy and slow when compared to a “regular” website.\n\n## Conclusion\n\nFlutter Web seems to be a viable candidate to build truly cross-platform applications. Adding Web as a target for existing Flutter mobile apps should be relatively easy. The layout will probably need to be optimized to get the full experience. Native Web APIs seem to be well supported and documented.\n\nThe resulting web application is a PWA running inside a container. It is relatively heavy and requires much more resources to run, when compared to a “regular” web application.\n\nI hope you found this useful!\n\nThis is post 012 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Flutter Web - the Good, the Bad and the Ugly","date":"2021-02-17","tags":"flutter, 100DaysToOffload, note, programming"}},{"slug":"2021-02-11-10-percent-100daystooffload","markdownBody":"\nComing into this, I didn't know what to expect. I'm not a huge writer, but so far I am pleasantly surprised about how relaxing this challenge is.\n\nAt first glance, writing a blog post every 3-5 days seems daunting. But the more I write, the more it becomes an enjoyable habit. I'm oftentimes looking forward to writing these posts. Whenever I have something on my mind, I jot it down without a plan or structure. And that's exactly the point of [#100DaysToOffload](https://100daystooffload.com/): **Just. Write.**\n\nSo far, these blog posts have helped me get a lot of my thoughts out of my head and onto paper (or on a screen). While writing, I reflect on what I think. I sometimes realize that what I thought is utter nonsense, but this in itself is an important reflection. With each post, I feel like I am getting more confident about the process.\n\nIf you're reading this and you don't have a blog yet, I would encourage you to give this technique a try. It doesn't matter that you produce quality content, nor does anyone have to see this. It's not the content that matters, but the process of producing it. Set up your own blog on [write.as](https://write.as/) or simply open your text editor of your choice and **Just. Write.**\n\nThis is post 011 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Thoughts after 10 Days of 100DaysToOffload","date":"2021-02-11","tags":"note, meta, 100DaysToOffload"}},{"slug":"2021-02-07-storage-setup","markdownBody":"\nI used to rely on Google Drive and Photos to store my entire data. Now, that [Google has decided to ditch unlimited photo storage in the near future](https://blog.google/products/photos/storage-changes/) and Google basically being the devil himself, I decided to step up my game and get my hands dirty on a DIY storage solution.\n\n## The goal\n\nBefore I got started, I thought about the expectations I have towards a system like this. It boils down to these four points (in this order): I want my solution to be **resiliant**, **scalable**, **easy to maintain** and **easy to access**. Looking back, I think I met all of these requirements fairly well. Let me walk you through how I managed to do that.\n\n## Data resiliance\n\nKeeping data on a single device is obviously a really bad idea. Drives eventually fail, which means that your data will be lost. Heck, even my house could burn down, which means that any number of local copies could burn to ashes. To prevent data loss, I strictly adhere to the [3-2-1 backup strategy](https://www.backblaze.com/blog/the-3-2-1-backup-strategy/). A 3-2-1 strategy means having **at least three total copies of your data, two of which are local but on different mediums (read: devices), and at least one copy off-site**. If a drive fails, I can replace it. If my house burns down, I get two new drives and clone my offsite backup to them.\n\nTo get an offsite backup, I set up a spare Raspberry Pi with a single large HDD and instructed it to do daily backups of my entire data. I asked a family member if they would be willing to have a tiny computer plugged in to their router 24/7, and they kindly agreed. A Pi and a HDD are very efficient in terms of power, so there is not a lot to worry about.\n\n## Scalability\n\nI currently don't have a huge amount of data. If that were to change (i.e. if I continue to shoot a lot of high-res photos and shove them into my setup), I need a way to simply attach more drives, or ones with more capacity. I looked at different file-systems that allowed to easy extendability while also being resiliant.\n\nAn obvious candidate was **ZFS**, but there are a couple of reasons I ditched this idea. First of all, it is really hard to get up and running on Raspberry Pi running Linux, since it's not natively supported by all distributions. This increases the complexity of the setup. Another reason is that I don't like the way it scales. Please correct me if I'm wrong here, since I only did limited research on this. From what I know though, ZFS can only be extended by shoving a large amount of drives in the setup to achieve perfect redundancy.\n\nIn the end, I settled on **BTRFS**. For me, it scratches all the itches that ZFS has. It is baked into the linux kernel, which makes it really easy to install on most distributions, and I can scale it to any number of drives I want. If I find a spare drive somewhere with any storage capacity, I can plug it into the system and it will just work, without having to think about balancing or redundancy shenanigans.\n\n## Maintainability\n\nI need my setup to be easy to maintain. If a drive fails, I want to be able to replace it within a matter of minutes, not hours. If my host (a Raspberry Pi) bursts into flames, I want to be able to swap in a new one and still access my data. If I'm out and about and something goes south, I want to be able to fix it remotely. BTRFS helps a lot here. It's really the foundation for all the key points mentioned here. It gives me a simple interface to maintain the data on the drives, and tries to fix issues itself whenever possible.\n\nExposing random ports to the general public is a huge security risk. To still be able to access the Pi remotely, I set up **an encrypted WireGuard tunnel**. This way, I only have to expose a single port for WireGuard to talk to the device as if I'm sitting next to it.\n\n## Accessibility\n\nSince the data needs to be accessed frequently, I need a simple interface for it that can be used on any device. I decided to host a **Nextcloud** instance and mount the drive as external storage. Why external storage? Because Nextcloud does some weird thing with the data it stores. If I decide to ditch Nextcloud at some point, I have the data on the disks \"as is\", without some sort of abstraction on top of it. This also has the benefit of allowing access from multiple sources. I don't have to use Nextcloud, but instead can mount the volume as a FTP, SMB or NFS share and do whatever I want with it. From the nextcloud perspective, this has some drawbacks like inefficient caching or file detection, but I'm willing to make that tradeoff.\n\n## In a nutshell\n\nThis entire setup cost me about 150€ in total. Some components were scraped from old PC parts. So, what does the solution look like? Here is the gist:\n\n- A Raspberry Pi 4 as a main host and an older Raspberry Pi 3 for offsite backup, both running Raspberry Pi OS\n- Two external harddrives in a RAID 1 (mirrored) configuration, running on an external USB3 hub\n- A single internal HDD that served no purpose in my old PC, now serving as backup storage\n- All drives are using BTRFS\n- WireGuard tunnels between main and remote host, as well as most access devices\n- Nextcloud on main host, accessible over TLS (if I need to access data from outside the secure tunnel-network)\n- SMB share accessible from within the tunnel-network\n- Circa 4.5 terabyte total disk size; 1.5 terabyte of usable storage\n- Snapper for local incremental backups on main host; BTRBK for remote incremental backups\n- Cron jobs for regular backups and repairs (scrub/rebalance)\n\nThis is post 010 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"My storage setup (Feburary 2021)","date":"2021-02-07","tags":"infrastructure, guide, 100DaysToOffload"}},{"slug":"2021-02-02-bem-methodology","markdownBody":"\nIn the coming weeks, months and years, I will be working on frontend-development as part of my dayjob. These are some personal notes I took during my research about the BEM methodology. If you want to read the official introduction, you should visit [their website](http://getbem.com/).\n\n# Overview - What is BEM?\n\nBEM — Block Element Modifier is a methodology that helps you to create reusable components and code sharing in front-end development. It aims to group css-classes in a meaningful way, making it easier to understand\n\n1. where this class is used\n2. what it describes and\n3. what state the element is in.\n\nThe BEM-notation is divided into three main parts: Blocks, Elements and Modifiers.\n\n## Blocks\n\nA standalone entity that is meaningful on its own. Some examples might be **headers, containers, menus, inputs, checkboxes**, etc.\n\n## Elements\n\nA part of a block that has no standalone meaning and is semantically tied to its block. This could be a **menu item or an input placeholder**.\n\n## Modifiers\n\nA flag on a block or an element. Used to change appearance or behavior. This might be **disabled, checked, fixed, big**, etc.\n\n# Putting it together\n\nA block itself is referenced though its name.\n\n```css\n.button {\n}\n```\n\nTo reference elements inside of the block, you add it to the block element with two underscores (`__`):\n\n```css\n.button {\n}\n.button__text {\n}\n```\n\nIf you want to add a modifier to a block or an element, you separate it with two dashes (`--`):\n\n```css\n.button {\n}\n.button--disabled {\n}\n.button__text--inverted {\n}\n```\n\n# Benefits of BEM\n\n**Modularity**: Block styles never depend on one another. They can easily be moved to other parts of the app.\n\n**Reusability**: Composing styles in a meaningful way reduces the amount of code duplication.\n\n**Structure**: BEM gives your code a solid structure that is both easy to understand and to expand.\n\n# References\n\n- http://getbem.com/\n- https://csswizardry.com/2013/01/mindbemding-getting-your-head-round-bem-syntax/\n\nThis is post 009 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Notes about BEM (Block Element Modifier)","date":"2021-02-02","tags":"note, css, 100DaysToOffload, programming"}},{"slug":"2021-01-29-sudo-to-doas","markdownBody":"\nYou might have heard that there is currently [a pretty significant vulnerability](https://www.qualys.com/2021/01/26/cve-2021-3156/baron-samedit-heap-based-overflow-sudo.txt) affecting `sudo`, the program we all know and love. It is the de facto standard for when you want to run a command as a priviledged user, but that's really it. Under the hood, sudo is a very powerful tool with a lot of features. It can be used to build out complex permission-systems that span across entire clusters of servers. But all of these features come at a price: **complexity**. Last time I checked, the [source code](https://www.sudo.ws/repos/sudo) of sudo had about 330k lines of code (using cloc as a benchmark). This massive complexity plays a large role in its security.\n\nLuckily, there is a **far** more lightweight alternative to sudo called [doas](https://github.com/Duncaen/OpenDoas.git). It essentially does all the things you'd expect from sudo for your average end user. Doas is written in just over 3k lines of code, which, if you think of it, should be more than enough to provide a tool that executes a command as a priviledged user.\n\n## Setup\n\nWhile there are packages for [some distibutions](https://github.com/slicer69/doas#installation-via-packagesrepositories), I personally had trouble setting it up on arch using yay (for permission reasons, ironically). I recommend going the extra mile and building it from source, which consists of a few commands and some seconds of your time:\n\n```sh\ngit clone https://github.com/slicer69/doas\ncd doas\nmake\nsudo make install\n```\n\nNext, you will need to create a config file at `/usr/local/etc/doas.conf`. Paste the following line into it to give your user root access:\n\n```sh\npermit alice as root\n```\n\nYou obviously want to substitute alice with your username. If you have multiple users on your system, simply duplicate that line and substitute the username accordingly. Just restart your terminal window, and you should be able to run programs as root using doas instead of sudo:\n\n```sh\n➜  ~ doas id\nuid=0(root) gid=0(root) groups=0(root)\n```\n\n## Bonus: Save your muscle memory\n\nIf you still want to \"use\" sudo on your machine, you can set up a simple alias in your `.{bash|zsh|fish}rc`. This will also help with compatibility issues of some scripts, if you decide to ditch the actual sudo from your Box entirely. Just paste this line into your corresponding rc file:\n\n```\nalias sudo=\"doas\"\n```\n\n## Bonus Bonus: Passwordless authentification\n\nYou can setup doas to skip the password prompt every time you run a command with it. Simply add the `nopass` option in your doas configuration file:\n\n```sh\npermit nopass alice as root\n```\n\nI hope you found this useful!\n\nThis is post 008 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"From sudo to doas","date":"2021-01-29","tags":"linux, 100DaysToOffload, guide"}},{"slug":"2021-01-26-vim-macros","markdownBody":"\nFor a long time, macros in Vim were a huge mystery for me. I knew they existed, but I didn't know how or why you'd use them. A recent task of mine involved replacing the unsafe operator (`!!`) in a large kotlin codebase with a null-safe operator (`?`). This game me a good opportunity to learn about macros. This is a snippet I encountered numerous times:\n\n```kt\nmLeftButton!!.text = \"Left\"\nmLeftButton!!.setOnClickListener(leftListener)\nmLeftButton!!.visibility = View.VISIBLE\nmRightButton!!.text = \"Right\"\nmRightButton!!.setOnClickListener(rightListener)\nmRightButton!!.visibility = View.VISIBLE\n```\n\nYou could go ahead and change each line individually, or use the IDEs built in \"multi-cursor\" tool to save you some work. But, let me show you how I automated this using a Vim-Plugin for Android Studio. Not that the plugin matter, it will work in every Vim-like editor.\n\nA macro in Vim works like this:\n\n1. Record any sequence of keystrokes and assign them to a key\n1. Execute that sequence as often as you wish\n\nSo let's see how we'd do that.\n\n## Recording a macro\n\nTo record a macro in Vim, you press `q` (In normal mode) followed by a key you want to assign the macro to. So, if you wanted to record a macro and save it to the `q` key, you'd press `qq`. Vim will notify you that a macro is being recorded. Now, you can press the keystrokes that define your actions. When you're done, press `q` in normal mode again to quit your macro.\n\nComing back to my task, I would want to do the following:\n\n1. `qq` Record a macro and save it to the `q` key\n1. `_` - Jump to the beginning of the line\n1. `f!` - Find next occurrence of `!`\n1. `cw` - Change word (Delete word and enter insert mode)\n1. `?.` - Insert the new characters\n1. `<esc>` - Enter normal mode\n1. `j` - go down a line\n1. `q` - Finish macro\n\nIf everything went right, this line:\n\n```\nmLeftButton!!.text = \"Left\"\n```\n\nShould now look like this:\n\n```\nmLeftButton?.text = \"Left\"\n```\n\nand your macro should be saved under the `q` key.\n\n## Using the macro\n\nIn order to use a macro in vim, you press the `@` key, followed by the key the macro is saved under. Since our macro is defined as `q`, we'd press `@q`, and the macro is executed immediately.\n\nLet's take this further. You might have noticed that I went down a line before closing the macro. This becomes handy when you want to execute it many times. In our case we have 6 lines we want to refactor. 1 line has already been altered, so we have to execute it 5 more times. As per usual with vim, you can execute an action n times by specifying a number before doing the action. Let's press `5@q` to execute the macro 5 times. And voila! Our unsafe code is now null-safe.\n\n```kt\nmLeftButton?.text = \"Left\"\nmLeftButton?.setOnClickListener(leftListener)\nmLeftButton?.visibility = View.VISIBLE\nmRightButton?.text = \"Right\"\nmRightButton?.setOnClickListener(rightListener)\nmRightButton?.visibility = View.VISIBLE\n```\n\nMacros are really satisfying to watch, if you ask me!\n\nThis is post 007 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Using Macros in Vim","date":"2021-01-26","tags":"vim, 100DaysToOffload, guide"}},{"slug":"2021-01-23-signal-to-noise","markdownBody":"\nFor a very long time, the thought of leaving GitHub and moving to another platform daunted me. Having more users on one platforms means that more people will contribute to my project, right? Wrong.\n\nThe problem with GitHub is that there's a lot of things going on around you. How many times have you discovered a cool project on GitHub, starred it and never heard from it again? In essence, this is the same phenomenon as with modern social media. A bombardment of positive stimulants makes the user crave for more, letting them forget about previously consumed content. Sure, if you just want to get your code out there, GitHub might be a great place, but if you are just starting out as a developer and you're looking for contributers and feedback, you will probably be very bummed to find out that nobody cares about your work. Many developers are using the platform because other developers are using it. Your project on GitHub is a drop in an ocean of other projects.\n\nA few months ago, I decided to make the leap and switch most of my development over to [Sourcehut](https://sourcehut.org/), a free and open source code-hosting platform. Besides its great tooling (mailing lists, automated builds, etc.), it has the benefit of a high **signal-to-noise ratio**. Less developers are using the platform, but most of them are very passionate about their work. They care about collaborating with others and they believe in what they are doing, which probably lead them to sign up for this platform in the first place.\n\nOf course, switching away from a platform like GitHub alone does not ensure more contributions. You might be trying to advertise your projects by spamming links on popular newsboards and forums, but this only generates **noise**. Instead, you should intentionally talk about your personal journey with the project in a smaller circle. If other developers in your niche see that you continuously give updates about the project and its improvements, they will eventually start to relate to it. Some of them will look at your project and give feedback, or even contribute patches.\n\nThis is post 006 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Signal-to-Noise, or why nobody cares about your GitHub project","date":"2021-01-23","tags":"note, 100DaysToOffload"}},{"slug":"2021-01-18-reasons-the-fediverse-is-better","markdownBody":"\nSocial media sucks. Platforms like Twitter, Facebook and Instagram are designed to turn your precious free time into money. What we see as a nice way to stay in touch with our friends, in reality are just many hits of dopamine stimulating precise spots in your brain, leading to you spending more time on the platform consuming ads.\n\nBut what if I told you that there is a huge ad-free social network out there, not governed by a central authority, full of great people and completely free to use? This place is called the fediverse. Well, it's not really **a** place, it's many places.\n\n## What is the fediverse?\n\nAt its core, the fediverse is a mesh of interconnected nodes on the internet, all communicating in the same language. Every instance on the fediverse implements the [ActivityPub](https://activitypub.rocks/) protocol, which allows it to talk to other instances on the network. The phrase \"Fedi\" comes from \"federated\", meaning that content on the network is shared and accessible by anyone.\n\nThere are many different software projects for the fediverse out there. If you like the concept of Twitter, you could take a look at [Mastodon](https://joinmastodon.org/), a microblogging-platform for the fediverse. There's also [PeerTube](https://joinpeertube.org/), a federated clone of YouTube (PeerTube recently [released support for peer2peer live streaming support](https://framablog.org/2021/01/07/peertube-v3-its-a-live-a-liiiiive/), like what?! 🤯). You like instagram? [Pixelfed](https://pixelfed.org/) got you covered. Of course, there are many other services worth mentioning, so feel free to dig around a bit! As mentioned, the great thing about the fediverse is that all of these services are connected with each other. If I signed up for an account on a mastodon instance, I can subscribe to your posts on Pixelfed, and vice versa. If I want to get notified about your videos on PeerTube, I can just go ahead and follow your account and comment on your videos.\n\n> \"This all sounds great, but why should I bother?\"\n\nLet me give you 6 reasons why the fediverse is far superior to all other social media platforms out there, and why you should consider signing up for an account on one of the many instances of the fediverse.\n\n## Reason 1: It's decentralized\n\nRegular social media platforms like Facebook have a **single point of failure**. If their servers go down, your content goes down with it. Content on the fediverse on the other hand is scattered around many instances, which means it is very resilient. If your instance dies, you can move to a new instance. This goes hand in hand with the next reason.\n\n## Reason 2: It can't be censored\n\nYou probably heard that the Twitter-account of Donald Trump recently got compromised by the owners of the platform. I don't want to engage in any political discussions, but the main flaw with this is the violation of **freedom of speech**. Even if everything someone says is controversial nonsense, it is still in his good right to express his thoughts.\n\nOn the fediverse, a scenario like this would certainly not happen, given its decentralized nature. Some instances still moderate their content, meaning that if someone posts inappropriate content, it might get blocked. The twist here is, if that person disagrees with the rules of the instance, he is free to join another instance.\n\n## Reason 3: Free as in freedom\n\nThere's this saying, criticizing modern software projects:\n\n> \"If it's free, you're the product\"\n\nThis is not true in all cases. \"Free\" can be understood in two ways.\n\n**\"Free as in beer\"** means that something might seem free at first glance (E.g. Free beer at Oktoberfest), but in the end you often leave with less than you came with. In the case of beer, you often buy another beer after the first free one. Therefore, even though you think you've saved the money for one beer, in reality bought an extra beer. In the case of proprietary software, it's a similar story. While you think that a service is free, you give up your privacy and get monetized with ads.\n\n**\"Free as in freedom\"** on the other hand means that you won't get \"screwed over\" like this. Most, if not all of the software for the fediverse is **free and open source software** (FOSS). If you don't like how a certain feature works, **you are completely free to change it**. You can look at the source code and propose changes to the main project, or launch your own spin of that product.\n\nSince everyone can openly look at the source code, it is **audited** by many people, including security experts. This vastly improves the security and stability of the product. If the developers would do shady things, they will most certainly get called out by people, as soon as that code enters the main repositories. Proprietary (closed sourced) platforms like Facebook and Twitter can not be audited. The owners can do whatever they want, including spying on their users, or collect and sell the data of their users.\n\n## Reason 4: It respects your privacy\n\nSince the software on the fediverse is audited by a lot of people, you can be almost 100% certain that joining an instance will not collect any of your personal data. If you are still concerned about your privacy though, you can still be part of the network by launching your own instance, with your own rules. There are tutorials out there, explaining how you can set up a small instance for a very cheap price on your local network.\n\n## Reason 5: It's all about the community\n\nI used to spend a lot of time on \"regular\" social media platforms. From personal experiences, these platforms are all about promoting yourself and building up your follower count. Connecting with your friends is of little importance. In the eye of some people, you are not worthy to talk to if the amount of followers or likes-per-post didn't exceed a certain threshold.\n\nIt has now been about 5 months since I created [my mastodon account](https://fosstodon.org/@garritfra). Talking to people on the fediverse is a completely different experience compared to Facebook or Twitter. Almost everyone I talked to is a polite, grounded person, willing to engage in constructive and fun discussions. I met many people who disagree with my views, but instead of leaving a comment saying that this post sucks, all of them took the time to express their alternative opinions. Every member of the fediverse wants to drive the network forward, which is reflected in their posts.\n\n## Reason 6: There's an instance for everyone\n\nWhether you're into Gaming, Painting, or Spanish dancing music, there is an instance for you. If it isn't, you are free to create one and promote it to people of that niche. You won't loose the social aspect by launching your own instance, since you are still available to other people on the network. If you just want to get started with the fediverse, I recommend that you check out one of the many [lists of mastodon instances](https://instances.social/). If you like instagram and want to stay in a familiar environment, take a look at [Pixelfed](https://pixelfed.org/), and join an instance from the list they provide.\n\nI myself am a person who cares a lot about free and open source software, therefore my choice of instance was [fosstodon.org](https://fosstodon.org/), a mastodon instance geared towards awesome like-minded people.\n\n**Note**: This post has generated some interesting discussions on [Hacker News](https://news.ycombinator.com/item?id=25820646).\n\nThis is post 005 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"6 reasons the Fediverse is better than regular social media","date":"2021-01-18","tags":"note, 100DaysToOffload, fediverse, socialmedia"}},{"slug":"2021-01-15-compiling-your-own-kernel","markdownBody":"\nI'm currently in the midst of fiddling around with the kernel a bit, and I figured I just documented my process a bit. Unfortunately, since I'm using a Mac for day to day work, I have to rely on a virtual machine to run anything Linux-related. VirtualBox doesn't support the most recent kernels (5.9 is the most recent supported one), so there won't be any cutting-edge development happening here. I decided to use ubuntu as my guest system, since it's very easy to set up.\n\nSo, the first step is to get the sources. You could simply go ahead and download a specific release from [kernel.org](https://kernel.org/), but since I want to hack on it, I decided to go the git-route. Simply download the sources from [their repo](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/) and check out the tag you want to build.\n\n> **Note**: this might take a while. Their repository is huge! If you want to only need the `HEAD` and want to build on bare-metal (no VirtualBox), you could only clone the latest commit using `git clone git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git --depth=1`.\n\nNext up, you need to generate a `.config`. This file describes which features you want to compile into your kernel. To make a generic config that only compiles drivers for the hardware of your system, you can run the following commands:\n\n```bash\n# Copy the config of your current kernel into the repo\nmake oldconfig\n\n# Only enable modules that are currently used by the system\nmake localmodconfig\n```\n\nNow, let's get to actually compiling the kernel. In my case, I assigned 4 cores to my VM. The `-j` option tells make to run 4 jobs in parallel.\n\n> **Caution**: Just providing -j will freeze your system, since make will try to launch an infinite amount of processes!\n\n```\nmake -j4\n```\n\nAgain, this might take some time. Go for a walk, get a coffee or watch your favorite TV-show. After compilation has finished, we need to install the kernel. To do so, run the following commands:\n\n```\nsudo make modules_install\nsudo make install\n```\n\nIn order to boot, we need to tell our bootloader about our new kernel. Run this command to update your grub config:\n\n```\nsudo update-grub2\n```\n\nAnd voila! Your new kernel should be ready.\n\nReboot the system, and grub should pick up the new kernel and boot to it. If that's not the case, you should be able to pick the kernel from the grub menu under `advanced options`.\n\n## Retrospective\n\nI found that building my own kernel is a highly educational and fun experience. Using VirtualBox is a pain in the `/dev/null` to work with, since it has to add a lot of overhead to the system in order to work. You sometimes have to wait over 6 month until the support for a new kernel arrives. This problem should not apply if you compile on bare metal systems.\n\nThanks for your time!\n\nThis is post 004 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Compiling your own kernel","date":"2021-01-15","tags":"linux, guide, 100DaysToOffload, infrastructure"}},{"slug":"2021-01-13-512kb-club","markdownBody":"\nJavaScript rules the web, literally. In fact, this website is built with JavaScript (Next.js). I recently started to think about if I really needed this much overhead for a simple site like this. After all, I don't have any fancy user interaction features or complex animation that would justify the JavaScript on this page.\n\nThere is a new (no, not that new) philosophy called [the lean web](https://leanweb.dev/). It essentially tries to keep websites tiny and semantically correct. This has many benefits, ranging from less pollution generated by your site to improved SEO, since many search engines favor a semantically correct website over a site that abuses JavaScript to mimic the features, that are baked into html anyway.\n\nIn order to get lean, I decided to join [the 512KB club](https://512kb.club/). This website lists sites that are below 512KB in total (uncompressed, with all dependencies). To get below that mark, I had to remove my face from the frontpage (I'm sure you'll miss it😅), since the image itself was roughly 750KB. I'm now just below 500KB, which qualifies me to join the blue team.\n\n[![Blue Team](https://512kb.club/images/blue-team.svg)](https://512kb.club)\n\nI'm not planning to stop here though. I think keeping a website small and simple is an excellent practice. My next step will be to get rid of all the JS junk on this site and only rely on HTML and CSS. I still want to be able to write my posts in Markdown, so I will have to come up with a way to generate pages from them. A safe bet for this would be to use a SSG like [Hugo](https://gohugo.io/). Frankly, [writing my own simple SSG probably wouldn't hurt either](https://erikwinter.nl/articles/2020/why-i-built-my-own-shitty-static-site-generator/). Let's see how high I can climb the ranks of the 512KB club. Care to join me?\n\nThis is post 003 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"I joined the 512KB club","date":"2021-01-13","tags":"web, javascript, 100DaysToOffload, meta"}},{"slug":"2021-01-11-are-humans-still-evolving","markdownBody":"\nThis is by no means a scientifically accurate article. These are my thoughts, and I'd be happy to discuss this topic. Take it with a grain of salt.\n\nEvolution builds upon natural selection. With every generation of a species, there is a slight chance of mutation, possibly giving an individual advantages or disadvantages in the ability to survive and give offspring. Somewhere way up in our evolutionary tree, a microbe might have mutated a gene that allowed it expand and retract a part of its body. As it turns out, this proves useful in fleeing from predators, while other individuals of this species might fall prey on their first day. The microbe has a slightly larger chance to survive and give offspring. On the other hand, mutations might also result in a fatal illness (cancer, in other words). Oftentimes, this individual does not survive long enough to give offspring.\n\n150 years ago, giving birth was literally an act of life and death. Many children died at a young age. They were not tough enough from an evolutionary standpoint, and were therefore \"filtered\" out by natural selection. Only the strongest survived and gave offspring.\n\nTodays medicine is (fortunately!) very powerful. Very few children die at birth and a lot less people are dying from an illness like the flu. This is an evolutionary anomaly. Natural selection has been defeated to a certain degree. We don't need to run from predators anymore and are less prone to desease. Every one of us is more or less equally able to give offspring. We can't really gain an advantage over others anymore. We are not evolving.\n\nThis is post 002 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Are humans still evolving?","date":"2021-01-11","tags":"note, 100DaysToOffload"}},{"slug":"2021-01-11-100daystooffload","markdownBody":"\nFor some time now, I've seen this #100DaysToOffload hashtag on my social medias. I knew that it was some kind of writing challenge, but I never thought about taking part in it. Since I recently started to blog more frequently though, I think this challenge could be very beneficial to my writing skills, and just jotting my thoughts down in general. So, starting with this entry, I will try to publish 100 (hopefully) useful posts on this blog within one year. My \"deadline\" for this will be January 11, 2022. I will post every entry to [my mastodon account](https://fosstodon.org/@garritfra).\n\nThis is post 001 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"100DaysToOffload","date":"2021-01-11","tags":"note, 100DaysToOffload"}},{"slug":"2021-01-07-delete-facebook","markdownBody":"\nI know I should have done this a while ago, but with ever-increasing scandals about data privacy surrounding Facebook, I finally decided to get rid of it.\n\nI haven't used the service in a long time anyway, but I always told myself \"what if I needed the data later?\", or \"what if a friend contacted me, and I didn't respond?\", \"what if I missed the birthday of someone I'm close with?!\". Well, according to my facebook inbox, the only messages I received lately were some random links from people I'm not really in touch with anymore. Birthdays? Do you think someone you haven't talked to in over three years will get mad at you, for forgetting their birthday? And regarding your data: you won't loose it! [This guide](https://www.facebook.com/help/212802592074644) describes how you can download a copy of your data as html and/or json.\n\nGo ahead and ask yourself: Is there anything holding you back from deleting your Facebook account? What would you loose? How often do you even use the service? Do the social benefits of Facebook **really** outweigh the negative aspects (privacy concerns, data collection, etc.)?\n","frontmatter":{"title":"I closed my Facebook account, and you should too","date":"2021-01-07","tags":"privacy, 100DaysToOffload, note"}},{"slug":"2020-12-18-update-december","markdownBody":"\nIt's christmas season! I hope you and your family are safe and sound. My main focus this month is to expand my knowledge about compilers, by [building one from scratch](https://sr.ht/~garritfra/sabre/). It's a compiler for a very simple language, but it gets the job done. So far, you can write [simple algorithms](https://git.sr.ht/~garritfra/sabre/tree/master/examples), and compile them to JavaScript. A more sophisticated C backend is in development, but I still need a plan for expanding the target-specific builtin functions to provide more features in the standard library. An important topic at the moment is the [documentation of the project](https://garritfra.github.io/sabre/). Since the compiler itself has gotten relatively stable, all the language-features now need to be captured and written down. There is also a [contributing guide](https://garritfra.github.io/sabre/developers/contributing.html), if you want to help out, or want to get into compiler design.\n\nStay home and stay safe!\n","frontmatter":{"title":"Updates, December 2020","date":"2020-12-18","tags":"update, note"}},{"slug":"2020-11-17-booleans-are-wasted-memory","markdownBody":"\nA boolean is either `true` or `false`. That translates to `1` or `0`. If you think that one bit is enough to store this information, you'd be wrong.\n\nIn order to keep the binary layout of a program simple and convenient, most languages store information in 8 bit (1 byte) blocks.\nIf you allocate a `bool` in Rust or (most) other languages that are based on LLVM, [it will take up 1 `i1`, or 1 byte of memory](https://llvm.org/docs/LangRef.html#simple-constants). If you allocate a boolean value in C, you will get [an integer constant with a value of either 1 or 0](https://pubs.opengroup.org/onlinepubs/9699919799/basedefs/stdbool.h.html).\n\nIf you find yourself having to store multiple boolean states somewhere, you might simply declare those booleans and call it a day:\n\n```c\n#include <stdbool.h>\n#include <stdio.h>\nint main()\n{\n    bool can_read = true;\n    bool can_write = true;\n    bool can_execute = false;\n\n    if (can_read)\n        printf(\"read bit set\\n\");\n    if (can_write)\n        printf(\"write bit set\\n\");\n    if (can_execute)\n        printf(\"execute bit set\\n\");\n\n    // Output:\n    // read bit set\n    // write bit set\n}\n```\n\n## We can do better than this\n\nAn alternative approach to store boolean values is to share a \"chunk\" of bits with other values. This is usually done using bitwise operations:\n\n```c\n#include <stdbool.h>\n#include <stdio.h>\n\n// Define permissions\n#define PERM_NONE       0b000\n#define PERM_READ       0b001\n#define PERM_WRITE      0b010\n#define PERM_EXECUTE    0b100\n\n#define PERM_ALL        PERM_READ | PERM_WRITE | PERM_EXECUTE\n\nint main()\n{\n    // Allocate 1 byte for permissions\n    char permissions = PERM_READ | PERM_WRITE;\n\n    if (permissions & PERM_READ)\n        printf(\"write bit set\\n\");\n    if (permissions & PERM_WRITE)\n        printf(\"read bit set\\n\");\n    if (permissions & PERM_EXECUTE)\n        printf(\"execute bit set\\n\");\n\n    // Output:\n    // read bit set\n    // write bit set\n}\n```\n\nThis example still wastes 5 bits since we only use 3 out of 8 possible bits of the char type, but I'm sure you get the point. Allocating 3 boolean values independently would waste 7 \\* 3 = 21 bits, so it's a massive improvement. Whenever you find yourself needing multiple boolean values, think twice if you can use this pattern.\n\nMicrocontrollers have a very constrainted environment, therefore bitwise operations are essential in those scenarios. 7 wasted bits are a lot if there are only 4 kb of total memory available. For larger systems we often forget about these constraints, until they add up.\n\n## My Plea\n\n- Be mindful about the software you create.\n- Appreciate the resources at your disposal.\n","frontmatter":{"title":"Booleans are wasted memory","date":"2020-11-17","tags":"note, guide, programming"}},{"slug":"2020-11-06-current-doings","markdownBody":"\nHi, I wanted to share some things I'm currently working on. Maybe I'll turn this into a monthly thing, who knows. :)\n\nOne major goal I set for myself in the upcoming months is to build a SaaS for freelancers. Some features of this will include handling clients, projects and expenses. A thing I'm struggeling with right now it to find a lightweight way to host it. It is currently deployable through docker containers, but I am not 100% satisfied with my current setup. I will give some updates on this in the future. I aim to release a very early alpha version for free soon, so that some people can stress test it extensively. For now, you can of course self-host it. I really don't want to impose subscription fees for it's users, but I will see how it goes. You can find the source code for it [here](https://github.com/garritfra/omega-crm).\n\nRecently, I increasingly gained interested in the [Gemini project](https://gemini.circumlunar.space/). In a nutshell, this is a very minimal alternative to HTTP, with a strong emphasis on simplicity. The maintainers clearly embrace a DIY mindset, which I want to follow. I set myself the rule to only interact with gemini using tools I wrote myself. To achive this, I am currently writing [my own Gemini server called \"Taurus\"](https://git.sr.ht/~garritfra/taurus) to eventually set up my own geminispace. I have not yet looked deeply into building a client, but I might do this once I'm happy with my server. I admit that I'm currently cheating a bit, by testing my server using a browser recommended by the gemini team ;)\nIf you are interested in this project, I highly recommend to check out the [gemini specification](https://gemini.circumlunar.space/docs/specification.html), and play around with some geminispaces. Maybe you could set up a server for yourself?\n","frontmatter":{"title":"Updates, November 2020","date":"2020-11-06","tags":"update, note"}}],"description":""},"__N_SSG":true}