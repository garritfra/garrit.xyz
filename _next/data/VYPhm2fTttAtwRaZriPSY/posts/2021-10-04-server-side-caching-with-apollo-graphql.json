{"pageProps":{"post":{"slug":"2021-10-04-server-side-caching-with-apollo-graphql","markdownBody":"\nI recently implemented server-side caching for one of our applications at work.\nThis guide tries to document that I've learned. It assumes that you are using\nan apollo server of version 3 or higher.\n\n### What is Server-Side Caching?\n\nThe point of server-side caching is to reduce the load of your database by\nâ€œrememberingâ€ the results of a query for a certain period. If the exact same\nquery comes in again, that remembered result will be returned.\n\nCaching should be handled with care. You should never enable caching for your\nentire application. Instead, you should identify the bottlenecks and develop a\nstrategy to overcome them.\n\n### Enabling caching on the server\n\nThe Apollo Team has done a great job\n[documenting](https://www.apollographql.com/docs/apollo-server/performance/caching/)\nthe caching behavior of their server. To add caching to your existing\nApollo-Server, you first have to add the `responseCachePlugin` to your\nconfiguration as shown\n[here](https://www.apollographql.com/docs/apollo-server/performance/caching/#caching-with-responsecacheplugin-advanced):\n\n```js\nimport responseCachePlugin from \"apollo-server-plugin-response-cache\";\n\nconst server = new ApolloServer({\n\t// ...other options...\n\tplugins: [responseCachePlugin()],\n});\n```\n\nThen, you have to configure a cache backend. By default, Apollo Server will\nstore the caches in RAM, but Iâ€™d recommend [using\nRedis](https://www.apollographql.com/docs/apollo-server/data/data-sources/#using-memcachedredis-as-a-cache-storage-backend)\n(or Memcached, if you like), especially if your application is spread across\nmultiple instances of the same backend.\n\n```js\nconst { BaseRedisCache } = require(\"apollo-server-cache-redis\");\nconst Redis = require(\"ioredis\");\n\nconst server = new ApolloServer({\n\t// ...\n\tcache: new BaseRedisCache({\n\t\tplugins: [responseCachePlugin()],\n\t\tclient: new Redis({\n\t\t\thost: \"redis-server\",\n\t\t}),\n\t}),\n});\n```\n\n> Note that you have to use the ioredis library here. node_redis is deprecated\n> as of v2.6.0 of apollo-server-cache-redis.\n\nIf everything went well, your server should now know how to cache responses!\nThis alone wonâ€™t get you very far, since it doesnâ€™t know what to cache.\n\n### Telling Apollo what to cache\n\nTo make a type cachable, you have to declare **cache hints**. These properties\ncan either be set in the\n[resolver](https://www.apollographql.com/docs/apollo-server/performance/caching/#in-your-resolvers-dynamic),\nor\n[statically](https://www.apollographql.com/docs/apollo-server/performance/caching/#in-your-schema-static)\nin the schema. To keep it simple, this guide will stick to the static method.\nFeel free to experiment with the dynamic approach though!\n\nTo enable cache hints, simply add the following directive to your schema (you\nonly have to do this once):\n\n```gql\nenum CacheControlScope {\n\tPUBLIC\n\tPRIVATE\n}\n\ndirective @cacheControl(\n\tmaxAge: Int\n\tscope: CacheControlScope\n\tinheritMaxAge: Boolean\n) on FIELD_DEFINITION | OBJECT | INTERFACE | UNION\n```\n\nNow you can add the `@cacheControl` directive to every type that should be cached.\n\n```gql\n# This type will be cached for 30 seconds\ntype Post @cacheControl(maxAge: 30) {\n\tid: ID!\n\ttitle: String\n\tauthor: Author\n\tcomments: [Comment]\n}\n```\n\nFor security reasons, these conditions are [very\nstrict](https://www.apollographql.com/docs/apollo-server/performance/caching/#why-are-these-the-maxage-defaults):\n\n> Our philosophy behind Apollo Server caching is that a response should only be\n> considered cacheable if every part of that response opts in to being\n> cacheable.\n\nThis means that every type needs to explicitly decide how long it should be\ncached. According to this note, the example above actually wonâ€™t be cached at\nall!\n\nHaving to specify the `maxAge` of every type would be tedious, so the authors\nhave come up with the `inheritMaxAge` property, which allows the type to\ninherit the settings from its parent. So, in order to make our example\ncachable, we have to enable cache control for all its subfields, either by\nsetting the `maxAge` explicitly or by inheriting it from the parent:\n\n```gql\ntype Post @cacheControl(maxAge: 30) {\n\tid: ID!\n\ttitle: String\n\tauthor: Author\n\tcomments: [Comment]\n}\n\ntype Author @cacheControl(inheritMaxAge: true) {\n\tid: ID!\n\tname: String\n}\n\ntype Comment @cacheControl(inheritMaxAge: true) {\n\tid: ID!\n\tbody: String\n}\n```\n\nNow, whenever you query a `Post`, it will be thrown in the cache. If you query\nthe type again within 30 seconds, the query resolver wonâ€™t execute. Instead, it\nwill be read from the cache. Keep in mind that cache hints can also be set on\n`query` and `mutation` fields. This can be handy if you want to cache the\nentire response of a request.\n\n### Gotcha 1: Multiple Response Variations\n\nThe use-case where this topic first came up required us to have different\nresponses based on the type of the logged in user. An `Admin` should see a\ndifferent result than a `Visitor`. If you ignore this fact, it could be that a\nvisitor could see the cache result of a query previously executed by an admin!\n\nThis problem can be counteracted by setting extra information in the cache key\nvia `extraCacheKeyData` (see\n[this](https://www.apollographql.com/docs/apollo-server/performance/caching/#configuring-reads-and-writes)\nparagraph):\n\n```js\nplugins: [\n    responseCachePlugin({\n        extraCacheKeyData: (ctx) => (\n            ctx.context.auth.isAdmin\n        ),\n    }),\n],\n```\n\nThis example can create two distinct caches: One for users that are marked as\nadmins, and one for regular users.\n\n### Gotcha 2: User-specific caches\n\nBesides caching for a group of users, you can also cache responses [for every\nuser\nindividually](https://www.apollographql.com/docs/apollo-server/performance/caching/#identifying-users-for-private-responses).\nYou may have noticed that you can also set a `scope` field in the cache control\ndirective. This will only cache the response if a user is logged in:\n\n```gql\ntype Post {\n\tid: ID!\n\ttitle: String\n\tauthor: Author @cacheControl(maxAge: 10, scope: PRIVATE)\n}\n```\n\nApollo determines if a user is logged in or not, based on if the `sessionId`\nfunction has returned a value other than `null`.\n\n```js\nimport responseCachePlugin from \"apollo-server-plugin-response-cache\";\nconst server = new ApolloServer({\n\t// ...other settings...\n\tplugins: [\n\t\tresponseCachePlugin({\n\t\t\tsessionId: (requestContext) =>\n\t\t\t\trequestContext.request.http.headers.get(\"sessionid\") || null,\n\t\t}),\n\t],\n});\n```\n\nIâ€™m unsure how effective this pattern is, since every user will receive its key\nin the cache. This kind of defeats the purpose of server-side caching, which is\nmeant to reduce load on the database. If youâ€™re trying to cache fields for\nindividual users, you might also want to take a look at client-side caching via\n[apollo-augmented-hooks](https://github.com/appmotion/apollo-augmented-hooks).\n\nThis is post 020 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Server-Side Caching with Apollo GraphQL","date":"2021-10-04","tags":"javascript, graphql, guide, 100DaysToOffload, programming"},"tags":["javascript","graphql","guide","100DaysToOffload","programming"]},"recommendedPosts":[{"slug":"2023-06-01-single-page-applications-on-github-pages","markdownBody":"\nMy latest project, [sendpasswords.net](https://sendpasswords.net/) is a [Single Page Application](https://developer.mozilla.org/en-US/docs/Glossary/SPA) deployed on GitHub Pages.\n\nGitHub Pages is configured in a way to host static HTML files without any bells and whistles. This means that if you try to fetch a document that's *not* the index, for example `/foo`, the server will try to load the file with that name. \n\nBy nature, SPAs only consist of a single HTML entry point (`index.html` in most cases). It's responsible for routing the user to the correct page if there are multiple paths. And here's the crux: if the user tries to load `/foo`, he will not land at the SPA entry point. Instead, he will see a `404` error.\n\n## The solution\n\nA `404` response will automatically return a file called `404.html`, which we can use to our advantage. After building the application, simply copy the `index.html` to `404.html`, as demonstrated by [this commit](https://github.com/garritfra/sendpasswords.net/commit/66bdb68c229a3ac3386f7816a746155e658eb586). This will use `index.html` to serve the application on the root level, and `404.html` to load *the same app* if the page doesn't exist as a file. Whether the `index.html` is needed if there's already a `404.html` is up to you. I left it in to make clear that this is just a workaround.\n\nThis is a [well known](https://stackoverflow.com/a/69308662/9046809) workaround, but I wanted to bring some extra awareness to it, since it's a problem I ran into a couple of times so far. Happy SPAing!\n\n---\n\nThis is post 069 (nice) of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Single Page Applications on GitHub Pages","date":"2023-06-01","tags":"100DaysToOffload, guide, note, web, javascript, github"},"tags":["100DaysToOffload","guide","note","web","javascript","github"]},{"slug":"2023-04-27-migrating-homeassistant-from-sd-to-ssd","markdownBody":"\nI finally got frustrated with the performance of my Raspberry Pi 4 running Homeassistant on a SD card, so I went ahead and got an SSD.\r\n\r\nThe migration was **very** easy:\r\n\r\n1. Create and download a full backup through the UI\r\n2. Flash Homeassistant onto the SSD\r\n3. Remove the SD card and plug the SSD into a USB 3.0 port of the Pi\r\n4. Boot\r\n5. Go through the onboarding procedure\r\n6. Restore Backup\r\n7. Profit\r\n\r\nIt worked like a charm! The speed has improved A LOT, and everything was set up as it should be. \r\n\r\n...Until we turned on the lights in the livingroom. My ZigBee-dongle, plugged into another USB port, wasn't able to communicate with the devices on the network.\r\n\r\nAfter some digging around, I came across several threads stating that an SSD over USB 3.0 apparently creates a lot of interference to surrounding hardware, including my ZigBee dongle. The fix was simple: either get an extension port for the dongle, or plug the SSD into a USB 2.0 port of the Pi. Since I didn't have an extension cord to get the dongle far away enough from the SSD, I went with the latter option for now. And that fixed it! The performance was much worse, but still better than the SD I used before. My next step will be to grab an extension cord from my parents. I'm sure they won't mind.\r\n\r\nI hope this helps!\r\n\r\n---\r\n\r\nThis is post 066 of [#100DaysToOffload](https://100daystooffload.com/).\r\n\r\n\r\n\r\n\r\n\n","frontmatter":{"title":"Migrating Homeassistant from SD to SSD","date":"2023-04-27","tags":"100DaysToOffload, guide, note, homeassistant, homelab"},"tags":["100DaysToOffload","guide","note","homeassistant","homelab"]},{"slug":"2023-04-12-instant-dark-theme","markdownBody":"\nThanks to [Jacksons](https://jacksonchen666.com/) [update to darktheme.club](https://github.com/garritfra/darktheme.club/pull/79), I just came across a neat little [CSS property](https://developer.mozilla.org/en-US/docs/Web/CSS/color-scheme) that turns a mostly CSS-free document into a pleasantly dark site:\r\n\r\n```css\r\n:root {\r\n  color-scheme: light dark;\r\n}\r\n```\r\n\r\nThis will adjust all elements on the page to the color scheme preferred by the user - without any other custom styles! ðŸ¤¯ It is also [widely supported](https://caniuse.com/mdn-css_properties_color-scheme) by browsers.\r\n\r\nI've always been quite dependent on CSS-frameworks for any project I'm starting. Going forward, I'd be interested to see how framework-less sites would feel using this property. If all else fails, there's always the awesome [simple.css](https://simplecss.org/) library, which you can slap on top of a raw document to make it pretty (and dark, if preferred) without using custom classes.\r\n\r\n---\r\n\r\nThis is post 064 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Instant dark theme","date":"2023-04-12","tags":"100DaysToOffload, guide, note, learnings, web, css, til"},"tags":["100DaysToOffload","guide","note","learnings","web","css","til"]},{"slug":"2023-03-30-designing-resilient-cloud-infrastructure","markdownBody":"\r\nAs mentioned in a [previous post](/posts/2023-03-16-terraform-project-learnings), I'm currently finishing up building my first cloud infrastructure on AWS for a client at work. During the development, I learned a lot about designing components to be resilient and scalable. Here are some key takeaways.\r\n\r\nOne of the most critical components of a resilient infrastructure is redundancy. On AWS, you place your components inside a \"region\". This could be `eu-central-1` (Frankfurt) or `us-east-1` (North Virgina), etc. To further reduce the risk of an outage, each region is divided into multiple [Availability Zones](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvailabilityZones.html) (AZs). The AZs of a region are usually located some distance apart from each other. In case of a flood, a fire or a bomb detonating near one AZ, the other AZs should in most cases still be intact. You should have at least two, preferably three replicas of each component across multiple availability zones in a region. By having replicas of your components in different availability zones, you reduce the risk of downtime caused by an outage in a single availability zone.\r\n\r\nAnother way to ensure scalability and resilience for your database is to use [Aurora Serverless v2](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless-v2.html). This database service is specifically designed for scalable, on-demand, and cost-effective performance. The database scales itself up or down based on the workload, which allows you to automatically and dynamically adjust the database capacity to meet the demand of your application, ensuring that your application is responsive and performs well without the need for manual intervention. Adding Serverless instances to an existing RDS cluster is also a seemless proccess.\r\n\r\nIn addition to switching to Aurora Serverless v2, using read replicas for cache and database in a separate availability zone can act as a hot standby without extra configuration. Keep in mind that read replicas are only utilized by explicitly using the read-only endpoint of a cluster. But even if you're only using the \"main\" cluster endpoint (and therefore just the primary instance), a read replica can promote itself to the primary instance in case of a fail over, which drastically reduces downtime.\r\n\r\nWhen using Amazon Elastic Container Service (ECS), use Fargate as opposed to EC2 instances. Fargate is a serverless compute engine for containers that allows you to run containers without having to manage the underlying infrastructure. It smartly locates instances across availability zones, ensuring that your application is always available.\r\n\r\nIn conclusion, you should always ensure that there are more than one instance of a component in your infrastructure. There are also services on AWS that abstract away the physical infrastructure (Fargate, S3, Lambda) and use a multi-AZ pattern by default.\r\n\r\n---\r\n\r\nThis is post 061 of [#100DaysToOffload](https://100daystooffload.com/).\r\n\r\n","frontmatter":{"title":"Designing resilient cloud infrastructure","date":"2023-03-30","tags":"100DaysToOffload, infrastructure, aws, guide, note, learnings"},"tags":["100DaysToOffload","infrastructure","aws","guide","note","learnings"]},{"slug":"2023-03-23-fullscreen-terminals-in-vscode","markdownBody":"\nI often find myself using a \"real\" terminal alongside my VSCode setup, because for some tasks the built-in terminal, due to its small size, is quite flimsy to use. *But*! I just found out there's a a way to switch the terminal into fullscreen mode, using the \"View: Toggle Maximized Panel\" command.\r\n\r\nYou can bind it to a shortcut, which makes switching between editor and terminal a breeze! Simply add this to your `keybindings.json` (also accessible via the [command palette](https://code.visualstudio.com/docs/getstarted/userinterface#_command-palette)):\r\n\r\n```\r\n    {\r\n        \"key\": \"cmd+alt+m\",\r\n        \"command\": \"workbench.action.toggleMaximizedPanel\"\r\n    }\r\n```\r\n\r\n### References\r\n\r\n- [Original StackOverflow answer](https://stackoverflow.com/a/48512128/9046809)\r\n\r\n---\r\n\r\nThis is post 059 of [#100DaysToOffload](https://100daystooffload.com/).\r\n\n","frontmatter":{"title":"Fullscreen Terminals in VSCode","date":"2023-03-23","tags":"100DaysToOffload, guide, note, editors"},"tags":["100DaysToOffload","guide","note","editors"]}]},"__N_SSG":true}