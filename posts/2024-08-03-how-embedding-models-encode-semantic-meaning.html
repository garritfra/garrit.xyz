<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width, initial-scale=1"/><meta charSet="utf-8"/><title>How embedding models encode semantic meaning | Garrit&#x27;s Notes</title><meta name="Description" content="Generalist software developer writing about scalable infrastructure, fullstack development and DevOps practices."/><link rel="icon" type="image/svg+xml" href="/favicon.svg"/><link rel="manifest" href="/site.webmanifest"/><link rel="webmention" href="https://webmention.io/garrit.xyz/webmention"/><link rel="pingback" href="https://webmention.io/garrit.xyz/xmlrpc"/><meta name="next-head-count" content="8"/><link rel="preload" href="/_next/static/css/935debb317df188e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/935debb317df188e.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-ee7e63bc15b31913.js" defer=""></script><script src="/_next/static/chunks/framework-144ddb6b9c2105c7.js" defer=""></script><script src="/_next/static/chunks/main-2ec2bff4dc625bbc.js" defer=""></script><script src="/_next/static/chunks/pages/_app-5d4795abae9e13c0.js" defer=""></script><script src="/_next/static/chunks/630-15567ca4e241c039.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpost%5D-276741fc94169dfe.js" defer=""></script><script src="/_next/static/Eij8LQL7oL5-NctZ4SCT9/_buildManifest.js" defer=""></script><script src="/_next/static/Eij8LQL7oL5-NctZ4SCT9/_ssgManifest.js" defer=""></script></head><body><div id="__next"><section class="layout"><header class="header"><nav class="nav" role="navigation" aria-label="main navigation"><div class="header__container"><a href="/" class="header__container__logo">Garrit&#x27;s Notes</a></div><ul class="header__links"><li><a href="/posts">Blog</a></li><li><a href="/contact">Contact</a></li><li><a href="/links">More ...</a></li></ul></nav></header><div class="content"><article class="page h-entry"><div class="page__info"><h1 class="p-name">How embedding models encode semantic meaning</h1><time class="page__info__date">Aug 03 2024</time><p class="tag-list"><a href="/posts?tags=note">#<!-- -->note</a><a href="/posts?tags=tech">#<!-- -->tech</a><a href="/posts?tags=math">#<!-- -->math</a><a href="/posts?tags=llm">#<!-- -->llm</a><a href="/posts?tags=ai">#<!-- -->ai</a></p></div><div class="page__body e-content"><p>Embedding models have long been a daunting concept for me. But what are they? And why are they so useful? Let&#x27;s break it down in simple terms.</p>
<h2 id="what&#x27;s-an-embedding?">What&#x27;s an embedding?</h2>
<p>An embedding is basically a numerical representation of a piece of information - it could be text, audio, an image, or even a video. Think of it as a way to capture the essence or meaning of that information in a list of numbers.</p>
<p>For example, let&#x27;s say we have this text: &quot;show me a list of ground transportation at boston airport&quot;. An embedding model might turn that into something like this:</p>
<pre><code>[0.03793335, -0.008010864, -0.002319336, -0.0110321045, -0.019882202, -0.023864746, 0.011428833, -0.030349731, -0.044830322, 0.028289795, -0.02810669, -0.0032749176, -0.04208374, -0.0077705383, -0.0033798218, -0.06335449, ... ]
</code></pre>
<p>At first, thus looks like a jumble of numbers. But each of these numbers points to a specific area within the embedding model&#x27;s &quot;space&quot;, where similar words or concepts might be located.</p>
<h2 id="visualizing-embeddings">Visualizing embeddings</h2>
<p>To help wrap our heads around this, let&#x27;s look at a visualization. This beautiful image shows the entirety of the <a href="https://huggingface.co/nomic-ai/nomic-embed-text-v1.5">nomic-embed-text-v1.5</a> embedding model, as generated by <a href="https://atlas.nomic.ai/map/nomic-text-embed-v1-5m-sample">this visualization tool</a>:</p>
<p><img src="/assets/posts/2024-08-03-how-embedding-models-encode-semantic-meaning/nomic-embed-text-v1.5-full.jpeg" alt="nomic-embed-text-v1.5-full"/></p>
<p>Now, if we take our example text about Boston airport transportation and plot its embeddings on this map, we&#x27;d see that some clusters are lit up, especially around &quot;transportation&quot;. This means that the model has figured out that the topic of the query must be related to transportation in some way.</p>
<p><img src="/assets/posts/2024-08-03-how-embedding-models-encode-semantic-meaning/nomic-embed-text-v1.5-query.jpeg" alt="nomic-embed-text-v1.5-query"/></p>
<p>Zooming into this image, we can see more specific topics around transportation, like &quot;Airport&quot;, &quot;Travel&quot; or &quot;Highways&quot; are lit up, which more closely matches our query.</p>
<p><img src="/assets/posts/2024-08-03-how-embedding-models-encode-semantic-meaning/nomic-embed-text-v1.5-transportation.jpeg" alt="nomic-embed-text-v1.5-transportation"/></p>
<p>In a nutshell, embedding models are able to group terms by topics that are related to each other.</p>
<h2 id="why-should-we-care-about-embeddings?">Why should we care about embeddings?</h2>
<p>Encoding meaning in text has tons of different use cases. One that I&#x27;m particularly excited about is building RAG applications. RAG stands for Retrieval-Augmented Generation and refers to a method for Large Language Models (LLMs), where, given a question, you enrich the original question with relevant bits of information before answering it.</p>
<p>Here&#x27;s how embeddings are useful for RAG:</p>
<ol>
<li>You have a bunch of documents in your data source.</li>
<li>You use an embedding model to turn each document into a list of numbers (like we saw earlier).</li>
<li>When someone asks a question, you also turn that question into a list of numbers.</li>
<li>Then, you find the documents whose number lists are most similar to your question&#x27;s number list.</li>
<li>Voila! You&#x27;ve found the most relevant documents to answer the question.</li>
</ol>
<p>This method is way better than previously used techniques like just searching for exact words in the documents. It&#x27;s like the difference between having a librarian who only looks at book titles, and one who actually understands what the books are about.</p>
<h2 id="other-things-you-can-do-with-embeddings">Other things you can do with embeddings</h2>
<p>Beyond RAG applications, embeddings are super useful for all sorts of things:</p>
<ol>
<li><strong>Smarter searches</strong>: Find related stuff even if the exact words don&#x27;t match.</li>
<li><strong>Better recommendations</strong>: &quot;You liked this? You might also like these similar things!&quot;</li>
<li><strong>Language translation</strong>: Help computers understand that &quot;dog&quot; in English and &quot;perro&quot; in Spanish mean the same thing.</li>
<li><strong>Sentiment analysis</strong>: Figure out if someone&#x27;s happy or grumpy based on their tweet.</li>
</ol>
<h2 id="wrapping-it-up">Wrapping it up</h2>
<p>Embeddings are a clever way to turn words (or images, or sounds) into numbers that computers can understand and compare. By doing this, we can make emerging AI technologies a whole lot smarter at understanding language and finding connections between ideas.</p>
<p>Next time you&#x27;re chatting with an AI or getting scarily accurate recommendations online, you can nod knowingly and think, &quot;Ah yes, embeddings at work!&quot;</p><p class="horizontal-list"><button>üíåÔ∏è Reply via E-Mail</button><button>üîó Share</button><button>‚úèÔ∏è Fix Typo</button></p><hr/><h2>Continue Reading</h2><div><div class="blog__list__post"><time class="blog__list__post__date">Jun 11 2025</time><br/><a href="/posts/2025-06-11-git-diff-ignore-all-space-makes-code-reviews-way-easier">git diff --ignore-all-space makes code reviews way easier</a></div><div class="blog__list__post"><time class="blog__list__post__date">May 20 2025</time><br/><a href="/posts/2025-05-20-no-matter-what-you-do-always-leave-a-breadcrumb">No matter what you do, always leave a breadcrumb</a></div><div class="blog__list__post"><time class="blog__list__post__date">Mar 25 2025</time><br/><a href="/posts/2025-03-25-container-interfaces">About Container Interfaces</a></div><div class="blog__list__post"><time class="blog__list__post__date">Feb 27 2025</time><br/><a href="/posts/2025-02-27-a-trick-to-manage-frequently-used-prompts-in-claude-chatgpt">A trick to manage frequently used prompts in Claude/ChatGPT</a></div><div class="blog__list__post"><time class="blog__list__post__date">Sep 24 2024</time><br/><a href="/posts/2024-09-24-installing-mssql-client-drivers-for-a-php-application">Installing MSSQL Client Drivers for a PHP Application</a></div></div></div></article></div><footer class="footer"><div class="notice"><p>I invite you to read my new book<!-- --> <a target="_blank" href="https://www.buymeacoffee.com/garrit/e/233695">Five Years of Blogging: Ideas, Opinions and Guides written 2019 to 2024</a>. Becoming a member on<!-- --> <a target="_blank" href="https://www.buymeacoffee.com/garrit">Buy Me a Coffee</a> <!-- -->will grant you free access to the book!</p><a href="https://www.buymeacoffee.com/garrit/extras"><img src="/assets/five-years-of-blogging-cover-3d.png" alt="Cover of Five Years of Blogging" loading="lazy"/></a></div><div class="footer__content"><h3>Links of Interest</h3><a href="/rss.xml">RSS Feed</a><br/><a href="/todo">Todo List</a><br/><a href="https://keys.openpgp.org/vks/v1/by-fingerprint/2218337E54AA1DBE207B404DBB54AF7EB0939F3D">PGP Key</a><br/><a href="/guestbook">Guestbook</a><br/><a href="/blogroll">Blogroll</a><br/><a href="/ctf">Capture the Flag</a><h3>Elsewhere</h3><a href="https://github.com/garritfra" rel="me">Github</a><br/><a href="https://www.linkedin.com/in/garritfranke/">LinkedIn</a><br/><a href="https://fosstodon.org/@garritfra">Mastodon (ActivityPub)</a><br/><a href="/contact">Contact</a></div><a href="https://www.buymeacoffee.com/garrit" target="_blank"><img src="https://img.buymeacoffee.com/button-api/?text=Buy me a tea&amp;emoji=&amp;slug=garrit&amp;button_colour=FFB300&amp;font_colour=000000&amp;font_family=Cookie&amp;outline_colour=000000&amp;coffee_colour=ffffff"/></a><p>üëª Proud member of<!-- --> <a target="_blank" href="https://darktheme.club/">darktheme.club</a> <!-- -->üëª</p><p>¬© 2018-<!-- -->2025<!-- --> Garrit Franke<br/><a href="/privacy">Privacy</a> |<!-- --> <a target="_blank" href="https://github.com/garritfra/garrit.xyz">Source Code</a></p></footer></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"slug":"2024-08-03-how-embedding-models-encode-semantic-meaning","markdownBody":"\nEmbedding models have long been a daunting concept for me. But what are they? And why are they so useful? Let's break it down in simple terms.\n\n## What's an embedding?\n\nAn embedding is basically a numerical representation of a piece of information - it could be text, audio, an image, or even a video. Think of it as a way to capture the essence or meaning of that information in a list of numbers.\n\nFor example, let's say we have this text: \"show me a list of ground transportation at boston airport\". An embedding model might turn that into something like this:\n\n```\n[0.03793335, -0.008010864, -0.002319336, -0.0110321045, -0.019882202, -0.023864746, 0.011428833, -0.030349731, -0.044830322, 0.028289795, -0.02810669, -0.0032749176, -0.04208374, -0.0077705383, -0.0033798218, -0.06335449, ... ]\n```\n\nAt first, thus looks like a jumble of numbers. But each of these numbers points to a specific area within the embedding model's \"space\", where similar words or concepts might be located.\n\n## Visualizing embeddings\n\nTo help wrap our heads around this, let's look at a visualization. This beautiful image shows the entirety of the [nomic-embed-text-v1.5](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5) embedding model, as generated by [this visualization tool](https://atlas.nomic.ai/map/nomic-text-embed-v1-5m-sample):\n\n![nomic-embed-text-v1.5-full](/assets/posts/2024-08-03-how-embedding-models-encode-semantic-meaning/nomic-embed-text-v1.5-full.jpeg)\n\nNow, if we take our example text about Boston airport transportation and plot its embeddings on this map, we'd see that some clusters are lit up, especially around \"transportation\". This means that the model has figured out that the topic of the query must be related to transportation in some way.\n\n![nomic-embed-text-v1.5-query](/assets/posts/2024-08-03-how-embedding-models-encode-semantic-meaning/nomic-embed-text-v1.5-query.jpeg)\n\nZooming into this image, we can see more specific topics around transportation, like \"Airport\", \"Travel\" or \"Highways\" are lit up, which more closely matches our query.\n\n![nomic-embed-text-v1.5-transportation](/assets/posts/2024-08-03-how-embedding-models-encode-semantic-meaning/nomic-embed-text-v1.5-transportation.jpeg)\n\nIn a nutshell, embedding models are able to group terms by topics that are related to each other.\n\n## Why should we care about embeddings?\n\nEncoding meaning in text has tons of different use cases. One that I'm particularly excited about is building RAG applications. RAG stands for Retrieval-Augmented Generation and refers to a method for Large Language Models (LLMs), where, given a question, you enrich the original question with relevant bits of information before answering it.\n\nHere's how embeddings are useful for RAG:\n\n1. You have a bunch of documents in your data source.\n2. You use an embedding model to turn each document into a list of numbers (like we saw earlier).\n3. When someone asks a question, you also turn that question into a list of numbers.\n4. Then, you find the documents whose number lists are most similar to your question's number list.\n5. Voila! You've found the most relevant documents to answer the question.\n\nThis method is way better than previously used techniques like just searching for exact words in the documents. It's like the difference between having a librarian who only looks at book titles, and one who actually understands what the books are about.\n\n## Other things you can do with embeddings\n\nBeyond RAG applications, embeddings are super useful for all sorts of things:\n\n1. **Smarter searches**: Find related stuff even if the exact words don't match.\n2. **Better recommendations**: \"You liked this? You might also like these similar things!\"\n3. **Language translation**: Help computers understand that \"dog\" in English and \"perro\" in Spanish mean the same thing.\n4. **Sentiment analysis**: Figure out if someone's happy or grumpy based on their tweet.\n\n## Wrapping it up\n\nEmbeddings are a clever way to turn words (or images, or sounds) into numbers that computers can understand and compare. By doing this, we can make emerging AI technologies a whole lot smarter at understanding language and finding connections between ideas.\n\nNext time you're chatting with an AI or getting scarily accurate recommendations online, you can nod knowingly and think, \"Ah yes, embeddings at work!\"\n\n","frontmatter":{"title":"How embedding models encode semantic meaning","date":"2024-08-03","tags":"note, tech, math, llm, ai"},"tags":["note","tech","math","llm","ai"]},"recommendedPosts":[{"slug":"2025-06-11-git-diff-ignore-all-space-makes-code-reviews-way-easier","frontmatter":{"title":"git diff --ignore-all-space makes code reviews way easier","date":"2025-06-11","tags":"guide, note, til, git, tech, programming"},"tags":["guide","note","til","git","tech","programming"]},{"slug":"2025-05-20-no-matter-what-you-do-always-leave-a-breadcrumb","frontmatter":{"title":"No matter what you do, always leave a breadcrumb","date":"2025-05-20","tags":"note, practices, writing, life, tech"},"tags":["note","practices","writing","life","tech"]},{"slug":"2025-03-25-container-interfaces","frontmatter":{"title":"About Container Interfaces","date":"2025-03-25","tags":"infrastructure, note, tech"},"tags":["infrastructure","note","tech"]},{"slug":"2025-02-27-a-trick-to-manage-frequently-used-prompts-in-claude-chatgpt","frontmatter":{"title":"A trick to manage frequently used prompts in Claude/ChatGPT","date":"2025-02-27","tags":"guide, note, tech, llm, ai"},"tags":["guide","note","tech","llm","ai"]},{"slug":"2024-09-24-installing-mssql-client-drivers-for-a-php-application","frontmatter":{"title":"Installing MSSQL Client Drivers for a PHP Application","date":"2024-09-24","tags":"guide, note, web, tech, programming, php"},"tags":["guide","note","web","tech","programming","php"]}]},"__N_SSG":true},"page":"/posts/[post]","query":{"post":"2024-08-03-how-embedding-models-encode-semantic-meaning"},"buildId":"Eij8LQL7oL5-NctZ4SCT9","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>