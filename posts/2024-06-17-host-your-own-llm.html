<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width, initial-scale=1"/><meta charSet="utf-8"/><title>Host your own LLM | Garrit&#x27;s Notes</title><meta name="Description" content="Generalist software developer writing about scalable infrastructure, fullstack development and DevOps practices."/><link rel="icon" type="image/svg+xml" href="/favicon.svg"/><link rel="manifest" href="/site.webmanifest"/><link rel="webmention" href="https://webmention.io/garrit.xyz/webmention"/><link rel="pingback" href="https://webmention.io/garrit.xyz/xmlrpc"/><script defer="" data-domain="garrit.xyz" src="https://analytics.slashdev.space/js/plausible.js"></script><meta name="next-head-count" content="9"/><link rel="preload" href="/_next/static/css/935debb317df188e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/935debb317df188e.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-144ddb6b9c2105c7.js" defer=""></script><script src="/_next/static/chunks/main-fc56ac81e639fb5e.js" defer=""></script><script src="/_next/static/chunks/pages/_app-03f0c943f0690293.js" defer=""></script><script src="/_next/static/chunks/630-5082ce9035a1301d.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpost%5D-d231bbc4f923c521.js" defer=""></script><script src="/_next/static/cXsWJN7bsoxDu_Gjz_7jy/_buildManifest.js" defer=""></script><script src="/_next/static/cXsWJN7bsoxDu_Gjz_7jy/_ssgManifest.js" defer=""></script></head><body><div id="__next"><section class="layout"><header class="header"><nav class="nav" role="navigation" aria-label="main navigation"><div class="header__container"><a href="/" class="header__container__logo">Garrit&#x27;s Notes</a></div><ul class="header__links"><li><a href="/posts">Blog</a></li><li><a href="/contact">Contact</a></li><li><a href="/links">More ...</a></li></ul></nav></header><div class="content"><article class="page h-entry"><div class="page__info"><h1 class="p-name">Host your own LLM</h1><time class="page__info__date">Jun 17 2024</time><p class="tag-list"><a href="/posts?tags=infrastructure">#<!-- -->infrastructure</a><a href="/posts?tags=guide">#<!-- -->guide</a><a href="/posts?tags=note">#<!-- -->note</a><a href="/posts?tags=homelab">#<!-- -->homelab</a><a href="/posts?tags=tech">#<!-- -->tech</a><a href="/posts?tags=llm">#<!-- -->llm</a><a href="/posts?tags=ai">#<!-- -->ai</a></p></div><div class="page__body e-content"><p>I&#x27;m currently dipping my toes into Large Language Models (LLMs, or &quot;AI&quot;) and what you can do with them. It&#x27;s a fascinating topic, so expect some more posts on this in the coming days and weeks.</p>
<p>For starters, I wanted to document how I got my first LLM running on my local machine (a 2022 MacBook Pro). <a href="https://ollama.com/">Ollama</a> makes this process super easy. You just install it (<code>brew install ollama</code> in my case) and then run the model:</p>
<pre><code>ollama run llama3
</code></pre>
<p>This will download the model and open a prompt, so you can start chatting right away!</p>
<p>You can think of Ollama as the <a href="https://www.docker.com/">Docker</a> CLI but for LLMs. There&#x27;s a <a href="https://ollama.com/library">directory of LLMs</a>, and if a model has multiple different sizes, you can use it like you would pull a different docker tag:</p>
<pre><code>ollama pull llama3:8b
ollama pull llama3:70b
</code></pre>
<p>The best thing about ollama is that it also exposes a web server for you to integrate the LLM into your application. As an example, here&#x27;s how you would curl your local LLM:</p>
<pre><code>curl http://localhost:11434/api/chat -d &#x27;{
    &quot;model&quot;: &quot;llama3&quot;,      
    &quot;messages&quot;: [{ &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Are you a robot?&quot; }],
    &quot;stream&quot;: false
}&#x27;
{&quot;model&quot;:&quot;llama3&quot;,&quot;created_at&quot;:&quot;2024-06-17T11:19:23.510588Z&quot;,&quot;message&quot;:{&quot;role&quot;:&quot;assistant&quot;,&quot;content&quot;:&quot;I am not a human, but I&#x27;m also not a traditional robot. I&#x27;m an artificial intelligence language model designed to simulate conversation and answer questions to the best of my ability. My \&quot;brain\&quot; is a complex algorithm that processes natural language inputs and generates responses based on patterns and associations learned from large datasets.\n\nWhile I don&#x27;t have a physical body or consciousness like humans do, I&#x27;m designed to interact with humans in a way that feels natural and conversational. I can understand and respond to questions, make suggestions, and even tell jokes (though my humor may be a bit... algorithmic).\n\nSo, while I&#x27;m not a human or a traditional robot, I exist at the intersection of technology and language, designed to assist and communicate with humans in a helpful way!&quot;},&quot;done_reason&quot;:&quot;stop&quot;,&quot;done&quot;:true,&quot;total_duration&quot;:12565842250,&quot;load_duration&quot;:7059262291,&quot;prompt_eval_count&quot;:15,&quot;prompt_eval_duration&quot;:331275000,&quot;eval_count&quot;:156,&quot;eval_duration&quot;:5172858000}
</code></pre>
<p>If your local machine is not beefy enough and you want to try out a large LLM on a rented server (AWS has <code>g5.2xlarge</code>, which gave me good results for <code>mixtral 8x7b</code>), you also have to set <code>OLLAMA_HOST=0.0.0.0</code> in your environment variables to be able to reach the remote server. <strong>This exposes the LLM to the public internet, so be careful when chosing your deployment strategy.</strong></p>
<p>And there you go! You just deployed your very own LLM. Pretty cool, huh?</p><p class="horizontal-list"><button>üíåÔ∏è Reply via E-Mail</button><button>üîó Share</button><button>‚úèÔ∏è Fix Typo</button></p><hr/><h2>Continue Reading</h2><div><div class="blog__list__post"><time class="blog__list__post__date">Dec 11 2025</time><br/><a href="/posts/2025-12-11-custom-entities-in-home-assistant">Custom Entities in Home Assistant</a></div><div class="blog__list__post"><time class="blog__list__post__date">Sep 15 2025</time><br/><a href="/posts/2025-09-15-making-family-it-support-effortless-and-free">Making family IT support effortless (and free)</a></div><div class="blog__list__post"><time class="blog__list__post__date">Jun 11 2025</time><br/><a href="/posts/2025-06-11-git-diff-ignore-all-space-makes-code-reviews-way-easier">git diff --ignore-all-space makes code review way easier</a></div><div class="blog__list__post"><time class="blog__list__post__date">May 20 2025</time><br/><a href="/posts/2025-05-20-no-matter-what-you-do-always-leave-a-breadcrumb">No matter what you do, always leave a breadcrumb</a></div><div class="blog__list__post"><time class="blog__list__post__date">Mar 25 2025</time><br/><a href="/posts/2025-03-25-container-interfaces">About Container Interfaces</a></div></div></div></article></div><footer class="footer"><div class="notice"><p>I invite you to read my new book<!-- --> <a target="_blank" href="https://www.buymeacoffee.com/garrit/e/233695">Five Years of Blogging: Ideas, Opinions and Guides written 2019 to 2024</a>. Becoming a member on<!-- --> <a target="_blank" href="https://www.buymeacoffee.com/garrit">Buy Me a Coffee</a> <!-- -->will grant you free access to the book!</p><a href="https://www.buymeacoffee.com/garrit/extras"><img src="/assets/five-years-of-blogging-cover-3d.png" alt="Cover of Five Years of Blogging" loading="lazy"/></a></div><div class="footer__content"><h3>Links of Interest</h3><a href="/rss.xml">RSS Feed</a><br/><a href="/todo">Todo List</a><br/><a href="https://keys.openpgp.org/vks/v1/by-fingerprint/2218337E54AA1DBE207B404DBB54AF7EB0939F3D">PGP Key</a><br/><a href="/guestbook">Guestbook</a><br/><a href="/blogroll">Blogroll</a><br/><a href="/ctf">Capture the Flag</a><h3>Elsewhere</h3><a href="https://github.com/garritfra" rel="me">Github</a><br/><a href="https://www.linkedin.com/in/garritfranke/">LinkedIn</a><br/><a href="https://fosstodon.org/@garritfra">Mastodon (ActivityPub)</a><br/><a href="/contact">Contact</a></div><a href="https://www.buymeacoffee.com/garrit" target="_blank"><img src="https://img.buymeacoffee.com/button-api/?text=Buy me a tea&amp;emoji=&amp;slug=garrit&amp;button_colour=FFB300&amp;font_colour=000000&amp;font_family=Cookie&amp;outline_colour=000000&amp;coffee_colour=ffffff"/></a><p>üëª Proud member of<!-- --> <a target="_blank" href="https://darktheme.club/">darktheme.club</a> <!-- -->üëª</p><p>¬© 2018-<!-- -->2026<!-- --> Garrit Franke<br/><a href="/privacy">Privacy</a> |<!-- --> <a target="_blank" href="https://github.com/garritfra/garrit.xyz">Source Code</a></p></footer></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"slug":"2024-06-17-host-your-own-llm","markdownBody":"\nI'm currently dipping my toes into Large Language Models (LLMs, or \"AI\") and what you can do with them. It's a fascinating topic, so expect some more posts on this in the coming days and weeks.\r\n\r\nFor starters, I wanted to document how I got my first LLM running on my local machine (a 2022 MacBook Pro). [Ollama](https://ollama.com/) makes this process super easy. You just install it (`brew install ollama` in my case) and then run the model:\r\n\r\n```\r\nollama run llama3\r\n```\r\n\r\nThis will download the model and open a prompt, so you can start chatting right away!\r\n\r\nYou can think of Ollama as the [Docker](https://www.docker.com/) CLI but for LLMs. There's a [directory of LLMs](https://ollama.com/library), and if a model has multiple different sizes, you can use it like you would pull a different docker tag:\r\n\r\n```\r\nollama pull llama3:8b\r\nollama pull llama3:70b\r\n```\r\n\r\nThe best thing about ollama is that it also exposes a web server for you to integrate the LLM into your application. As an example, here's how you would curl your local LLM:\r\n\r\n```\r\ncurl http://localhost:11434/api/chat -d '{\r\n    \"model\": \"llama3\",      \r\n    \"messages\": [{ \"role\": \"user\", \"content\": \"Are you a robot?\" }],\r\n    \"stream\": false\r\n}'\r\n{\"model\":\"llama3\",\"created_at\":\"2024-06-17T11:19:23.510588Z\",\"message\":{\"role\":\"assistant\",\"content\":\"I am not a human, but I'm also not a traditional robot. I'm an artificial intelligence language model designed to simulate conversation and answer questions to the best of my ability. My \\\"brain\\\" is a complex algorithm that processes natural language inputs and generates responses based on patterns and associations learned from large datasets.\\n\\nWhile I don't have a physical body or consciousness like humans do, I'm designed to interact with humans in a way that feels natural and conversational. I can understand and respond to questions, make suggestions, and even tell jokes (though my humor may be a bit... algorithmic).\\n\\nSo, while I'm not a human or a traditional robot, I exist at the intersection of technology and language, designed to assist and communicate with humans in a helpful way!\"},\"done_reason\":\"stop\",\"done\":true,\"total_duration\":12565842250,\"load_duration\":7059262291,\"prompt_eval_count\":15,\"prompt_eval_duration\":331275000,\"eval_count\":156,\"eval_duration\":5172858000}\r\n```\r\n\r\nIf your local machine is not beefy enough and you want to try out a large LLM on a rented server (AWS has `g5.2xlarge`, which gave me good results for `mixtral 8x7b`), you also have to set `OLLAMA_HOST=0.0.0.0` in your environment variables to be able to reach the remote server. **This exposes the LLM to the public internet, so be careful when chosing your deployment strategy.**\r\n\r\nAnd there you go! You just deployed your very own LLM. Pretty cool, huh?\n","frontmatter":{"title":"Host your own LLM","date":"2024-06-17","tags":"infrastructure, guide, note, homelab, tech, llm, ai"},"tags":["infrastructure","guide","note","homelab","tech","llm","ai"]},"recommendedPosts":[{"slug":"2025-12-11-custom-entities-in-home-assistant","frontmatter":{"title":"Custom Entities in Home Assistant","date":"2025-12-11","tags":"guide, note, homeassistant, homelab, tech, programming"},"tags":["guide","note","homeassistant","homelab","tech","programming"]},{"slug":"2025-09-15-making-family-it-support-effortless-and-free","frontmatter":{"title":"Making family IT support effortless (and free)","date":"2025-09-15","tags":"note, life, tech"},"tags":["note","life","tech"]},{"slug":"2025-06-11-git-diff-ignore-all-space-makes-code-reviews-way-easier","frontmatter":{"title":"git diff --ignore-all-space makes code review way easier","date":"2025-06-11","tags":"guide, note, til, git, tech, programming"},"tags":["guide","note","til","git","tech","programming"]},{"slug":"2025-05-20-no-matter-what-you-do-always-leave-a-breadcrumb","frontmatter":{"title":"No matter what you do, always leave a breadcrumb","date":"2025-05-20","tags":"note, practices, writing, life, tech"},"tags":["note","practices","writing","life","tech"]},{"slug":"2025-03-25-container-interfaces","frontmatter":{"title":"About Container Interfaces","date":"2025-03-25","tags":"infrastructure, note, tech"},"tags":["infrastructure","note","tech"]}]},"__N_SSG":true},"page":"/posts/[post]","query":{"post":"2024-06-17-host-your-own-llm"},"buildId":"cXsWJN7bsoxDu_Gjz_7jy","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>