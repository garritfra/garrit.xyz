<?xml version="1.0"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>
            garrit.xyz
        </title>
        <link>
            https://garrit.xyz
        </link>
        <description>
            Garrit Franke
        </description>
        <language>
            en
        </language>
        <lastBuildDate>
            Wed, 11 Jun 2025 00:00:00 +0000
        </lastBuildDate>
        <item>
            <title>
                git diff --ignore-all-space makes code reviews way easier
            </title>
            <guid>
                https://garrit.xyz/posts/2025-06-11-git-diff-ignore-all-space-makes-code-reviews-way-easier
            </guid>
            <link>
                https://garrit.xyz/posts/2025-06-11-git-diff-ignore-all-space-makes-code-reviews-way-easier?utm_source=rss
            </link>
            <pubDate>
                Wed, 11 Jun 2025 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>I just learned a cool trick that I want to share. Let&#39;s review the diff of a file using <code>git diff</code>. I redacted most of it, but you probably found yourself in the situation of extremely long changes before:</p>

<p><code></code>`
diff --git a/lib/ui/screens/detail/components/body/event<em>body.dart b/lib/ui/screens/detail/components/body/event</em>body.dart
index d19d70a..1a61380 100644
--- a/lib/ui/screens/detail/components/body/event<em>body.dart
+++ b/lib/ui/screens/detail/components/body/event</em>body.dart
@@ -3,6 +3,7 @@
 class EventBody extends StatelessWidget {
   final EventDetails details;
@@ -18,35 +19,43 @@ class EventBody extends StatelessWidget {</p>

<p>   @override
   Widget build(BuildContext context) =&gt; Column(
-        crossAxisAlignment: CrossAxisAlignment.stretch,
-        children: [
-          EventInfoView(
-            location: details.location,
-            start: details.start,
-            end: details.end,
-            certified: details.certified,
-            paid: details.paid,
-            points: null, // Already shown in app bar
+    crossAxisAlignment: CrossAxisAlignment.stretch,
+    children: [
+      EventInfoView(
+        location: details.location,
+        start: details.start,
+        end: details.end,
+        certified: details.certified,
+        paid: details.paid,
+        points: null, // Already shown in app bar
+      ),
+      const SizedBox(height: 24),
+      Html(
+        data: details.description,
+        style: {
+          &#39;body&#39;: Style(
+            margin: Margins.zero,
...
 }
<code></code>`</p>

<p>But do you spot what has ACTUALLY been changed? In a real world scenario, it probably took you a while before you realised that it&#39;s the result of formatting the entire file. Nobody cares about whitespace when reviewing code. Or rather, your brain should not take the burden of having to care about that. That&#39;s what linters are for!</p>

<p>Let&#39;s look at the same diff with the <code>--ignore-all-space</code> (shorthand <code>-w</code>) flag activated:</p>

<p><code>
diff --git a/lib/ui/screens/detail/components/body/event_body.dart b/lib/ui/screens/detail/components/body/event_body.dart
index d19d70a..1a61380 100644
--- a/lib/ui/screens/detail/components/body/event_body.dart
+++ b/lib/ui/screens/detail/components/body/event_body.dart
@@ -3,6 +3,7 @@
 class EventBody extends StatelessWidget {
   final EventDetails details;
@@ -29,6 +30,14 @@ class EventBody extends StatelessWidget {
         points: null, // Already shown in app bar
       ),
       const SizedBox(height: 24),
+      Html(
+        data: details.description,
+        style: {
+          &#39;body&#39;: Style(
+            margin: Margins.zero,
+          ),
+        },
+      ),
       Text(details.description, style: context.theme.textTheme.body2Regular),
       if (details.registrationUrl != null || details.programUrl != null) const SizedBox(height: 16),
       if (details.registrationUrl != null) ...[
</code></p>

<p>Huh, so it&#39;s NOT just a formatted file. All whitespace changes have been stripped out, and you only see the changes that are relevant for the review. Neat!</p>

<p>Many tools also support ignoring whitespace. GitLab let&#39;s you disable <code>Show whitespace changes</code> in the merge request diff viewer. VSCode has the <code>diffEditor.ignoreTrimWhitespace</code> setting. So, if you want to make this the default in your tools, chances are there&#39;s an option for only showing relevant changes.</p>

<p>A bit of a sloppy post, but I hope this is useful to someone. Happy code reviewing!</p>]]>
            </description>
        </item>
        <item>
            <title>
                No matter what you do, always leave a breadcrumb
            </title>
            <guid>
                https://garrit.xyz/posts/2025-05-20-no-matter-what-you-do-always-leave-a-breadcrumb
            </guid>
            <link>
                https://garrit.xyz/posts/2025-05-20-no-matter-what-you-do-always-leave-a-breadcrumb?utm_source=rss
            </link>
            <pubDate>
                Tue, 20 May 2025 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>This applies to work tasks just as much as it applies to hobbies. No matter what you do, always create some sort of breadcrumb that you or someone else can pick up down the line.</p>

<p>If you build a feature into your app, you&#39;ve generated an output, and therefore value. But what if you bang your head against the wall and don&#39;t make any progress? Write about it. Write about your challenges, document what you&#39;ve learned and share it with your peers. Not reaching a set goal doesn&#39;t mean failure. It&#39;s an opportunity to create value in a way you didn&#39;t anticipate.</p>

<p>If you&#39;re out on a run and a thought pops into your head, record a memo on your phone, or better yet: <a href="https://garrit.xyz/posts/2023-09-09-everyday-carry-notebooks">keep a tiny notebook with you at all times</a>. Thoughts slip in and out of our heads at a very rapid pace. Capturing thoughts, even if you don&#39;t share them with anyone else, is a great way to generate value out of thin air.</p>

<p>If you&#39;re into crafty hobbies like woodworking, painting or baking, you likely know how important it is to finish a project. No matter how good or bad the outcome is, every finished project is an artifact that you can look back at later and see the progress you&#39;ve made.</p>

<p>Output always means value, even if it&#39;s not immediately apparent. Just keep on laying down those breadcrumbs.</p>]]>
            </description>
        </item>
        <item>
            <title>
                About Container Interfaces
            </title>
            <guid>
                https://garrit.xyz/posts/2025-03-25-container-interfaces
            </guid>
            <link>
                https://garrit.xyz/posts/2025-03-25-container-interfaces?utm_source=rss
            </link>
            <pubDate>
                Tue, 25 Mar 2025 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>There are a couple of interfaces that container orchestration systems (like <a href="https://kubernetes.io">Kubernetes</a>) implement to expose certain behavior to their container workloads. I will only be talking about Kubernetes in this post since it&#39;s the orchestrator I&#39;m most comfortable with, but some interfaces are also implemented in other orchestrators (like <a href="https://www.nomadproject.io/">HashiCorp Nomad</a>) too, which makes the interfaces cross-platform.</p>

<h2>Container Storage Interface (CSI)</h2>

<p>Storage behavior used to be built into Kubernetes. The <a href="https://github.com/container-storage-interface/spec/blob/master/spec.md">container storage interface (CSI)</a> defines a unified interface to manage storage volumes, regardless of the orchestrator (as long as they implement the CSI). This makes it way easier for third-party storage providers to expose data to Kubernetes. If a storage provider implements this interface, orchestrators can use it to provision volumes to containers. Notable storage providers are:</p>

<ul><li><a href="https://github.com/kubernetes-csi/csi-driver-nfs">NFS</a></li><li><a href="https://github.com/kubernetes/cloud-provider-openstack/tree/master/pkg/csi/cinder">Cinder (Openstack)</a></li><li><a href="https://github.com/ceph/ceph-csi">Ceph</a></li><li><a href="https://github.com/kubernetes-sigs/aws-ebs-csi-driver">AWS EBS</a></li></ul>

<p>A full list of CSI drivers can be found <a href="https://kubernetes-csi.github.io/docs/drivers.html">here</a>.</p>

<h2>Container Runtime Interface (CRI)</h2>

<p>The <a href="https://github.com/kubernetes/cri-api">Container Runtime Interface (CRI)</a> is an API that allows Kubernetes to use different container runtimes without needing to recompile the entire Kubernetes codebase. Before CRI, Kubernetes had a direct integration with Docker, making it difficult to use alternative container runtimes.</p>

<p>CRI defines the API between Kubernetes components (specifically kubelet) and container runtimes. This abstraction allows Kubernetes to support multiple container runtimes simultaneously, giving users the flexibility to choose the runtime that best fits their needs. Some popular container runtimes that implement the CRI include:</p>

<ul><li><a href="https://containerd.io/">containerd</a> - The industry-standard container runtime that powers Docker and is maintained by the CNCF</li><li><a href="https://cri-o.io/">CRI-O</a> - A lightweight runtime specifically designed for Kubernetes</li><li><a href="https://katacontainers.io/">Kata Containers</a> - A secure container runtime that uses hardware virtualization for stronger isolation</li></ul>

<p>With CRI, switching between different runtimes becomes more straightforward, allowing operators to optimize for security, performance, or compatibility based on their specific requirements.</p>

<h2>Container Network Interface (CNI)</h2>

<p>The <a href="https://github.com/containernetworking/cni">Container Network Interface (CNI)</a> defines a standard for configuring network interfaces for Linux containers. Similar to CSI and CRI, the CNI was created to decouple Kubernetes from specific networking implementations, allowing for a pluggable architecture.</p>

<p>CNI plugins are responsible for allocating IP addresses to pods and ensuring proper network connectivity between pods, nodes, and external networks. They implement features like network policies, load balancing, and network security. Some popular CNI plugins include:</p>

<ul><li><a href="https://www.tigera.io/project-calico/">Calico</a> - Known for its performance, flexibility, and strong network policy support</li><li><a href="https://cilium.io/">Cilium</a> - Uses eBPF for high-performance networking and security</li><li><a href="https://github.com/flannel-io/flannel">Flannel</a> - Simple overlay network focused on ease of use</li><li><a href="https://github.com/aws/amazon-vpc-cni-k8s">AWS VPC CNI</a> - Integrates pods directly with Amazon VPC networking</li></ul>

<p>Each CNI plugin has its strengths and is suitable for different use cases. For example, Calico excels at enforcing network policies, Cilium is optimized for performance and observability, while Flannel is valued for its simplicity.</p>

<h2>Wrapping Up</h2>

<p>One thing I&#39;ve always admired about Kubernetes is its pluggable architecture. These standardized interfaces (CSI, CRI, and CNI) showcase how well-designed the system really is. Instead of building everything into the core, the Kubernetes team made the smart decision to create extension points that allow the community to innovate without touching the core codebase.</p>

<p>The great news? You don&#39;t <em>have</em> to swap out all these components or even understand them deeply to use Kubernetes effectively. While the array of options might seem daunting at first glance, most Kubernetes distributions (like EKS, GKE, AKS, or Rancher) come with sane defaults that work well out of the box. They&#39;ve already made sensible choices about which storage, runtime, and networking components to include.</p>

<p>This pluggability is what makes Kubernetes so powerful for those who need it. Need a specific storage solution? Plug in a CSI driver. Want a more secure container runtime? Swap in a different CRI implementation. But for everyone else, the defaults will serve you just fine.</p>

<p>The beauty of this approach is that it gives you room to grow. Start with the defaults, and when you have specific requirements, the extension points are there waiting for you. That&#39;s the real magic of Kubernetes â€“ it works great out of the box but doesn&#39;t limit your options as your needs evolve.</p>]]>
            </description>
        </item>
        <item>
            <title>
                A trick to manage frequently used prompts in Claude/ChatGPT
            </title>
            <guid>
                https://garrit.xyz/posts/2025-02-27-a-trick-to-manage-frequently-used-prompts-in-claude-chatgpt
            </guid>
            <link>
                https://garrit.xyz/posts/2025-02-27-a-trick-to-manage-frequently-used-prompts-in-claude-chatgpt?utm_source=rss
            </link>
            <pubDate>
                Thu, 27 Feb 2025 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>So far, whenever I wanted to recycle a prompt from another context in <a href="https://www.anthropic.com/claude">Claude</a> (though this also applies to ChatGPT and some other LLMs), I went back in my conversation history, copied the prompt, pasted it in a new chat and adjusted the context. But I recently discovered that <a href="https://www.anthropic.com/news/projects">Claude Projects</a> can be misused as &quot;prompt templates&quot;, which makes it way easier to handle repetitive tasks.</p>

<p>In Projects, you can set a system prompt that will be applied to all conversations in the project. I guess it&#39;s supposed to be used for relevant information about whatever you want to work on, but I like to think about a project more as a prompt template, rather than a project. For example, here&#39;s a project prompt that I use to brainstorm project ideas:</p>

<blockquote><p>Ask me one question at a time so we can develop a thorough, step-by-step spec for this idea. Each question should build on my previous answers, and our end goal is to have a detailed specification I can hand off to a developer. Letâ€™s do this iteratively and dig into every relevant detail. Remember, only one question at a time.</p></blockquote>

<p>(Stolen from <a href="https://harper.blog/2025/02/16/my-llm-codegen-workflow-atm/">this</a> great blog post)</p>

<p>While you could copy and paste this from a text file into every new conversation, Claude&#39;s projects make it super easy to save this as a template.</p>

<p>I guess this is an obvious feature for some people, but to me, it was a huge help once I found this out.</p>

<p>Got any other neat tricks for working with LLMs? I&#39;d love to hear them!</p>]]>
            </description>
        </item>
        <item>
            <title>
                Installing MSSQL Client Drivers for a PHP Application
            </title>
            <guid>
                https://garrit.xyz/posts/2024-09-24-installing-mssql-client-drivers-for-a-php-application
            </guid>
            <link>
                https://garrit.xyz/posts/2024-09-24-installing-mssql-client-drivers-for-a-php-application?utm_source=rss
            </link>
            <pubDate>
                Tue, 24 Sep 2024 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>I just had the pleasure (<em>cough</em>) to connect an <a href="https://de.wikipedia.org/wiki/Microsoft_SQL_Server">MSSQL</a> database to a <a href="https://laravel.com/">Laravel</a> application at work. Because the process was <em>super</em> tedious, I wanted to quickly jot this down so I will never have to go through this again.</p>

<h2>Our setup</h2>

<p>We&#39;re building a Laravel application with <a href="https://ddev.com/">DDEV</a>. DDEV essentially moves all development tools into Docker containers and adds some nice features like local database management.</p>

<h2>The process</h2>

<p>Laravel comes with the boilerplate to use MSSQL out of the box. In your app, just set the database config to use <code>sqlsrv</code>:</p>

<p><code>php
    &#39;connections&#39; =&gt; [
        &#39;sqlsrv&#39; =&gt; [
            &#39;driver&#39; =&gt; &#39;sqlsrv&#39;,
            &#39;url&#39; =&gt; env(&#39;DB_URL&#39;),
            &#39;host&#39; =&gt; env(&#39;DB_HOST&#39;, &#39;127.0.0.1&#39;),
            &#39;port&#39; =&gt; env(&#39;DB_PORT&#39;, &#39;1433&#39;),
            &#39;database&#39; =&gt; env(&#39;DB_DATABASE&#39;, &#39;laravel&#39;),
            &#39;username&#39; =&gt; env(&#39;DB_USERNAME&#39;, &#39;root&#39;),
            &#39;password&#39; =&gt; env(&#39;DB_PASSWORD&#39;, &#39;&#39;),
            &#39;unix_socket&#39; =&gt; env(&#39;DB_SOCKET&#39;, &#39;&#39;),
            &#39;charset&#39; =&gt; env(&#39;DB_CHARSET&#39;, &#39;utf8&#39;),
            &#39;prefix&#39; =&gt; &#39;&#39;,
            &#39;prefix_indexes&#39; =&gt; true,
            // &#39;encrypt&#39; =&gt; env(&#39;DB_ENCRYPT&#39;, &#39;yes&#39;),
            // &#39;trust_server_certificate&#39; =&gt; env(&#39;DB_TRUST_SERVER_CERTIFICATE&#39;, &#39;false&#39;),
        ],
    ],
</code></p>

<p>You will see errors when starting your app, because you need to install the corresponding drivers first. Instead of adding them through <a href="https://getcomposer.org/">Composer</a> (a widely adopted package manager for PHP), you have to install the ODBC drivers <strong>through the system package manager</strong>, because Microsoft doesn&#39;t maintain a PHP package. Furthermore, you also have to install the driver repository because <strong>Microsoft doesn&#39;t even maintain packages for the major Linux distributions</strong>. In our setup with DDEV, this has to be done by amending the Dockerfile used for the application container. Create a file at <code>.ddev/web-build/Dockerfile</code> and add the following contents:</p>

<p><code></code>`dockerfile</p>

<h1>https://ddev.readthedocs.io/en/stable/users/extend/customizing-images/#adding-extra-dockerfiles-for-webimage-and-dbimage</h1>

<h1>https://stackoverflow.com/questions/58086933/how-to-install-the-sql-server-php-drivers-in-ddev-local#new-answer</h1>

<p>ARG BASE<em>IMAGE
FROM $BASE</em>IMAGE</p>

<p>RUN npm install --global forever
RUN echo &quot;Built on $(date)&quot; &gt; /build-date.txt</p>

<h1>RUN curl https://packages.microsoft.com/keys/microsoft.asc | sudo apt-key add -</h1>

<h1>RUN curl https://packages.microsoft.com/config/debian/11/prod.list &gt; /etc/apt/sources.list.d/mssql-release.list</h1>

<p>RUN curl -fsSL https://packages.microsoft.com/keys/microsoft.asc | sudo gpg --dearmor -o /usr/share/keyrings/microsoft-prod.gpg
RUN curl https://packages.microsoft.com/config/debian/12/prod.list | sudo tee /etc/apt/sources.list.d/mssql-release.list</p>

<p>RUN apt-get update
RUN apt-get --allow-downgrades -y install libssl-dev
RUN apt-get -y update &amp;&amp; yes | ACCEPT<em>EULA=Y apt-get -y install php8.3-dev php-pear unixodbc-dev htop
RUN ACCEPT</em>EULA=Y apt-get -y install msodbcsql18 mssql-tools18
RUN sudo pecl channel-update pecl.php.net
RUN sudo pecl install sqlsrv
RUN sudo pecl install pdo_sqlsrv</p>

<p>RUN sudo printf &quot;; priority=20\nextension=sqlsrv.so\n&quot; &gt; /etc/php/8.3/mods-available/sqlsrv.ini
RUN sudo printf &quot;; priority=30\nextension=pdo<em>sqlsrv.so\n&quot; &gt; /etc/php/8.3/mods-available/pdo</em>sqlsrv.ini
RUN sudo phpenmod -v 8.3 -s cli sqlsrv pdo<em>sqlsrv
RUN sudo phpenmod -v 8.3 -s fpm sqlsrv pdo</em>sqlsrv
RUN sudo phpenmod -v 8.3 -s apache2 sqlsrv pdo_sqlsrv</p>

<p>RUN sudo printf &quot;; priority=20\nextension=sqlsrv.so\n&quot; &gt; /etc/php/8.3/mods-available/sqlsrv.ini
RUN sudo printf &quot;; priority=30\nextension=pdo<em>sqlsrv.so\n&quot; &gt; /etc/php/8.3/mods-available/pdo</em>sqlsrv.ini
RUN sudo phpenmod -v 8.3 -s cli sqlsrv pdo<em>sqlsrv
RUN sudo phpenmod -v 8.3 -s fpm sqlsrv pdo</em>sqlsrv
RUN sudo phpenmod -v 8.3 -s apache2 sqlsrv pdo_sqlsrv</p>

<p>RUN echo &#39;export PATH=&quot;$PATH:/opt/mssql-tools/bin&quot;&#39; &gt;&gt; ~/.bash_profile
<code></code>`</p>

<p>If you&#39;re reading this in the future and Microsoft may have released a new version of the ODBC drivers, you may have to follow the new <a href="https://learn.microsoft.com/de-de/sql/connect/odbc/linux-mac/installing-the-microsoft-odbc-driver-for-sql-server?view=sql-server-ver16&amp;tabs=debian18-install%2Calpine17-install%2Cdebian8-install%2Credhat7-13-install%2Crhel7-offline#18">installation instructions from their documentation</a>. It took me a while to realize that I couldn&#39;t install version 17 of the driver because I was using the installation instructions for version 18. They are apparently incompatible with each other.</p>

<p>I hope that you&#39;ll never have to touch the shithole that is MSSQL, but if you do, I hope that this guide will be of value to you.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Sentiment analysis using ML models
            </title>
            <guid>
                https://garrit.xyz/posts/2024-08-31-sentiment-analysis-using-ml-models
            </guid>
            <link>
                https://garrit.xyz/posts/2024-08-31-sentiment-analysis-using-ml-models?utm_source=rss
            </link>
            <pubDate>
                Sat, 31 Aug 2024 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>I just rewrote parts of my <a href="https://github.com/garritfra/positive_hackernews">Positive Hacker News RSS Feed</a> project to use an ML model to filter out any negative news from the Hacker News timeline. This method is far more reliable than the previous method of using a <a href="https://www.nltk.org/api/nltk.sentiment.vader.html">rule-based sentiment analyzer</a> through NLTK.</p>

<p>I&#39;m using the model <a href="https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest?text=Messer-Angreiferin+verletzt+f%C3%BCnf+Menschen+in+einem+Bus">cardiffnlp/twitter-roberta-base-sentiment-latest</a>, which was trained on a huge amount of tweets. It&#39;s really tiny (~500 MB) and easily runs inside the existing GitHub Actions workflows. You can try out the model yourself on the HuggingFace model card.</p>

<p>&lt;img width=&quot;522&quot; alt=&quot;grafik&quot; src=&quot;https://github.com/user-attachments/assets/06f42df6-624a-4108-ada8-d0d37a53e693&quot;&gt;</p>

<p>If you want to subscribe to more positive tech news, simply replace your Hacker News feed of your RSS reader with this one (or add it if you haven&#39;t already):
https://garritfra.github.io/positive_hackernews/feed.xml</p>]]>
            </description>
        </item>
        <item>
            <title>
                How embedding models encode semantic meaning
            </title>
            <guid>
                https://garrit.xyz/posts/2024-08-03-how-embedding-models-encode-semantic-meaning
            </guid>
            <link>
                https://garrit.xyz/posts/2024-08-03-how-embedding-models-encode-semantic-meaning?utm_source=rss
            </link>
            <pubDate>
                Sat, 03 Aug 2024 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>Embedding models have long been a daunting concept for me. But what are they? And why are they so useful? Let&#39;s break it down in simple terms.</p>

<h2>What&#39;s an embedding?</h2>

<p>An embedding is basically a numerical representation of a piece of information - it could be text, audio, an image, or even a video. Think of it as a way to capture the essence or meaning of that information in a list of numbers.</p>

<p>For example, let&#39;s say we have this text: &quot;show me a list of ground transportation at boston airport&quot;. An embedding model might turn that into something like this:</p>

<p><code>
[0.03793335, -0.008010864, -0.002319336, -0.0110321045, -0.019882202, -0.023864746, 0.011428833, -0.030349731, -0.044830322, 0.028289795, -0.02810669, -0.0032749176, -0.04208374, -0.0077705383, -0.0033798218, -0.06335449, ... ]
</code></p>

<p>At first, thus looks like a jumble of numbers. But each of these numbers points to a specific area within the embedding model&#39;s &quot;space&quot;, where similar words or concepts might be located.</p>

<h2>Visualizing embeddings</h2>

<p>To help wrap our heads around this, let&#39;s look at a visualization. This beautiful image shows the entirety of the <a href="https://huggingface.co/nomic-ai/nomic-embed-text-v1.5">nomic-embed-text-v1.5</a> embedding model, as generated by <a href="https://atlas.nomic.ai/map/nomic-text-embed-v1-5m-sample">this visualization tool</a>:</p>

<p><img alt="nomic-embed-text-v1.5-full" src="/assets/posts/2024-08-03-how-embedding-models-encode-semantic-meaning/nomic-embed-text-v1.5-full.jpeg"/></p>

<p>Now, if we take our example text about Boston airport transportation and plot its embeddings on this map, we&#39;d see that some clusters are lit up, especially around &quot;transportation&quot;. This means that the model has figured out that the topic of the query must be related to transportation in some way.</p>

<p><img alt="nomic-embed-text-v1.5-query" src="/assets/posts/2024-08-03-how-embedding-models-encode-semantic-meaning/nomic-embed-text-v1.5-query.jpeg"/></p>

<p>Zooming into this image, we can see more specific topics around transportation, like &quot;Airport&quot;, &quot;Travel&quot; or &quot;Highways&quot; are lit up, which more closely matches our query.</p>

<p><img alt="nomic-embed-text-v1.5-transportation" src="/assets/posts/2024-08-03-how-embedding-models-encode-semantic-meaning/nomic-embed-text-v1.5-transportation.jpeg"/></p>

<p>In a nutshell, embedding models are able to group terms by topics that are related to each other.</p>

<h2>Why should we care about embeddings?</h2>

<p>Encoding meaning in text has tons of different use cases. One that I&#39;m particularly excited about is building RAG applications. RAG stands for Retrieval-Augmented Generation and refers to a method for Large Language Models (LLMs), where, given a question, you enrich the original question with relevant bits of information before answering it.</p>

<p>Here&#39;s how embeddings are useful for RAG:</p>

<ol><li>You have a bunch of documents in your data source.</li><li>You use an embedding model to turn each document into a list of numbers (like we saw earlier).</li><li>When someone asks a question, you also turn that question into a list of numbers.</li><li>Then, you find the documents whose number lists are most similar to your question&#39;s number list.</li><li>Voila! You&#39;ve found the most relevant documents to answer the question.</li></ol>

<p>This method is way better than previously used techniques like just searching for exact words in the documents. It&#39;s like the difference between having a librarian who only looks at book titles, and one who actually understands what the books are about.</p>

<h2>Other things you can do with embeddings</h2>

<p>Beyond RAG applications, embeddings are super useful for all sorts of things:</p>

<ol><li><strong>Smarter searches</strong>: Find related stuff even if the exact words don&#39;t match.</li><li><strong>Better recommendations</strong>: &quot;You liked this? You might also like these similar things!&quot;</li><li><strong>Language translation</strong>: Help computers understand that &quot;dog&quot; in English and &quot;perro&quot; in Spanish mean the same thing.</li><li><strong>Sentiment analysis</strong>: Figure out if someone&#39;s happy or grumpy based on their tweet.</li></ol>

<h2>Wrapping it up</h2>

<p>Embeddings are a clever way to turn words (or images, or sounds) into numbers that computers can understand and compare. By doing this, we can make emerging AI technologies a whole lot smarter at understanding language and finding connections between ideas.</p>

<p>Next time you&#39;re chatting with an AI or getting scarily accurate recommendations online, you can nod knowingly and think, &quot;Ah yes, embeddings at work!&quot;</p>]]>
            </description>
        </item>
        <item>
            <title>
                ðŸ”— Linkdump: LLMs
            </title>
            <guid>
                https://garrit.xyz/posts/2024-07-02-linkdump-llms
            </guid>
            <link>
                https://garrit.xyz/posts/2024-07-02-linkdump-llms?utm_source=rss
            </link>
            <pubDate>
                Tue, 02 Jul 2024 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>The more I&#39;m getting into large language models (LLMs), the more I&#39;m fascinated about what you can do with them. To &quot;digest&quot; my reading list of cool articles and projects regarding LLMs, I assembled the following list. If you&#39;re also interested but haven&#39;t started your journey into this neverending rabbit hole, these may contain some good pointers:</p>

<h2>Running LLMs</h2>

<ul><li><a href="https://ollama.com/">Ollama</a> - Host different LLMs locally (extremely easy!)</li><li><a href="https://docs.vllm.ai">vLLM</a> - Like Ollama but more scalable and production-ready</li><li><a href="https://github.com/Mozilla-Ocho/llamafile">llamafile</a> - LLMs as a single, portable binary</li></ul>

<h2>Building Chatbots &amp; Applications</h2>

<ul><li><a href="https://www.llamaindex.ai/">LlamaIndex</a> - Framework for building LLM applications</li><li><a href="https://github.com/run-llama/create-llama/">create-llama</a> - LlamaIndex template generator (perfect for if you don&#39;t know how to structure your app)</li><li><a href="https://www.langchain.com/">LangChain</a> - Like LlamaIndex with a less clean interface IMO</li><li><a href="https://haystack.deepset.ai/overview/intro">Haystack</a> - Another LLM framework. Haven&#39;t tested it though</li><li><a href="https://sdk.vercel.ai/docs/introduction">Vercel AI SDK</a> - For integrating your LLM app into a web frontend</li></ul>

<h2>Training and finetuning</h2>

<ul><li><a href="https://huggingface.co/">HuggingFace</a> - GitHub but for LLMs</li><li><a href="https://medium.com/data-science-in-your-pocket/lora-for-fine-tuning-llms-explained-with-codes-and-example-62a7ac5a3578">LoRA for Fine-Tuning LLMs explained with codes and example</a> - Great LoRA Primer</li><li><a href="https://huggingface.co/autotrain">AutoTrain</a> - NoCode LLM finetuning</li><li><a href="https://github.com/georgian-io/LLM-Finetuning-Toolkit">llm-toolkit</a> - Another finetuning framework</li><li><a href="https://mlflow.org/">MLFlow</a> - End to End platform for LLM training</li></ul>

<h2>Miscellaneous projects</h2>

<ul><li><a href="https://github.com/iyaja/llama-fs">llama-fs</a> - Self-organinizing filesystem</li><li><a href="https://github.com/ask-fini/paramount">paramount</a> - Measure agent accuracy</li></ul>

<h2>Blogs</h2>

<ul><li><a href="https://simonwillison.net/tags/llms/">Simon Willison</a> - Awesome posts from a LLM enthusiast</li><li><a href="https://matt-rickard.com/tags/ai">Matt Richard</a> - Matt is currently inactive, but his blog is a treasure trove</li></ul>]]>
            </description>
        </item>
        <item>
            <title>
                Testing SMTP connections
            </title>
            <guid>
                https://garrit.xyz/posts/2024-06-27-testing-smtp-connections
            </guid>
            <link>
                https://garrit.xyz/posts/2024-06-27-testing-smtp-connections?utm_source=rss
            </link>
            <pubDate>
                Thu, 27 Jun 2024 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>Just a quick note to my future self on how to test a SMTP connection with nothing but a tiny busybox container.</p>

<p>In my case specifically, I tested the connection from inside a Kubernetes cluster. Here&#39;s the quickest way to get a temporary pod up and running:</p>

<p><code>
kubectl run -n backend -i --tty --rm debug --image=busybox --restart=Never
</code></p>

<p>Busybox comes with telnet installed, which we can use to establish a connection to the server:</p>

<p><code>
/ # telnet smtp.mydomain.com 25
Connected to smtp.mydomain.com
220 mail.mydomain.com ESMTP Postfix (SMTP)
</code></p>

<p>Next, we can issue the SMTP commands through the open TCP connection to send a test mail. Lines beginning with a status code are server responses:</p>

<p><code></code>`
HELO smtp.mydomain.com
250 smtp.mydomain.com
MAIL FROM:<a href="mailto:noreply@mydomain.com">noreply@mydomain.com</a>                       <br/>250 2.1.0 Ok
RCPT TO:<a href="mailto:receiver@foo.com">receiver@foo.com</a>
250 2.1.5 Ok
DATA<br/>354 End data with &lt;CR&gt;&lt;LF&gt;.&lt;CR&gt;&lt;LF&gt;
From: [noreply] <a href="mailto:noreply@mydomain.com">noreply@mydomain.com</a>
To: [Receiver] <a href="mailto:receiver@foo.com">receiver@foo.com</a>
Date: Thu, 27 Jun 2024 10:08:26 -0200
Subject: Test Message</p>

<p>This is a test message.</p>

<p>.
250 2.0.0 Ok: queued as 2478B7F135
<code></code>`</p>

<p>In case there&#39;s a firewall issue, you might not be able to establish a connection in the first place, or you won&#39;t get a reply to your TCP commands. In our case, everything worked fine.</p>

<p>I hope this is useful!</p>]]>
            </description>
        </item>
        <item>
            <title>
                Host your own LLM
            </title>
            <guid>
                https://garrit.xyz/posts/2024-06-17-host-your-own-llm
            </guid>
            <link>
                https://garrit.xyz/posts/2024-06-17-host-your-own-llm?utm_source=rss
            </link>
            <pubDate>
                Mon, 17 Jun 2024 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>I&#39;m currently dipping my toes into Large Language Models (LLMs, or &quot;AI&quot;) and what you can do with them. It&#39;s a fascinating topic, so expect some more posts on this in the coming days and weeks.</p>

<p>For starters, I wanted to document how I got my first LLM running on my local machine (a 2022 MacBook Pro). <a href="https://ollama.com/">Ollama</a> makes this process super easy. You just install it (<code>brew install ollama</code> in my case) and then run the model:</p>

<p><code>
ollama run llama3
</code></p>

<p>This will download the model and open a prompt, so you can start chatting right away!</p>

<p>You can think of Ollama as the <a href="https://www.docker.com/">Docker</a> CLI but for LLMs. There&#39;s a <a href="https://ollama.com/library">directory of LLMs</a>, and if a model has multiple different sizes, you can use it like you would pull a different docker tag:</p>

<p><code>
ollama pull llama3:8b
ollama pull llama3:70b
</code></p>

<p>The best thing about ollama is that it also exposes a web server for you to integrate the LLM into your application. As an example, here&#39;s how you would curl your local LLM:</p>

<p><code>
curl http://localhost:11434/api/chat -d &#39;{
    &quot;model&quot;: &quot;llama3&quot;,      
    &quot;messages&quot;: [{ &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Are you a robot?&quot; }],
    &quot;stream&quot;: false
}&#39;
{&quot;model&quot;:&quot;llama3&quot;,&quot;created_at&quot;:&quot;2024-06-17T11:19:23.510588Z&quot;,&quot;message&quot;:{&quot;role&quot;:&quot;assistant&quot;,&quot;content&quot;:&quot;I am not a human, but I&#39;m also not a traditional robot. I&#39;m an artificial intelligence language model designed to simulate conversation and answer questions to the best of my ability. My \&quot;brain\&quot; is a complex algorithm that processes natural language inputs and generates responses based on patterns and associations learned from large datasets.\n\nWhile I don&#39;t have a physical body or consciousness like humans do, I&#39;m designed to interact with humans in a way that feels natural and conversational. I can understand and respond to questions, make suggestions, and even tell jokes (though my humor may be a bit... algorithmic).\n\nSo, while I&#39;m not a human or a traditional robot, I exist at the intersection of technology and language, designed to assist and communicate with humans in a helpful way!&quot;},&quot;done_reason&quot;:&quot;stop&quot;,&quot;done&quot;:true,&quot;total_duration&quot;:12565842250,&quot;load_duration&quot;:7059262291,&quot;prompt_eval_count&quot;:15,&quot;prompt_eval_duration&quot;:331275000,&quot;eval_count&quot;:156,&quot;eval_duration&quot;:5172858000}
</code></p>

<p>If your local machine is not beefy enough and you want to try out a large LLM on a rented server (AWS has <code>g5.2xlarge</code>, which gave me good results for <code>mixtral 8x7b</code>), you also have to set <code>OLLAMA_HOST=0.0.0.0</code> in your environment variables to be able to reach the remote server. <strong>This exposes the LLM to the public internet, so be careful when chosing your deployment strategy.</strong></p>

<p>And there you go! You just deployed your very own LLM. Pretty cool, huh?</p>]]>
            </description>
        </item>
    </channel>
</rss>