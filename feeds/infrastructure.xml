<?xml version="1.0"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>
            garrit.xyz
        </title>
        <link>
            https://garrit.xyz
        </link>
        <description>
            Garrit Franke
        </description>
        <language>
            en
        </language>
        <lastBuildDate>
            Tue, 25 Mar 2025 00:00:00 +0000
        </lastBuildDate>
        <item>
            <title>
                About Container Interfaces
            </title>
            <guid>
                https://garrit.xyz/posts/2025-03-25-container-interfaces
            </guid>
            <link>
                https://garrit.xyz/posts/2025-03-25-container-interfaces?utm_source=rss
            </link>
            <pubDate>
                Tue, 25 Mar 2025 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>There are a couple of interfaces that container orchestration systems (like <a href="https://kubernetes.io">Kubernetes</a>) implement to expose certain behavior to their container workloads. I will only be talking about Kubernetes in this post since it&#39;s the orchestrator I&#39;m most comfortable with, but some interfaces are also implemented in other orchestrators (like <a href="https://www.nomadproject.io/">HashiCorp Nomad</a>) too, which makes the interfaces cross-platform.</p>

<h2>Container Storage Interface (CSI)</h2>

<p>Storage behavior used to be built into Kubernetes. The <a href="https://github.com/container-storage-interface/spec/blob/master/spec.md">container storage interface (CSI)</a> defines a unified interface to manage storage volumes, regardless of the orchestrator (as long as they implement the CSI). This makes it way easier for third-party storage providers to expose data to Kubernetes. If a storage provider implements this interface, orchestrators can use it to provision volumes to containers. Notable storage providers are:</p>

<ul><li><a href="https://github.com/kubernetes-csi/csi-driver-nfs">NFS</a></li><li><a href="https://github.com/kubernetes/cloud-provider-openstack/tree/master/pkg/csi/cinder">Cinder (Openstack)</a></li><li><a href="https://github.com/ceph/ceph-csi">Ceph</a></li><li><a href="https://github.com/kubernetes-sigs/aws-ebs-csi-driver">AWS EBS</a></li></ul>

<p>A full list of CSI drivers can be found <a href="https://kubernetes-csi.github.io/docs/drivers.html">here</a>.</p>

<h2>Container Runtime Interface (CRI)</h2>

<p>The <a href="https://github.com/kubernetes/cri-api">Container Runtime Interface (CRI)</a> is an API that allows Kubernetes to use different container runtimes without needing to recompile the entire Kubernetes codebase. Before CRI, Kubernetes had a direct integration with Docker, making it difficult to use alternative container runtimes.</p>

<p>CRI defines the API between Kubernetes components (specifically kubelet) and container runtimes. This abstraction allows Kubernetes to support multiple container runtimes simultaneously, giving users the flexibility to choose the runtime that best fits their needs. Some popular container runtimes that implement the CRI include:</p>

<ul><li><a href="https://containerd.io/">containerd</a> - The industry-standard container runtime that powers Docker and is maintained by the CNCF</li><li><a href="https://cri-o.io/">CRI-O</a> - A lightweight runtime specifically designed for Kubernetes</li><li><a href="https://katacontainers.io/">Kata Containers</a> - A secure container runtime that uses hardware virtualization for stronger isolation</li></ul>

<p>With CRI, switching between different runtimes becomes more straightforward, allowing operators to optimize for security, performance, or compatibility based on their specific requirements.</p>

<h2>Container Network Interface (CNI)</h2>

<p>The <a href="https://github.com/containernetworking/cni">Container Network Interface (CNI)</a> defines a standard for configuring network interfaces for Linux containers. Similar to CSI and CRI, the CNI was created to decouple Kubernetes from specific networking implementations, allowing for a pluggable architecture.</p>

<p>CNI plugins are responsible for allocating IP addresses to pods and ensuring proper network connectivity between pods, nodes, and external networks. They implement features like network policies, load balancing, and network security. Some popular CNI plugins include:</p>

<ul><li><a href="https://www.tigera.io/project-calico/">Calico</a> - Known for its performance, flexibility, and strong network policy support</li><li><a href="https://cilium.io/">Cilium</a> - Uses eBPF for high-performance networking and security</li><li><a href="https://github.com/flannel-io/flannel">Flannel</a> - Simple overlay network focused on ease of use</li><li><a href="https://github.com/aws/amazon-vpc-cni-k8s">AWS VPC CNI</a> - Integrates pods directly with Amazon VPC networking</li></ul>

<p>Each CNI plugin has its strengths and is suitable for different use cases. For example, Calico excels at enforcing network policies, Cilium is optimized for performance and observability, while Flannel is valued for its simplicity.</p>

<h2>Wrapping Up</h2>

<p>One thing I&#39;ve always admired about Kubernetes is its pluggable architecture. These standardized interfaces (CSI, CRI, and CNI) showcase how well-designed the system really is. Instead of building everything into the core, the Kubernetes team made the smart decision to create extension points that allow the community to innovate without touching the core codebase.</p>

<p>The great news? You don&#39;t <em>have</em> to swap out all these components or even understand them deeply to use Kubernetes effectively. While the array of options might seem daunting at first glance, most Kubernetes distributions (like EKS, GKE, AKS, or Rancher) come with sane defaults that work well out of the box. They&#39;ve already made sensible choices about which storage, runtime, and networking components to include.</p>

<p>This pluggability is what makes Kubernetes so powerful for those who need it. Need a specific storage solution? Plug in a CSI driver. Want a more secure container runtime? Swap in a different CRI implementation. But for everyone else, the defaults will serve you just fine.</p>

<p>The beauty of this approach is that it gives you room to grow. Start with the defaults, and when you have specific requirements, the extension points are there waiting for you. That&#39;s the real magic of Kubernetes â€“ it works great out of the box but doesn&#39;t limit your options as your needs evolve.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Testing SMTP connections
            </title>
            <guid>
                https://garrit.xyz/posts/2024-06-27-testing-smtp-connections
            </guid>
            <link>
                https://garrit.xyz/posts/2024-06-27-testing-smtp-connections?utm_source=rss
            </link>
            <pubDate>
                Thu, 27 Jun 2024 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>Just a quick note to my future self on how to test a SMTP connection with nothing but a tiny busybox container.</p>

<p>In my case specifically, I tested the connection from inside a Kubernetes cluster. Here&#39;s the quickest way to get a temporary pod up and running:</p>

<p><code>
kubectl run -n backend -i --tty --rm debug --image=busybox --restart=Never
</code></p>

<p>Busybox comes with telnet installed, which we can use to establish a connection to the server:</p>

<p><code>
/ # telnet smtp.mydomain.com 25
Connected to smtp.mydomain.com
220 mail.mydomain.com ESMTP Postfix (SMTP)
</code></p>

<p>Next, we can issue the SMTP commands through the open TCP connection to send a test mail. Lines beginning with a status code are server responses:</p>

<p><code></code>`
HELO smtp.mydomain.com
250 smtp.mydomain.com
MAIL FROM:<a href="mailto:noreply@mydomain.com">noreply@mydomain.com</a>                       <br/>250 2.1.0 Ok
RCPT TO:<a href="mailto:receiver@foo.com">receiver@foo.com</a>
250 2.1.5 Ok
DATA<br/>354 End data with &lt;CR&gt;&lt;LF&gt;.&lt;CR&gt;&lt;LF&gt;
From: [noreply] <a href="mailto:noreply@mydomain.com">noreply@mydomain.com</a>
To: [Receiver] <a href="mailto:receiver@foo.com">receiver@foo.com</a>
Date: Thu, 27 Jun 2024 10:08:26 -0200
Subject: Test Message</p>

<p>This is a test message.</p>

<p>.
250 2.0.0 Ok: queued as 2478B7F135
<code></code>`</p>

<p>In case there&#39;s a firewall issue, you might not be able to establish a connection in the first place, or you won&#39;t get a reply to your TCP commands. In our case, everything worked fine.</p>

<p>I hope this is useful!</p>]]>
            </description>
        </item>
        <item>
            <title>
                Host your own LLM
            </title>
            <guid>
                https://garrit.xyz/posts/2024-06-17-host-your-own-llm
            </guid>
            <link>
                https://garrit.xyz/posts/2024-06-17-host-your-own-llm?utm_source=rss
            </link>
            <pubDate>
                Mon, 17 Jun 2024 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>I&#39;m currently dipping my toes into Large Language Models (LLMs, or &quot;AI&quot;) and what you can do with them. It&#39;s a fascinating topic, so expect some more posts on this in the coming days and weeks.</p>

<p>For starters, I wanted to document how I got my first LLM running on my local machine (a 2022 MacBook Pro). <a href="https://ollama.com/">Ollama</a> makes this process super easy. You just install it (<code>brew install ollama</code> in my case) and then run the model:</p>

<p><code>
ollama run llama3
</code></p>

<p>This will download the model and open a prompt, so you can start chatting right away!</p>

<p>You can think of Ollama as the <a href="https://www.docker.com/">Docker</a> CLI but for LLMs. There&#39;s a <a href="https://ollama.com/library">directory of LLMs</a>, and if a model has multiple different sizes, you can use it like you would pull a different docker tag:</p>

<p><code>
ollama pull llama3:8b
ollama pull llama3:70b
</code></p>

<p>The best thing about ollama is that it also exposes a web server for you to integrate the LLM into your application. As an example, here&#39;s how you would curl your local LLM:</p>

<p><code>
curl http://localhost:11434/api/chat -d &#39;{
    &quot;model&quot;: &quot;llama3&quot;,      
    &quot;messages&quot;: [{ &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Are you a robot?&quot; }],
    &quot;stream&quot;: false
}&#39;
{&quot;model&quot;:&quot;llama3&quot;,&quot;created_at&quot;:&quot;2024-06-17T11:19:23.510588Z&quot;,&quot;message&quot;:{&quot;role&quot;:&quot;assistant&quot;,&quot;content&quot;:&quot;I am not a human, but I&#39;m also not a traditional robot. I&#39;m an artificial intelligence language model designed to simulate conversation and answer questions to the best of my ability. My \&quot;brain\&quot; is a complex algorithm that processes natural language inputs and generates responses based on patterns and associations learned from large datasets.\n\nWhile I don&#39;t have a physical body or consciousness like humans do, I&#39;m designed to interact with humans in a way that feels natural and conversational. I can understand and respond to questions, make suggestions, and even tell jokes (though my humor may be a bit... algorithmic).\n\nSo, while I&#39;m not a human or a traditional robot, I exist at the intersection of technology and language, designed to assist and communicate with humans in a helpful way!&quot;},&quot;done_reason&quot;:&quot;stop&quot;,&quot;done&quot;:true,&quot;total_duration&quot;:12565842250,&quot;load_duration&quot;:7059262291,&quot;prompt_eval_count&quot;:15,&quot;prompt_eval_duration&quot;:331275000,&quot;eval_count&quot;:156,&quot;eval_duration&quot;:5172858000}
</code></p>

<p>If your local machine is not beefy enough and you want to try out a large LLM on a rented server (AWS has <code>g5.2xlarge</code>, which gave me good results for <code>mixtral 8x7b</code>), you also have to set <code>OLLAMA_HOST=0.0.0.0</code> in your environment variables to be able to reach the remote server. <strong>This exposes the LLM to the public internet, so be careful when chosing your deployment strategy.</strong></p>

<p>And there you go! You just deployed your very own LLM. Pretty cool, huh?</p>]]>
            </description>
        </item>
        <item>
            <title>
                Going from self hosted to managed software
            </title>
            <guid>
                https://garrit.xyz/posts/2024-05-24-going-from-self-hosted-to-managed-software
            </guid>
            <link>
                https://garrit.xyz/posts/2024-05-24-going-from-self-hosted-to-managed-software?utm_source=rss
            </link>
            <pubDate>
                Fri, 24 May 2024 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>Some time ago I was heavily into <a href="https://garrit.xyz/posts/2022-09-26-self-hosted-software-im-thankful-for">self hosting my own software</a>. Over time though, it became apparent that maintaining these services is a huge burden. I either abandoned most of the services or found a replacement that suits my needs and saves me time that&#39;s better spent on other things in life.</p>

<p>Today, I finally pulled the plug on <a href="https://miniflux.app/">Miniflux</a>, the last service I used to self host (not counting House Assistant on a Pi, which I use to automate some stuff at home). The Hetzner server it ran on cost me ~4â‚¬ a month. The maintainer of Miniflux offers a managed hosting solution for 15$ a year, so to save money and time, and to support the project, I decided to switch from self-hosted to managed.</p>

<h2>Will I ever self host again?</h2>

<p>I really don&#39;t know. Some services (Miniflux included) required very little maintenance to keep running, so I could see myself spinning up a server again if I really wanted to run some software that doesn&#39;t have managed hosting. For now though, I&#39;m planning on using managed services wherever I can. Nevertheless I&#39;m proud of the things I learned during my time of self hosting my software. I think it gave me a huge boost both in terms of know how and my career, and I&#39;d encourage everyone to dip a toe into self hosting.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Cost per Request
            </title>
            <guid>
                https://garrit.xyz/posts/2024-01-18-cost-per-request
            </guid>
            <link>
                https://garrit.xyz/posts/2024-01-18-cost-per-request?utm_source=rss
            </link>
            <pubDate>
                Thu, 18 Jan 2024 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>When monitoring infrastructure, we often use metrics like &quot;Requests per second&quot; or &quot;Time per request&quot;.</p>

<p>Contrary to that, when monitoring the cost of our infrastructure, we often only review the cloud bill at the end of the month and tell ourselves that those costs are justified for our product.</p>

<p>What if we made &quot;Cost per request&quot; a key metric in our observability strategy, alongside &quot;Requests per second&quot;?</p>

<p>If we follow that path even further, we could derive a standard for how much a request should cost for x% of uptime, across the entire industry. This way, we should be able to identify how much we&#39;re overspending for our infrastructure compared to other companies or products.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Notes on "Immutability Changes Everything"
            </title>
            <guid>
                https://garrit.xyz/posts/2023-12-21-notes-on-immutability
            </guid>
            <link>
                https://garrit.xyz/posts/2023-12-21-notes-on-immutability?utm_source=rss
            </link>
            <pubDate>
                Thu, 21 Dec 2023 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<blockquote><p>Published books are immutable. Accountants don&#39;t use erasers or they go to jail.</p></blockquote>

<p>The paper <a href="https://www.cidrdb.org/cidr2015/Papers/CIDR15_Paper16.pdf">Immutability Changes Everything</a> by Pat Helland talks about the trend towards immutability in data storage thoughout all layers of the stack. From append-only apps (Record changes, then derive the current state), down to how bits are stored on a hard drive (e.g. Copy on Write).</p>

<p>I stumbled upon this paper through the <a href="https://paperswelove.org/">Papers We Love</a> collection. I was specifically looking for a paper that&#39;s short and easy to comprehend, since I don&#39;t have much experience in reading scientific papers. This is absolutely both short and easy to comprehend. If you&#39;re a beginner like me, I can highly recommend this one to you.</p>

<p>Key takeaways:</p>

<ul><li>Immutability enables clean replication</li><li>Change logs (e.g. <a href="https://en.wikipedia.org/wiki/Write-ahead_logging">write-ahead logs</a>) are the source of truth. The database is a cache of a subset of those change logs</li><li>It&#39;s okay to consider violating <a href="https://en.wikipedia.org/wiki/Database_normalization">normalization</a> rules to trade storage cost for read speed</li><li>Modern SSDs minimize wear by storing new versions of data to other blocks instead of mutating the data in place</li><li>The cost of immutability is increased storage</li></ul>

<hr/>

<p>This is post 098 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Serverless Framework Retrospective
            </title>
            <guid>
                https://garrit.xyz/posts/2023-04-28-serverless-framework-retrospective
            </guid>
            <link>
                https://garrit.xyz/posts/2023-04-28-serverless-framework-retrospective?utm_source=rss
            </link>
            <pubDate>
                Fri, 28 Apr 2023 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>A current project requires the infrastructure to be highly scalable. It&#39;s expected that &gt; 50k Users hit the platform within a five minute period. Regular ECS containers take about one minute to scale up. That just won&#39;t cut it. I decided to go all in on the <a href="https://www.serverless.com/">serverless</a> framework on AWS. Here&#39;s how it went.</p>

<h3>Setup</h3>

<p>Setting up a serverless application was a breeze. You create a config file and use their CLI to deploy the app.</p>

<h3>The rest of the infrastructure</h3>

<p>I decided to define the rest of the infrastructure (VPC, DB, cache, ...) in Terraform. But, since I wasn&#39;t familiar with how the Serverless Framework worked, I struggled to draw the line between what serverless should handle vs. what the rest of the infrastructure (Terraform) should provide. In a more traditional deployment workflow, you might let the CI deploy a container image to ECR and point the ECS service to that new image.</p>

<p>I chose to let Serverless deploy the entire app through CI and build the rest of the infrastructure around it. The problem with this approach is that we lose fine-grained control over what&#39;s deployed where, which leads to a lot of permission errors.</p>

<p>In retrospect, I should&#39;ve probably chosen the location of the S3 archive as the deployment target for the CI, and then point the lambda function to the location of the new artifact. This defeats the purpose of the framework, but it gives you a lot more control over your infrastructure. Once the next project comes along, I&#39;ll probably go that route instead.</p>

<h3>Permissions</h3>

<p>Serverless suggests to use admin permissions for deployments, and I see where they&#39;re coming from. Managing permissions in this framework is an absolute mess. Here&#39;s what the average deployment workflow looks like, if you want to use fine grained permissions:</p>

<ol><li>Wait for CloudFormation to roll back changes (~2 minutes)</li><li>Update IAM role</li><li>Deploy Serverless App</li><li>If there&#39;s an error, go to 1</li></ol>

<p>Thankfully, some people have already gone through the process of figuring this out. <a href="https://serverlessfirst.com/create-iam-deployer-roles-serverless-app/#determining-deploy-time-permissions">Here&#39;s</a> a great guide with a starting point of the needed permissions.</p>

<h3>Conclusion</h3>

<p>Using the serverless framework is a solid choice if you just want to throw an app out there. Unfortunately the app I was deploying isn&#39;t &quot;just&quot; a dynamic website. The next time I&#39;m building a serverless application it&#39;s probably not going to be with the Serverless Framework, though I learned a lot about serverless applications in general.</p>

<hr/>

<p>This is post 067 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Designing resilient cloud infrastructure
            </title>
            <guid>
                https://garrit.xyz/posts/2023-03-30-designing-resilient-cloud-infrastructure
            </guid>
            <link>
                https://garrit.xyz/posts/2023-03-30-designing-resilient-cloud-infrastructure?utm_source=rss
            </link>
            <pubDate>
                Thu, 30 Mar 2023 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>As mentioned in a <a href="/posts/2023-03-16-terraform-project-learnings">previous post</a>, I&#39;m currently finishing up building my first cloud infrastructure on AWS for a client at work. During the development, I learned a lot about designing components to be resilient and scalable. Here are some key takeaways.</p>

<p>One of the most critical components of a resilient infrastructure is redundancy. On AWS, you place your components inside a &quot;region&quot;. This could be <code>eu-central-1</code> (Frankfurt) or <code>us-east-1</code> (North Virgina), etc. To further reduce the risk of an outage, each region is divided into multiple <a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvailabilityZones.html">Availability Zones</a> (AZs). The AZs of a region are usually located some distance apart from each other. In case of a flood, a fire or a bomb detonating near one AZ, the other AZs should in most cases still be intact. You should have at least two, preferably three replicas of each component across multiple availability zones in a region. By having replicas of your components in different availability zones, you reduce the risk of downtime caused by an outage in a single availability zone.</p>

<p>Another way to ensure scalability and resilience for your database is to use <a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless-v2.html">Aurora Serverless v2</a>. This database service is specifically designed for scalable, on-demand, and cost-effective performance. The database scales itself up or down based on the workload, which allows you to automatically and dynamically adjust the database capacity to meet the demand of your application, ensuring that your application is responsive and performs well without the need for manual intervention. Adding Serverless instances to an existing RDS cluster is also a seemless proccess.</p>

<p>In addition to switching to Aurora Serverless v2, using read replicas for cache and database in a separate availability zone can act as a hot standby without extra configuration. Keep in mind that read replicas are only utilized by explicitly using the read-only endpoint of a cluster. But even if you&#39;re only using the &quot;main&quot; cluster endpoint (and therefore just the primary instance), a read replica can promote itself to the primary instance in case of a fail over, which drastically reduces downtime.</p>

<p>When using Amazon Elastic Container Service (ECS), use Fargate as opposed to EC2 instances. Fargate is a serverless compute engine for containers that allows you to run containers without having to manage the underlying infrastructure. It smartly locates instances across availability zones, ensuring that your application is always available.</p>

<p>In conclusion, you should always ensure that there are more than one instance of a component in your infrastructure. There are also services on AWS that abstract away the physical infrastructure (Fargate, S3, Lambda) and use a multi-AZ pattern by default.</p>

<hr/>

<p>This is post 061 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Terraform project learnings
            </title>
            <guid>
                https://garrit.xyz/posts/2023-03-16-terraform-project-learnings
            </guid>
            <link>
                https://garrit.xyz/posts/2023-03-16-terraform-project-learnings?utm_source=rss
            </link>
            <pubDate>
                Thu, 16 Mar 2023 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>I just finished my first ever infrastructure project for a client. My Terraform skills are good enough to be dangerous, but during the development of this project I learned a lot that I would do differently next time.</p>

<h2>Project structure</h2>

<p>Having worked with semi-professional Terraform code before, I applied what I knew to my new project. That was mainly that we have a shared base and an overlay directory for each environment. I went with a single Terraform module for the shared infrastructure, and variables for each environment. Naively, roughly every service had their own file.</p>

<p><code>
.
â”œâ”€â”€ modules
â”‚Â Â  â””â”€â”€ infrastructure
â”‚Â Â      â”œâ”€â”€ alb.tf
â”‚Â Â      â”œâ”€â”€ cache.tf
â”‚Â Â      â”œâ”€â”€ database.tf
â”‚Â Â      â”œâ”€â”€ dns.tf
â”‚Â Â      â”œâ”€â”€ ecr.tf
â”‚Â Â      â”œâ”€â”€ ecs.tf
â”‚Â Â      â”œâ”€â”€ iam.tf
â”‚Â Â      â”œâ”€â”€ logs.tf
â”‚Â Â      â”œâ”€â”€ main.tf
â”‚Â Â      â”œâ”€â”€ network.tf
â”‚Â Â      â”œâ”€â”€ secrets.tf
â”‚Â Â      â”œâ”€â”€ security.tf
â”‚Â Â      â”œâ”€â”€ ssl.tf
â”‚Â Â      â”œâ”€â”€ state.tf
â”‚Â Â      â””â”€â”€ variables.tf
â”œâ”€â”€ production
â”‚Â Â  â”œâ”€â”€ main.tf
â”‚Â Â  â””â”€â”€ secrets.tf
â””â”€â”€ staging
    â”œâ”€â”€ main.tf
    â””â”€â”€ secrets.tf
</code></p>

<p>This works very well, but I already started running into issues extending this setup. For my next project, I would probably find individual components and turn them into smaller reusable submodules. If I were to rewrite the project above, I would probably structure it like this (not a complete project, but I think you get the idea):</p>

<p><code>
.
â”œâ”€â”€ modules
â”‚Â Â  â””â”€â”€ infrastructure
â”‚Â Â      â”œâ”€â”€ main.tf
â”‚Â Â      â”œâ”€â”€ modules
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ database
â”‚Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ iam.tf
â”‚Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ logs.tf
â”‚Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ main.tf
â”‚Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ outputs.tf
â”‚Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ rds.tf
â”‚Â Â      â”‚Â Â  â”‚Â Â  â””â”€â”€ variables.tf
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ loadbalancer
â”‚Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ alb.tf
â”‚Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ logs.tf
â”‚Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ main.tf
â”‚Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ outputs.tf
â”‚Â Â      â”‚Â Â  â”‚Â Â  â””â”€â”€ variables.tf
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ network
â”‚Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ dns.tf
â”‚Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ logs.tf
â”‚Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ main.tf
â”‚Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ outputs.tf
â”‚Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ssl.tf
â”‚Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ variables.tf
â”‚Â Â      â”‚Â Â  â”‚Â Â  â””â”€â”€ vpc.tf
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ service
â”‚Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ecr.tf
â”‚Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ecs.tf
â”‚Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ iam.tf
â”‚Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ logs.tf
â”‚Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ main.tf
â”‚Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ outputs.tf
â”‚Â Â      â”‚Â Â  â”‚Â Â  â””â”€â”€ variables.tf
â”‚Â Â      â”‚Â Â  â””â”€â”€ state
â”‚Â Â      â”‚Â Â      â”œâ”€â”€ locks.tf
â”‚Â Â      â”‚Â Â      â”œâ”€â”€ main.tf
â”‚Â Â      â”‚Â Â      â”œâ”€â”€ outputs.tf
â”‚Â Â      â”‚Â Â      â”œâ”€â”€ s3.tf
â”‚Â Â      â”‚Â Â      â””â”€â”€ variables.tf
â”‚Â Â      â”œâ”€â”€ main.tf
â”‚Â Â      â”œâ”€â”€ outputs.tf
â”‚Â Â      â””â”€â”€ variables.tf
â”œâ”€â”€ production
â”‚Â Â  â”œâ”€â”€ main.tf
â”‚Â Â  â””â”€â”€ secrets.tf
â””â”€â”€ staging
    â”œâ”€â”€ main.tf
    â””â”€â”€ secrets.tf
</code></p>

<h2>Secrets</h2>

<p>I decided to use <a href="https://github.com/AGWA/git-crypt">git-crypt</a> to manage secrets, but that was only before I learned about <a href="https://github.com/mozilla/sops">SOPS</a>. It&#39;s too late to migrate now, but if I could, I would choose SOPS for secrets any day of the week for upcoming projects. It even has a <a href="https://registry.terraform.io/providers/carlpett/sops/latest/docs">Terraform provider</a>, so there&#39;s no excuse not to use it. ;)</p>

<h2>Conclusion</h2>

<p>Overall I&#39;m pretty happy with how the project turned out, but there are some things that I learned during this project that will pay off later.</p>

<hr/>

<p>This is post 057 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Debugging ECS Tasks
            </title>
            <guid>
                https://garrit.xyz/posts/2023-03-10-debugging-ecs-tasks
            </guid>
            <link>
                https://garrit.xyz/posts/2023-03-10-debugging-ecs-tasks?utm_source=rss
            </link>
            <pubDate>
                Fri, 10 Mar 2023 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>I just had to debug an application on AWS ECS. The whole procedure is documented in more detail in the <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-exec.html">documentation</a>, but I think it&#39;s beneficial (both for my future self and hopefully to someone out there) to write down the proccess in my own words.</p>

<p>First of all, you need access to the cluster via the <a href="https://aws.amazon.com/de/cli/">CLI</a>. In addition to the CLI, you need the <a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with-install-plugin.html">AWS Session Manager plugin for the CLI</a>. If you&#39;re on MacOS, you can install that via <a href="https://formulae.brew.sh/cask/session-manager-plugin">Homebrew</a>:</p>

<p><code>
brew install --cask session-manager-plugin
</code></p>

<p>Next, you need to allow the task you want to debug to be able to execute commands. Since I&#39;m using Terraform, this was just a matter of adding the <a href="https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/ecs_service#enable_execute_command"><code>enable_execute_command</code></a> attribute to the service:</p>

<p><code>tf
resource &quot;aws_ecs_service&quot; &quot;my_service&quot; {
  name            = &quot;my-service&quot;
  cluster         = aws_ecs_cluster.my_cluster.id
  task_definition = aws_ecs_task_definition.my_task_definition.id
  desired_count   = var.app_count
  launch_type     = &quot;FARGATE&quot;
  enable_execute_command = true # TODO: Disable after debugging
}
</code></p>

<p>You may also need specify an execution role in the task definition:</p>

<p><code>tf
resource &quot;aws_ecs_task_definition&quot; &quot;my_task_definition&quot; {
  family              = &quot;my-task&quot;
  task_role_arn       = aws_iam_role.ecs_task_execution_role.arn
  execution_role_arn  = aws_iam_role.ecs_task_execution_role.arn  # &lt;-- Add this
}
</code></p>

<p>Make sure that this role has the correct access rights. There&#39;s a nice <a href="https://aws.amazon.com/de/premiumsupport/knowledge-center/ecs-error-execute-command/">troubleshooting guide</a> going over the required permissions.</p>

<p>If you had to do some modifications, make sure to roll out a new deployment with the fresh settings:</p>

<p><code>
aws ecs update-service --cluster my-cluster --service my-service --force-new-deployment
</code></p>

<p>Now, you should be able to issue commands against any running container!</p>

<p><code>
aws ecs execute-command --cluster westfalen --task &lt;task-id-or-arn&gt; --container my-container --interactive --command=&quot;/bin/sh&quot;
</code></p>

<p>I hope this helps!</p>

<hr/>

<p>This is post 055 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
    </channel>
</rss>